{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Particula is a Python-based aerosol particle simulator. Its goal is to provide a robust aerosol simulation (including both gas and particle phases) that can be used to answer scientific questions arising from experiments and research endeavors.</p>"},{"location":"#pypi-installation","title":"PyPi Installation","text":"<p>If your Python environment is already set up, you can install <code>Particula</code> via pip using the following command:</p> <pre><code>pip install particula\n</code></pre> <p>Setup Particula How to Guides Tutorials API Reference</p>"},{"location":"#contributing-to-particula","title":"Contributing to <code>Particula</code>","text":"<p>We are open to and we welcome contributions from anyone who wants to contribute to this project. We have a short contributing document in the root of the repository, which you can read. However, feel free to reach out with any questions or comments!</p>"},{"location":"mk_generator/","title":"Mk generator","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nHandsdown to generate documentation for the project.\n\"\"\"\n</pre> \"\"\" Handsdown to generate documentation for the project. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import os\nfrom pathlib import Path\nfrom handsdown.generators.material import MaterialGenerator\nfrom handsdown.processors.pep257 import PEP257DocstringProcessor\nfrom handsdown.utils.path_finder import PathFinder\n</pre> import os from pathlib import Path from handsdown.generators.material import MaterialGenerator from handsdown.processors.pep257 import PEP257DocstringProcessor from handsdown.utils.path_finder import PathFinder <p>pytype: skip-file</p> In\u00a0[\u00a0]: Copied! <pre>repo_path = Path.cwd()\n</pre> repo_path = Path.cwd() In\u00a0[\u00a0]: Copied! <pre># this little tool works like `pathlib.Path.glob` with some extra magic\n# but in this case `repo_path.glob(\"**/*.py\")` would do as well\npath_finder = PathFinder(repo_path)\n</pre> # this little tool works like `pathlib.Path.glob` with some extra magic # but in this case `repo_path.glob(\"**/*.py\")` would do as well path_finder = PathFinder(repo_path) In\u00a0[\u00a0]: Copied! <pre># no docs for tests and build\npath_finder.exclude(\"tests/*\", \"build/*\")\n</pre> # no docs for tests and build path_finder.exclude(\"tests/*\", \"build/*\") In\u00a0[\u00a0]: Copied! <pre># generate folder structure, if needed\nos.makedirs(repo_path / \"docs/API/\", exist_ok=True)\n</pre> # generate folder structure, if needed os.makedirs(repo_path / \"docs/API/\", exist_ok=True) In\u00a0[\u00a0]: Copied! <pre># initialize generator\nhandsdown = MaterialGenerator(\n    input_path=repo_path,\n    output_path=repo_path / \"docs/API\",\n    source_paths=path_finder.glob(\"**/*.py\"),\n    source_code_url=\"https://github.com/uncscode/particula-beta/blob/main/\",\n    docstring_processor=PEP257DocstringProcessor(),\n)\n</pre> # initialize generator handsdown = MaterialGenerator(     input_path=repo_path,     output_path=repo_path / \"docs/API\",     source_paths=path_finder.glob(\"**/*.py\"),     source_code_url=\"https://github.com/uncscode/particula-beta/blob/main/\",     docstring_processor=PEP257DocstringProcessor(), ) In\u00a0[\u00a0]: Copied! <pre># generate all docs at once\nhandsdown.generate_docs()\n</pre> # generate all docs at once handsdown.generate_docs() In\u00a0[\u00a0]: Copied! <pre># generate index.md file\nhandsdown.generate_index()\n</pre> # generate index.md file handsdown.generate_index()"},{"location":"API/","title":"Particula-beta Index","text":"<p>Auto-generated documentation index.</p> <p>A full list of Particula-beta project modules.</p> <ul> <li>Particula Beta<ul> <li>Data<ul> <li>Lake</li> <li>Lake Stats</li> <li>Loader</li> <li>Loader Interface</li> <li>Loader Setting Builders</li> <li>Merger</li> <li>Mixin</li> <li>Process<ul> <li>Aerodynamic Convert</li> <li>Chamber Rate Fitting</li> <li>Kappa Via Extinction</li> <li>Lognormal 2mode</li> <li>Mie Angular</li> <li>Mie Bulk</li> <li>Ml Analysis<ul> <li>Generate And Train 2mode Sizer</li> <li>Get Ml Folder</li> <li>Run Ml Trainings</li> </ul> </li> <li>Optical Instrument</li> <li>Sample Distribution</li> <li>Scattering Truncation</li> <li>Size Distribution</li> <li>Stats</li> </ul> </li> <li>Settings Generator</li> <li>Stream</li> <li>Stream Stats</li> <li>Util<ul> <li>Convert Length</li> </ul> </li> </ul> </li> <li>Lagrangian<ul> <li>Boundary</li> <li>Collisions</li> <li>Integration</li> <li>Particle Pairs</li> <li>Particle Property</li> </ul> </li> <li>Time Manage</li> <li>Units</li> </ul> </li> </ul>"},{"location":"API/particula_beta/","title":"Particula Beta","text":"<p>Particula-beta Index / Particula Beta</p> <p>Auto-generated documentation for particula_beta module.</p>"},{"location":"API/particula_beta/#modules","title":"Modules","text":"<ul> <li>Data</li> <li>Lagrangian</li> <li>Time Manage</li> <li>Units</li> </ul>"},{"location":"API/particula_beta/time_manage/","title":"Time Manage","text":"<p>Particula-beta Index / Particula Beta / Time Manage</p> <p>Auto-generated documentation for particula_beta.time_manage module.</p>"},{"location":"API/particula_beta/time_manage/#datetime64_from_epoch_array","title":"datetime64_from_epoch_array","text":"<p>Show source in time_manage.py:60</p> <p>Converts an array of epoch times to a numpy array of datetime64 objects.</p>"},{"location":"API/particula_beta/time_manage/#arguments","title":"Arguments","text":"<pre><code>- `epoch_array` *np.ndarray* - Array of epoch times (in seconds since\n    the Unix epoch).\n- `delta` *int* - An optional offset (in seconds) to add to the epoch times\n    before converting to datetime64 objects.\n</code></pre>"},{"location":"API/particula_beta/time_manage/#returns","title":"Returns","text":"<pre><code>- `np.ndarray` - Array of datetime64 objects corresponding to the input\n    epoch times.\n</code></pre>"},{"location":"API/particula_beta/time_manage/#signature","title":"Signature","text":"<pre><code>def datetime64_from_epoch_array(\n    epoch_array: np.ndarray, delta: int = 0\n) -&gt; np.ndarray: ...\n</code></pre>"},{"location":"API/particula_beta/time_manage/#relative_time","title":"relative_time","text":"<p>Show source in time_manage.py:87</p> <p>Cacluates the relative time from the start of the epoch array in the specified units.</p>"},{"location":"API/particula_beta/time_manage/#arguments_1","title":"Arguments","text":"<ul> <li><code>-epoch_array</code> np.ndarray - Array of epoch times (in seconds since     the Unix epoch).</li> <li><code>-units</code> str - The units of the relative time. Default is hours.</li> </ul>"},{"location":"API/particula_beta/time_manage/#returns_1","title":"Returns","text":"<ul> <li><code>-np.ndarray</code> - Array of relative times in the specified units.</li> </ul>"},{"location":"API/particula_beta/time_manage/#signature_1","title":"Signature","text":"<pre><code>def relative_time(epoch_array: np.ndarray, units: str = \"hours\") -&gt; np.ndarray: ...\n</code></pre>"},{"location":"API/particula_beta/time_manage/#time_str_to_epoch","title":"time_str_to_epoch","text":"<p>Show source in time_manage.py:12</p> <p>Convert to UTC (epoch) timezone from all inputs. Using pytz library, which implements the Olson time zone database. tz identifiers are strings from the database. See https://en.wikipedia.org/wiki/List_of_tz_database_time_zones for a list of time zones.</p>"},{"location":"API/particula_beta/time_manage/#arguments_2","title":"Arguments","text":"<p>time : float (single value no arrays)     Epoch time in seconds. time_format : str     The format of the time string. See     https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes     for a list of format codes. timezone_identifier : str     The time zone identifier for the current time zone.</p>"},{"location":"API/particula_beta/time_manage/#returns_2","title":"Returns","text":"<p>new_time : float     The float time in the new time zone.</p> <ul> <li><code>Example</code> - Date Time Format Codes</li> <li>'2019-01-01 00:00:00' is '%Y-%m-%d %H:%M:%S'</li> <li>'10/01/2019 00:00:00' is '%d/%m/%Y %H:%M:%S'</li> <li>'2019-01-01 00:00:00.000000' is '%Y-%m-%d %H:%M:%S.%f'</li> <li>'5/1/2019 1:00:00 PM' is '%m/%d/%Y %I:%M:%S %p'</li> <li><code>-</code> %Y - Year with century as a decimal number.</li> <li><code>-</code> %m - Month as a zero-padded decimal number.</li> <li><code>-</code> %d - Day of the month as a zero-padded decimal number.</li> <li><code>-</code> %H - Hour (24-hour clock) as a zero-padded decimal number.</li> <li><code>-</code> %M - Minute as a zero-padded decimal number.</li> <li><code>-</code> %S - Second as a zero-padded decimal number.</li> <li><code>-</code> %f - Microsecond as a decimal number, zero-padded on the left.</li> <li><code>-</code> %p - Locales equivalent of either AM or PM.</li> </ul>"},{"location":"API/particula_beta/time_manage/#_1","title":"Time Manage","text":""},{"location":"API/particula_beta/time_manage/#signature_2","title":"Signature","text":"<pre><code>def time_str_to_epoch(\n    time: str, time_format: str, timezone_identifier: str = \"UTC\"\n) -&gt; float: ...\n</code></pre>"},{"location":"API/particula_beta/units/","title":"Units","text":"<p>Particula-beta Index / Particula Beta / Units</p> <p>Auto-generated documentation for particula_beta.units module.</p>"},{"location":"API/particula_beta/units/#convert_units","title":"convert_units","text":"<p>Show source in units.py:16</p> <p>Generic wrapper for pint to convert units.</p>"},{"location":"API/particula_beta/units/#arguments","title":"Arguments","text":"<ul> <li>old : old units defined by pint, e.g., \"m\", \"ft\", \"Km\", \"kg/m^3\"</li> <li>new : new units defined by pint, e.g., \"m\", \"ft\", \"Km\", \"kg/m^3\"</li> <li>value : value to convert, needed for units with and offset     e.g., \"degC\"</li> </ul>"},{"location":"API/particula_beta/units/#returns","title":"Returns","text":"<ul> <li>float : conversion multiplier from old to new units. If value is     provided, it returns the converted value in the new units.</li> </ul>"},{"location":"API/particula_beta/units/#raises","title":"Raises","text":"<ul> <li><code>ImportError</code> - if pint is not installed</li> </ul>"},{"location":"API/particula_beta/units/#examples","title":"Examples","text":"Example Usage<pre><code>conversion_multipliter = convert_units(\"ug/m^3\", \"kg/m^3\")\n# 1e-9\n</code></pre>"},{"location":"API/particula_beta/units/#signature","title":"Signature","text":"<pre><code>def convert_units(old: str, new: str, value: Optional[float] = None) -&gt; float: ...\n</code></pre>"},{"location":"API/particula_beta/data/","title":"Data","text":"<p>Particula-beta Index / Particula Beta / Data</p> <p>Auto-generated documentation for particula_beta.data module.</p>"},{"location":"API/particula_beta/data/#modules","title":"Modules","text":"<ul> <li>Lake</li> <li>Lake Stats</li> <li>Loader</li> <li>Loader Interface</li> <li>Loader Setting Builders</li> <li>Merger</li> <li>Mixin</li> <li>Process</li> <li>Settings Generator</li> <li>Stream</li> <li>Stream Stats</li> <li>Util</li> </ul>"},{"location":"API/particula_beta/data/lake/","title":"Lake","text":"<p>Particula-beta Index / Particula Beta / Data / Lake</p> <p>Auto-generated documentation for particula_beta.data.lake module.</p>"},{"location":"API/particula_beta/data/lake/#lake_1","title":"Lake","text":"<p>Show source in lake.py:10</p> <p>A class representing a lake which is a collection of streams.</p>"},{"location":"API/particula_beta/data/lake/#attributes","title":"Attributes","text":"<p>streams (Dict[str, Stream]): A dictionary to hold streams with their names as keys.</p>"},{"location":"API/particula_beta/data/lake/#signature","title":"Signature","text":"<pre><code>class Lake: ...\n</code></pre>"},{"location":"API/particula_beta/data/lake/#lakedelitem","title":"Lake().delitem","text":"<p>Show source in lake.py:97</p> <p>Remove a stream by name. Example: del lake['stream_name'].</p>"},{"location":"API/particula_beta/data/lake/#signature_1","title":"Signature","text":"<pre><code>def __delitem__(self, key: str) -&gt; None: ...\n</code></pre>"},{"location":"API/particula_beta/data/lake/#lakedir","title":"Lake().dir","text":"<p>Show source in lake.py:52</p> <p>List available streams. Example: dir(lake).</p>"},{"location":"API/particula_beta/data/lake/#signature_2","title":"Signature","text":"<pre><code>def __dir__(self) -&gt; list: ...\n</code></pre>"},{"location":"API/particula_beta/data/lake/#lakegetattr","title":"Lake().getattr","text":"<p>Show source in lake.py:40</p> <p>Allow accessing streams as an attributes.</p>"},{"location":"API/particula_beta/data/lake/#raises","title":"Raises","text":"<pre><code>- `AttributeError` - If the stream name is not in the lake.\n</code></pre> <ul> <li><code>Example</code> - lake.stream_name</li> </ul>"},{"location":"API/particula_beta/data/lake/#signature_3","title":"Signature","text":"<pre><code>def __getattr__(self, name: str) -&gt; Any: ...\n</code></pre>"},{"location":"API/particula_beta/data/lake/#lakegetitem","title":"Lake().getitem","text":"<p>Show source in lake.py:82</p> <p>Get a stream by name. Example: lake['stream_name'].</p>"},{"location":"API/particula_beta/data/lake/#signature_4","title":"Signature","text":"<pre><code>def __getitem__(self, key: str) -&gt; Any: ...\n</code></pre>"},{"location":"API/particula_beta/data/lake/#lakeiter","title":"Lake().iter","text":"<p>Show source in lake.py:58</p> <p>Iterate over the streams in the lake. Example: [stream.header for stream in lake]\"\".</p>"},{"location":"API/particula_beta/data/lake/#signature_5","title":"Signature","text":"<pre><code>def __iter__(self) -&gt; Iterator[Any]: ...\n</code></pre>"},{"location":"API/particula_beta/data/lake/#lakelen","title":"Lake().len","text":"<p>Show source in lake.py:76</p> <p>Return the number of streams in the lake. Example: len(lake).</p>"},{"location":"API/particula_beta/data/lake/#signature_6","title":"Signature","text":"<pre><code>def __len__(self) -&gt; int: ...\n</code></pre>"},{"location":"API/particula_beta/data/lake/#lakerepr","title":"Lake().repr","text":"<p>Show source in lake.py:106</p> <p>Return a string representation of the lake. Example: print(lake).</p>"},{"location":"API/particula_beta/data/lake/#signature_7","title":"Signature","text":"<pre><code>def __repr__(self) -&gt; str: ...\n</code></pre>"},{"location":"API/particula_beta/data/lake/#lakesetitem","title":"Lake().setitem","text":"<p>Show source in lake.py:88</p> <p>Set a stream by name. Example: lake['stream_name'] = new_stream.</p>"},{"location":"API/particula_beta/data/lake/#signature_8","title":"Signature","text":"<pre><code>def __setitem__(self, key: str, value: Stream) -&gt; None: ...\n</code></pre>"},{"location":"API/particula_beta/data/lake/#see-also","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/lake/#lakeadd_stream","title":"Lake().add_stream","text":"<p>Show source in lake.py:20</p> <p>Add a stream to the lake.</p>"},{"location":"API/particula_beta/data/lake/#arguments","title":"Arguments","text":"<ul> <li><code>stream</code> Stream - The stream object to be added.</li> <li><code>name</code> str - The name of the stream.</li> </ul>"},{"location":"API/particula_beta/data/lake/#raises_1","title":"Raises","text":"<pre><code>- `ValueError` - If the stream name is already in use or not a valid\nidentifier.\n</code></pre>"},{"location":"API/particula_beta/data/lake/#signature_9","title":"Signature","text":"<pre><code>def add_stream(self, stream: Stream, name: str) -&gt; None: ...\n</code></pre>"},{"location":"API/particula_beta/data/lake/#see-also_1","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/lake/#lakeitems","title":"Lake().items","text":"<p>Show source in lake.py:64</p> <p>Return an iterator over the key-value pairs.</p>"},{"location":"API/particula_beta/data/lake/#signature_10","title":"Signature","text":"<pre><code>def items(self) -&gt; Iterator[Tuple[Any, Any]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/lake/#lakekeys","title":"Lake().keys","text":"<p>Show source in lake.py:72</p> <p>Return an iterator over the keys.</p>"},{"location":"API/particula_beta/data/lake/#signature_11","title":"Signature","text":"<pre><code>def keys(self) -&gt; Iterator[Any]: ...\n</code></pre>"},{"location":"API/particula_beta/data/lake/#lakesummary","title":"Lake().summary","text":"<p>Show source in lake.py:112</p> <pre><code>Return a string summary iterating over each stream\nand print Stream.header.\n</code></pre> <p>Example: lake.summary.</p>"},{"location":"API/particula_beta/data/lake/#signature_12","title":"Signature","text":"<pre><code>@property\ndef summary(self) -&gt; None: ...\n</code></pre>"},{"location":"API/particula_beta/data/lake/#lakevalues","title":"Lake().values","text":"<p>Show source in lake.py:68</p> <p>Return an iterator over the values.</p>"},{"location":"API/particula_beta/data/lake/#signature_13","title":"Signature","text":"<pre><code>def values(self) -&gt; Iterator[Any]: ...\n</code></pre>"},{"location":"API/particula_beta/data/lake_stats/","title":"Lake Stats","text":"<p>Particula-beta Index / Particula Beta / Data / Lake Stats</p> <p>Auto-generated documentation for particula_beta.data.lake_stats module.</p>"},{"location":"API/particula_beta/data/lake_stats/#get_lake_average_std","title":"get_lake_average_std","text":"<p>Show source in lake_stats.py:12</p> <p>Averages the data in each stream within a 'Lake' object.</p> <p>If 'clone' is True, a new 'Lake' instance is created and the averaged data is stored there. If 'clone' is False, the original 'Lake' instance is modified. The averaged output also includes the standard deviation of the data.</p>"},{"location":"API/particula_beta/data/lake_stats/#examples","title":"Examples","text":"<pre><code># Example lake with two streams, each containing numerical data\nlake_data = Lake({'stream1': [1, 2, 3], 'stream2': [4, 5, 6]})\n# Average over a 60-second interval without creating a new lake.\naveraged_lake = average_std(lake_data, 60, clone=False)\nprint(averaged_lake)\nLake({'stream1': [2], 'stream2': [5]})\n</code></pre>"},{"location":"API/particula_beta/data/lake_stats/#arguments","title":"Arguments","text":"<ul> <li><code>lake</code> - The lake data structure containing multiple streams.</li> <li><code>average_interval</code> - The interval over which to average the data.     Default is 60.</li> <li><code>new_time_array</code> - A new array of time points at which to compute the     averages.</li> <li><code>clone</code> - Indicates whether to modify the original lake or return a new     one. Default is True.</li> </ul>"},{"location":"API/particula_beta/data/lake_stats/#returns","title":"Returns","text":"<ul> <li><code>Lake</code> - A lake instance with averaged data.</li> </ul>"},{"location":"API/particula_beta/data/lake_stats/#signature","title":"Signature","text":"<pre><code>def get_lake_average_std(\n    lake: Lake,\n    average_interval: Union[float, int] = 60,\n    new_time_array: Optional[NDArray[np.float64]] = None,\n    clone: bool = True,\n) -&gt; Lake: ...\n</code></pre>"},{"location":"API/particula_beta/data/lake_stats/#see-also","title":"See also","text":"<ul> <li>Lake</li> </ul>"},{"location":"API/particula_beta/data/loader/","title":"Loader","text":"<p>Particula-beta Index / Particula Beta / Data / Loader</p> <p>Auto-generated documentation for particula_beta.data.loader module.</p>"},{"location":"API/particula_beta/data/loader/#data_format_checks","title":"data_format_checks","text":"<p>Show source in loader.py:154</p> <p>Validate and format raw data according to specified checks.</p>"},{"location":"API/particula_beta/data/loader/#arguments","title":"Arguments","text":"<ul> <li><code>data</code> - List of strings containing the raw data to be checked.</li> <li><code>data_checks</code> - Dictionary specifying the format checks to apply,     such as character limits, character counts, and rows to skip.</li> </ul>"},{"location":"API/particula_beta/data/loader/#returns","title":"Returns","text":"<p>A list of strings containing the validated and formatted data.</p>"},{"location":"API/particula_beta/data/loader/#raises","title":"Raises","text":"<ul> <li><code>TypeError</code> - If <code>data</code> is not provided as a list.</li> </ul>"},{"location":"API/particula_beta/data/loader/#examples","title":"Examples","text":"Validate line based on counts<pre><code>data = ['row 1', 'row 2', 'row 3']\ndata_checks = {\n    \"characters\": [0, 10],\n    \"char_counts\": {\",\": 2, \"/\": 0, \":\": 0},\n    \"skip_rows\": 0,\n    \"skip_end\": 0\n}\nformatted_data = data_format_checks(data, data_checks)\nprint(formatted_data)\n['row 2']\n</code></pre>"},{"location":"API/particula_beta/data/loader/#signature","title":"Signature","text":"<pre><code>def data_format_checks(data: List[str], data_checks: dict) -&gt; List[str]: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#data_raw_loader","title":"data_raw_loader","text":"<p>Show source in loader.py:23</p> <p>Loads raw data from file.</p> <p>Load raw data from a file at the specified file path and return it as a list of strings. Attempts to handle UTF-8, UTF-16, and UTF-32 encodings. Defaults to UTF-8 if no byte order mark (BOM) is found.</p>"},{"location":"API/particula_beta/data/loader/#arguments_1","title":"Arguments","text":"<ul> <li><code>file_path</code> str - The file path of the file to read.</li> </ul>"},{"location":"API/particula_beta/data/loader/#returns_1","title":"Returns","text":"<ul> <li><code>list</code> - The raw data read from the file as a list of strings.</li> </ul>"},{"location":"API/particula_beta/data/loader/#examples_1","title":"Examples","text":"Load my_file.txt<pre><code>data = data_raw_loader('my_file.txt')\nLoading data from: my_file.txt\nprint(data)\n['line 1', 'line 2', 'line 3']\n</code></pre>"},{"location":"API/particula_beta/data/loader/#signature_1","title":"Signature","text":"<pre><code>def data_raw_loader(file_path: str) -&gt; list: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#filter_list","title":"filter_list","text":"<p>Show source in loader.py:72</p> <p>Filter rows from a list of strings based on character counts.</p> <p>Each row must contain a specified number of certain characters to pass the filter. The <code>char_counts</code> dictionary specifies the characters to count and the exact count required for each character in each row.</p>"},{"location":"API/particula_beta/data/loader/#arguments_2","title":"Arguments","text":"<ul> <li><code>data</code> - A list of strings to be filtered.</li> <li><code>char_counts</code> - A dictionary specifying character counts for filtering.     The keys are the characters to count, and the values are the     required counts for each character in a row.</li> </ul>"},{"location":"API/particula_beta/data/loader/#returns_2","title":"Returns","text":"<p>A new list of strings containing only the rows that meet the character count requirements.</p>"},{"location":"API/particula_beta/data/loader/#raises_1","title":"Raises","text":"<ul> <li><code>UserWarning</code> - If more than 90% of the rows are filtered out, indicating     that the filter may be too strict based on the specified     character(s).</li> </ul>"},{"location":"API/particula_beta/data/loader/#examples_2","title":"Examples","text":"Filter rows based on comma counts<pre><code>data = ['apple,banana,orange',\n         'pear,kiwi,plum', 'grapefruit,lemon']\nchar_counts = {',': 2}\nfiltered_data = filter_list(data, char_counts)\nprint(filtered_data)\n['apple,banana,orange', 'pear,kiwi,plum']\n</code></pre>"},{"location":"API/particula_beta/data/loader/#signature_2","title":"Signature","text":"<pre><code>def filter_list(data: List[str], char_counts: dict) -&gt; List[str]: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#general_data_formatter","title":"general_data_formatter","text":"<p>Show source in loader.py:454</p> <p>Format and sample data to extract time and data streams.</p>"},{"location":"API/particula_beta/data/loader/#arguments_3","title":"Arguments","text":"<ul> <li><code>data</code> - List of strings containing the raw data.</li> <li><code>data_checks</code> - Dictionary specifying validation rules for the data.</li> <li><code>data_column</code> - List of indices identifying the columns containing the     data.</li> <li><code>time_column</code> - Index or list of indices identifying the column(s)     containing the time information.</li> <li><code>time_format</code> - String specifying the format of the time information,     e.g., '%Y-%m-%d %H:%M:%S'.</li> <li><code>delimiter</code> - String used to separate columns in the data. Default is ','.</li> <li><code>header_row</code> - Index of the row containing column names. Default is 0.</li> <li><code>date_offset</code> - Optional string to add as a fixed offset to the timestamp.     Default is None.</li> <li><code>seconds_shift</code> - Number of seconds to add to the timestamp. Default is 0.</li> <li><code>timezone_identifier</code> - Timezone identifier for the timestamps.     Default is 'UTC'.</li> </ul>"},{"location":"API/particula_beta/data/loader/#returns_3","title":"Returns","text":"<p>Tuple (np.ndarray, np.ndarray):     - The first array contains the epoch times.     - The second array contains the corresponding data values.</p>"},{"location":"API/particula_beta/data/loader/#signature_3","title":"Signature","text":"<pre><code>def general_data_formatter(\n    data: list,\n    data_checks: dict,\n    data_column: list,\n    time_column: Union[int, List[int]],\n    time_format: str,\n    delimiter: str = \",\",\n    header_row: int = 0,\n    date_offset: Optional[str] = None,\n    seconds_shift: int = 0,\n    timezone_identifier: str = \"UTC\",\n) -&gt; Tuple[np.ndarray, np.ndarray]: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#get_files_in_folder_with_size","title":"get_files_in_folder_with_size","text":"<p>Show source in loader.py:675</p> <p>Returns a list of files in the specified folder and subfolder that match the given filename pattern and have a size greater than the specified minimum size.</p>"},{"location":"API/particula_beta/data/loader/#arguments_4","title":"Arguments","text":"<p>path : str     The path to the parent folder. subfolder : str     The name of the subfolder containing the files. filename_regex : str     A regular expression pattern for matching the filenames. min_size : int, optional     The minimum file size in bytes (default is 10).</p>"},{"location":"API/particula_beta/data/loader/#returns_4","title":"Returns","text":"<p>Tuple(List[str], List[str], List[int]): - <code>-</code> file_list - The filenames that match the pattern and size criteria. - <code>-</code> full_path - The full paths to the files. - <code>-</code> file_size - The file sizes in bytes.</p>"},{"location":"API/particula_beta/data/loader/#signature_4","title":"Signature","text":"<pre><code>def get_files_in_folder_with_size(\n    path: str, subfolder: str, filename_regex: str, min_size: int = 10\n) -&gt; Tuple[List[str], List[str], List[int]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#keyword_to_index","title":"keyword_to_index","text":"<p>Show source in loader.py:521</p> <p>Convert a keyword representing a column position in the header to its index.</p> <p>This function processes a keyword that can either be an integer index or a string corresponding to a column name. If the keyword is an integer, it is treated as the direct index of the column. If the keyword is a string, the function searches the header list for the column name and returns its index.</p>"},{"location":"API/particula_beta/data/loader/#arguments_5","title":"Arguments","text":"<ul> <li><code>keyword</code> - The keyword representing the column's position in the header.     It can be an integer index or a string specifying the column name.</li> <li><code>header</code> - A list of column names (header) in the data.</li> </ul>"},{"location":"API/particula_beta/data/loader/#returns_5","title":"Returns","text":"<p>The index of the column in the header.</p>"},{"location":"API/particula_beta/data/loader/#raises_2","title":"Raises","text":"<ul> <li><code>ValueError</code> - If the keyword is a string and is not found in the header,     or if the keyword is an integer but is out of the header's     index range.</li> </ul>"},{"location":"API/particula_beta/data/loader/#signature_5","title":"Signature","text":"<pre><code>def keyword_to_index(keyword: Union[str, int], header: List[str]) -&gt; int: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#load_lake","title":"load_lake","text":"<p>Show source in loader.py:956</p> <p>Load a lake object by loading individual streams from separate pickle files.</p>"},{"location":"API/particula_beta/data/loader/#arguments_6","title":"Arguments","text":"<ul> <li><code>path</code> - Path to load pickle files.</li> <li><code>suffix_name</code> - Suffix to add to pickle file names. The default is None.</li> <li><code>folder</code> - Folder to load pickle files from. The default is 'output'.</li> </ul>"},{"location":"API/particula_beta/data/loader/#returns_6","title":"Returns","text":"<ul> <li><code>Lake</code> - Reconstructed Lake object.</li> </ul>"},{"location":"API/particula_beta/data/loader/#signature_6","title":"Signature","text":"<pre><code>def load_lake(\n    path: str, suffix_name: Optional[str] = None, folder: str = \"output\"\n) -&gt; Lake: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#see-also","title":"See also","text":"<ul> <li>Lake</li> </ul>"},{"location":"API/particula_beta/data/loader/#load_stream","title":"load_stream","text":"<p>Show source in loader.py:845</p> <p>Load stream object from a pickle file.</p>"},{"location":"API/particula_beta/data/loader/#args","title":"Args","text":"<p>path : str     Path to load pickle file. suffix_name : str, optional     Suffix to add to pickle file name. The default is None. folder : str, optional     Folder to load pickle file from. The default is 'output'.</p>"},{"location":"API/particula_beta/data/loader/#returns_7","title":"Returns","text":"<p>Stream     Loaded Stream object.</p>"},{"location":"API/particula_beta/data/loader/#signature_7","title":"Signature","text":"<pre><code>def load_stream(\n    path: str, suffix_name: Optional[str] = None, folder: str = \"output\"\n) -&gt; Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#see-also_1","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/loader/#netcdf_data_1d_load","title":"netcdf_data_1d_load","text":"<p>Show source in loader.py:1067</p> <p>Given a netCDF file path and settings, returns a tuple containing the epoch time, header, and data as a numpy array. We do apply the mask to the data, and fill the masked values with nan.</p>"},{"location":"API/particula_beta/data/loader/#arguments_7","title":"Arguments","text":"<ul> <li><code>file_path</code> str - The path to the netCDF file.</li> <li><code>settings</code> dict - A dictionary containing settings for the instrument.</li> </ul>"},{"location":"API/particula_beta/data/loader/#returns_8","title":"Returns","text":"<p>Tuple[np.ndarray, list, np.ndarray]: A tuple containing the epoch time, header, and data as a numpy array.</p> <p>Errors:     - <code>KeyError</code> - If the settings dictionary does not contain 'data_1d'.</p>"},{"location":"API/particula_beta/data/loader/#signature_8","title":"Signature","text":"<pre><code>def netcdf_data_1d_load(\n    file_path: str, settings: dict\n) -&gt; Tuple[np.ndarray, list, np.ndarray]: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#netcdf_data_2d_load","title":"netcdf_data_2d_load","text":"<p>Show source in loader.py:1123</p> <p>Given a netCDF file path and settings, returns a tuple containing the epoch time, header, and data as a numpy array. We do apply the mask to the data, and fill the masked values with nan.</p>"},{"location":"API/particula_beta/data/loader/#arguments_8","title":"Arguments","text":"<ul> <li><code>file_path</code> str - The path to the netCDF file.</li> <li><code>settings</code> dict - A dictionary containing settings for the instrument.</li> </ul>"},{"location":"API/particula_beta/data/loader/#returns_9","title":"Returns","text":"<p>Tuple[np.ndarray, list, np.ndarray]: A tuple containing the epoch time, header, and data as a numpy array.</p> <p>Errors:     - <code>KeyError</code> - If the settings dictionary does not contain 'data_2d'.</p>"},{"location":"API/particula_beta/data/loader/#signature_9","title":"Signature","text":"<pre><code>def netcdf_data_2d_load(\n    file_path: str, settings: dict\n) -&gt; Tuple[np.ndarray, list, np.ndarray]: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#netcdf_get_epoch_time","title":"netcdf_get_epoch_time","text":"<p>Show source in loader.py:1041</p> <p>Given a netCDF file path and settings, returns an array of epoch times in seconds as a float.</p> <p>Currently only uses ARM 1.2 netCDF files (base_time + time_offset)</p>"},{"location":"API/particula_beta/data/loader/#arguments_9","title":"Arguments","text":"<ul> <li><code>file_path</code> str - The path to the netCDF file.</li> <li><code>settings</code> dict - A dictionary containing settings for the instrument.</li> </ul>"},{"location":"API/particula_beta/data/loader/#returns_10","title":"Returns","text":"<ul> <li><code>np.ndarray</code> - An array of epoch times, in seconds as a float.</li> </ul>"},{"location":"API/particula_beta/data/loader/#signature_10","title":"Signature","text":"<pre><code>def netcdf_get_epoch_time(file_path: str, settings: dict) -&gt; np.ndarray: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#netcdf_info_print","title":"netcdf_info_print","text":"<p>Show source in loader.py:1175</p> <p>Prints information about a netCDF file. Useful for generating settings dictionaries.</p>"},{"location":"API/particula_beta/data/loader/#arguments_10","title":"Arguments","text":"<ul> <li><code>file_path</code> str - The path to the netCDF file.</li> <li><code>file_return</code> bool - If True, returns the netCDF file object.     Defaults to False.</li> </ul>"},{"location":"API/particula_beta/data/loader/#returns_11","title":"Returns","text":"<ul> <li><code>nc_file</code> netCDF4.Dataset - The netCDF file object.</li> </ul>"},{"location":"API/particula_beta/data/loader/#signature_11","title":"Signature","text":"<pre><code>def netcdf_info_print(file_path, file_return=False): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#non_standard_date_location","title":"non_standard_date_location","text":"<p>Show source in loader.py:647</p> <p>Extracts the date from a non-standard location in the data.</p>"},{"location":"API/particula_beta/data/loader/#arguments_11","title":"Arguments","text":"<ul> <li><code>data</code> - A list of strings representing the data.</li> <li><code>date_location</code> - A dictionary specifying the method for extracting the     date from the data.         - <code>-</code> 'file_header_block' - The date is located in the file header             block, and its position is specified by the 'row',             'delimiter', and 'index' keys.</li> </ul>"},{"location":"API/particula_beta/data/loader/#returns_12","title":"Returns","text":"<ul> <li><code>str</code> - The date extracted from the specified location in the data.</li> </ul>"},{"location":"API/particula_beta/data/loader/#raises_3","title":"Raises","text":"<ul> <li><code>ValueError</code> - If an unsupported or invalid method is specified in     date_location.</li> </ul>"},{"location":"API/particula_beta/data/loader/#signature_12","title":"Signature","text":"<pre><code>def non_standard_date_location(data: list, date_location: dict) -&gt; str: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#parse_time_column","title":"parse_time_column","text":"<p>Show source in loader.py:223</p> <p>Parse the time column(s) from a data line and return the timestamp.</p>"},{"location":"API/particula_beta/data/loader/#arguments_12","title":"Arguments","text":"<ul> <li><code>time_column</code> - Index or list of indices identifying the column(s)     containing the time information.</li> <li><code>time_format</code> - String specifying the format of the time information,     e.g., '%Y-%m-%d %H:%M:%S'.</li> <li><code>line</code> - A numpy array representing the data line to parse.</li> <li><code>date_offset</code> - Optional string representing a fixed offset to add     to the timestamp. Default is None.</li> <li><code>seconds_shift</code> - Number of seconds to add to the timestamp. Default is 0.</li> <li><code>timezone_identifier</code> - Timezone identifier for the timestamp.     Default is 'UTC'.</li> </ul>"},{"location":"API/particula_beta/data/loader/#returns_13","title":"Returns","text":"<p>A float representing the timestamp in seconds since the epoch.</p>"},{"location":"API/particula_beta/data/loader/#raises_4","title":"Raises","text":"<ul> <li><code>ValueError</code> - If the specified time column or format is invalid.</li> </ul>"},{"location":"API/particula_beta/data/loader/#signature_13","title":"Signature","text":"<pre><code>def parse_time_column(\n    time_column: Union[int, List[int]],\n    time_format: str,\n    line: np.ndarray,\n    date_offset: Optional[str] = None,\n    seconds_shift: int = 0,\n    timezone_identifier: str = \"UTC\",\n) -&gt; float: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#replace_list","title":"replace_list","text":"<p>Show source in loader.py:119</p> <p>Replace characters in each string of a list based on a replacement dictionary.</p> <p>Each character specified in the <code>replace_dict</code> will be replaced with the corresponding value in every string in the input list.</p>"},{"location":"API/particula_beta/data/loader/#arguments_13","title":"Arguments","text":"<ul> <li><code>data</code> - A list of strings in which the characters will be replaced.</li> <li><code>replace_dict</code> - A dictionary specifying character replacements.     The keys are the characters to be replaced, and the values are the     replacement characters or strings.</li> </ul>"},{"location":"API/particula_beta/data/loader/#returns_14","title":"Returns","text":"<p>A new list of strings with the replacements applied.</p>"},{"location":"API/particula_beta/data/loader/#examples_3","title":"Examples","text":"Replace characters in a list of strings<pre><code>data = ['apple[banana]orange', '[pear] kiwi plum']\nreplace_dict = {'[': '', ']': ''}\nreplaced_data = replace_list(data, replace_dict)\nprint(replaced_data)\n['applebananaorange', 'pear kiwi plum']\n</code></pre>"},{"location":"API/particula_beta/data/loader/#signature_14","title":"Signature","text":"<pre><code>def replace_list(data: List[str], replace_dict: Dict[str, str]) -&gt; List[str]: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#sample_data","title":"sample_data","text":"<p>Show source in loader.py:291</p> <p>Extract time and data streams from input data.</p>"},{"location":"API/particula_beta/data/loader/#arguments_14","title":"Arguments","text":"<ul> <li><code>data</code> - List of strings containing the input data.</li> <li><code>time_column</code> - Index or list of indices indicating the column(s)     containing the time values.</li> <li><code>time_format</code> - Format string specifying the time format, e.g.,     '%Y-%m-%d %H:%M:%S'.</li> <li><code>data_columns</code> - List of indices identifying the columns containing     the data values.</li> <li><code>delimiter</code> - Character used to separate columns in the input data.</li> <li><code>date_offset</code> - Optional string representing an offset to apply to     the date, in the format 'days:hours:minutes:seconds'.     Default is None.</li> <li><code>seconds_shift</code> - Number of seconds to shift the timestamps. Default is 0.</li> <li><code>timezone_identifier</code> - Timezone of the data. Default is 'UTC'.</li> </ul>"},{"location":"API/particula_beta/data/loader/#returns_15","title":"Returns","text":"<p>Tuple (np.ndarray, np.ndarray):     - <code>-</code> <code>epoch_time</code> - A 1-D numpy array of epoch times.     - <code>-</code> <code>data_array</code> - A 2-D numpy array of data values.</p>"},{"location":"API/particula_beta/data/loader/#raises_5","title":"Raises","text":"<ul> <li><code>ValueError</code> - If the data is not in the expected format or     if no matching data value is found.</li> </ul>"},{"location":"API/particula_beta/data/loader/#signature_15","title":"Signature","text":"<pre><code>def sample_data(\n    data: List[str],\n    time_column: Union[int, List[int]],\n    time_format: str,\n    data_columns: List[int],\n    delimiter: str,\n    date_offset: Optional[str] = None,\n    seconds_shift: int = 0,\n    timezone_identifier: str = \"UTC\",\n) -&gt; Tuple[np.ndarray, np.ndarray]: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#save_lake","title":"save_lake","text":"<p>Show source in loader.py:884</p> <p>Save each stream in the lake as separate pickle files.</p>"},{"location":"API/particula_beta/data/loader/#arguments_15","title":"Arguments","text":"<ul> <li><code>path</code> - Path to save pickle files.</li> <li><code>lake</code> - Lake object to be saved.</li> <li><code>suffix_name</code> - Suffix to add to pickle file names. The default is None.</li> <li><code>folder</code> - Folder to save pickle files. The default is 'output'.</li> </ul>"},{"location":"API/particula_beta/data/loader/#signature_16","title":"Signature","text":"<pre><code>def save_lake(\n    path: str, lake: Lake, suffix_name: Optional[str] = None, folder: str = \"output\"\n) -&gt; None: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#see-also_2","title":"See also","text":"<ul> <li>Lake</li> </ul>"},{"location":"API/particula_beta/data/loader/#save_stream","title":"save_stream","text":"<p>Show source in loader.py:796</p> <p>Save stream object as a pickle file.</p>"},{"location":"API/particula_beta/data/loader/#args_1","title":"Args","text":"<p>stream : Stream     Stream object to be saved. path : str     Path to save pickle file. suffix_name : str, optional     Suffix to add to pickle file name. The default is None.</p>"},{"location":"API/particula_beta/data/loader/#signature_17","title":"Signature","text":"<pre><code>def save_stream(\n    path: str, stream: Stream, suffix_name: Optional[str] = None, folder: str = \"output\"\n) -&gt; None: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#see-also_3","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/loader/#save_stream_to_csv","title":"save_stream_to_csv","text":"<p>Show source in loader.py:723</p> <p>Save stream object as a CSV file, with an option to include formatted time.</p>"},{"location":"API/particula_beta/data/loader/#arguments_16","title":"Arguments","text":"<p>stream : Stream     Stream object to be saved. path : str     Path where the CSV file will be saved. suffix_name : str, optional     Suffix to add to CSV file name. The default is None. folder : str, optional     Subfolder within path to save the CSV file. The default is 'output'. include_time : bool, optional     Whether to include time data in the first column. The default is True. include_iso_datatime : bool, optional     Whether to include ISO formatted datetime in the second column.     The default is True. The format is ISO 8601,     '2021-01-01T00:00:00Z'.</p>"},{"location":"API/particula_beta/data/loader/#signature_18","title":"Signature","text":"<pre><code>def save_stream_to_csv(\n    stream: Stream,\n    path: str,\n    suffix_name: Optional[str] = None,\n    folder: str = \"output\",\n    include_time: bool = True,\n) -&gt; None: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader/#see-also_4","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/loader/#sizer_data_formatter","title":"sizer_data_formatter","text":"<p>Show source in loader.py:558</p> <p>Format data from a particle sizer into structured arrays.</p>"},{"location":"API/particula_beta/data/loader/#arguments_17","title":"Arguments","text":"<ul> <li><code>data</code> - List of raw data strings to be formatted.</li> <li><code>data_checks</code> - Dictionary specifying validation rules for the data.</li> <li><code>data_sizer_reader</code> - Dictionary containing mappings for interpreting     the sizer data format.</li> <li><code>time_column</code> - Index or list of indices indicating the position of     the time column(s) in the data.</li> <li><code>time_format</code> - Format string for parsing time information in the data.</li> <li><code>delimiter</code> - Delimiter used to separate values in the data.     Default is ','.</li> <li><code>header_row</code> - Row index of the header containing column names.     Default is 0.</li> <li><code>date_offset</code> - Optional string representing an offset to add to     timestamps. Default is None.</li> <li><code>seconds_shift</code> - Number of seconds to shift the timestamps.     Default is 0.</li> <li><code>timezone_identifier</code> - Timezone identifier for the data timestamps.     Default is 'UTC'.</li> </ul>"},{"location":"API/particula_beta/data/loader/#returns_16","title":"Returns","text":"<p>Tuple(np.ndarray, np.ndarray, list):     - A numpy array of epoch times.     - A numpy array of Dp header values.     - A list of numpy arrays representing the data.</p>"},{"location":"API/particula_beta/data/loader/#signature_19","title":"Signature","text":"<pre><code>def sizer_data_formatter(\n    data: List[str],\n    data_checks: Dict[str, Any],\n    data_sizer_reader: Dict[str, str],\n    time_column: Union[int, List[int]],\n    time_format: str,\n    delimiter: str = \",\",\n    header_row: int = 0,\n    date_offset: Optional[str] = None,\n    seconds_shift: int = 0,\n    timezone_identifier: str = \"UTC\",\n) -&gt; Tuple[np.ndarray, np.ndarray, list]: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_interface/","title":"Loader Interface","text":"<p>Particula-beta Index / Particula Beta / Data / Loader Interface</p> <p>Auto-generated documentation for particula_beta.data.loader_interface module.</p>"},{"location":"API/particula_beta/data/loader_interface/#get_1d_stream","title":"get_1d_stream","text":"<p>Show source in loader_interface.py:226</p> <p>Loads and formats a 1D data stream from a file and initializes or updates a Stream object.</p>"},{"location":"API/particula_beta/data/loader_interface/#arguments","title":"Arguments","text":"<p>file_path : str     The path of the file to load data from. first_pass : bool     Whether this is the first time data is being loaded. If True, the     stream is initialized.     If False, raises an error as only one file can be loaded. settings : dict     A dictionary containing data formatting settings such as data checks,     column names,     time format, delimiter, and timezone information. stream : Stream, optional     An instance of Stream class to be updated with loaded data. Defaults     to a new Stream object.</p>"},{"location":"API/particula_beta/data/loader_interface/#returns","title":"Returns","text":"<p>Stream     The Stream object updated with the loaded data and corresponding time     information.</p>"},{"location":"API/particula_beta/data/loader_interface/#raises","title":"Raises","text":"<p>ValueError     If <code>first_pass</code> is False, indicating data has already been loaded. TypeError     If <code>settings</code> is not a dictionary. FileNotFoundError     If the file specified by <code>file_path</code> does not exist. KeyError     If any required keys are missing in the <code>settings</code> dictionary.</p>"},{"location":"API/particula_beta/data/loader_interface/#signature","title":"Signature","text":"<pre><code>def get_1d_stream(\n    file_path: str,\n    settings: dict,\n    first_pass: bool = True,\n    stream: Optional[Stream] = None,\n) -&gt; Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_interface/#see-also","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/loader_interface/#get_2d_stream","title":"get_2d_stream","text":"<p>Show source in loader_interface.py:344</p> <p>Initializes a 2D stream using the settings in the DataLake object.</p>"},{"location":"API/particula_beta/data/loader_interface/#arguments_1","title":"Arguments","text":"<pre><code>- `key` *str* - The key of the stream to initialise.\n- `path` *str* - The path of the file to load data from.\n- `first_pass` *bool* - Whether this is the first time loading data.\n</code></pre>"},{"location":"API/particula_beta/data/loader_interface/#returns_1","title":"Returns","text":"<pre><code>None.\n</code></pre>"},{"location":"API/particula_beta/data/loader_interface/#signature_1","title":"Signature","text":"<pre><code>def get_2d_stream(\n    file_path: str,\n    settings: dict,\n    first_pass: bool = True,\n    stream: Optional[Stream] = None,\n) -&gt; Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_interface/#see-also_1","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/loader_interface/#get_netcdf_stream","title":"get_netcdf_stream","text":"<p>Show source in loader_interface.py:437</p> <p>Initialise a netcdf stream using the settings in the DataLake object. This can load either 1D or 2D data, as specified in the settings.</p>"},{"location":"API/particula_beta/data/loader_interface/#arguments_2","title":"Arguments","text":"<pre><code>- `key` *str* - The key of the stream to initialise.\n- `path` *str* - The path of the file to load data from.\n- `first_pass` *bool* - Whether this is the first time loading data.\n</code></pre>"},{"location":"API/particula_beta/data/loader_interface/#returns_2","title":"Returns","text":"<pre><code>None.\n</code></pre>"},{"location":"API/particula_beta/data/loader_interface/#signature_2","title":"Signature","text":"<pre><code>def get_netcdf_stream(\n    file_path: str,\n    settings: dict,\n    first_pass: bool = True,\n    stream: Optional[Stream] = None,\n) -&gt; Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_interface/#see-also_2","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/loader_interface/#get_new_files","title":"get_new_files","text":"<p>Show source in loader_interface.py:14</p> <p>Scan a directory for new files based on import settings and stream status.</p> <p>This function looks for files in a specified path using import settings. It compares the new list of files with a pre-loaded list in the stream object to determine which files are new. The comparison is made based on file names and sizes. It returns a tuple with the paths of new files, a boolean indicating if this was the first pass, and a list of file information for new files.</p>"},{"location":"API/particula_beta/data/loader_interface/#arguments_3","title":"Arguments","text":"<p>path : str     The top-level directory path to scan for files. import_settings : dict     A dictionary with 'relative_data_folder', 'filename_regex',     and 'MIN_SIZE_BYTES' as keys     used to specify the subfolder path and the regex pattern for filtering     file names. It should also include 'min_size' key to specify the     minimum size of the files to be considered. loaded_list : list of lists     A list of lists with file names and sizes that have already been     loaded. The default is None. If None, it will be assumed that no     files have been loaded.</p>"},{"location":"API/particula_beta/data/loader_interface/#returns_3","title":"Returns","text":"<p>tuple of (list, bool, list)     A tuple containing a list of full paths of new files, a boolean     indicating if no previous files were loaded (True if it's the first     pass), and a list of lists with new file names and sizes.</p>"},{"location":"API/particula_beta/data/loader_interface/#raises_1","title":"Raises","text":"<p>YourErrorType     Explanation of when and why your error is raised and what it means.</p>"},{"location":"API/particula_beta/data/loader_interface/#signature_3","title":"Signature","text":"<pre><code>def get_new_files(\n    path: str, import_settings: dict, loaded_list: Optional[list] = None\n) -&gt; tuple: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_interface/#load_files_interface","title":"load_files_interface","text":"<p>Show source in loader_interface.py:112</p> <p>Load files into a stream object based on settings.</p>"},{"location":"API/particula_beta/data/loader_interface/#arguments_4","title":"Arguments","text":"<p>path : str     The top-level directory path to scan for folders of data. folder_settings : dict     A dictionary with keys corresponding to the stream names and values     corresponding to the settings for each stream. The settings can     be generated using the settings_generator function. stream : Stream, optional     An instance of Stream class to be updated with loaded data. Defaults     to a new Stream object. - <code>sub_sample</code> - int, optional     sub-sample only the first n files. Defaults to None.</p>"},{"location":"API/particula_beta/data/loader_interface/#returns_4","title":"Returns","text":"<p>Stream     The Stream object updated with the loaded data.</p>"},{"location":"API/particula_beta/data/loader_interface/#signature_4","title":"Signature","text":"<pre><code>def load_files_interface(\n    path: str,\n    settings: dict,\n    stream: Optional[Stream] = None,\n    sub_sample: Optional[int] = None,\n) -&gt; Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_interface/#see-also_3","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/loader_interface/#load_folders_interface","title":"load_folders_interface","text":"<p>Show source in loader_interface.py:188</p> <p>Load files into a lake object based on settings.</p>"},{"location":"API/particula_beta/data/loader_interface/#arguments_5","title":"Arguments","text":"<p>path : str     The top-level directory path to scan for folders of data. folder_settings : dict     A dictionary with keys corresponding to the stream names and values     corresponding to the settings for each stream. The settings can     be generated using the settings_generator function. lake : Lake, optional     An instance of Lake class to be updated with loaded data. Defaults     to a new Lake object.</p>"},{"location":"API/particula_beta/data/loader_interface/#returns_5","title":"Returns","text":"<p>Lake     The Lake object updated with the loaded data streams.</p>"},{"location":"API/particula_beta/data/loader_interface/#signature_5","title":"Signature","text":"<pre><code>def load_folders_interface(\n    path: str, folder_settings: dict, lake: Optional[Lake] = None\n) -&gt; Lake: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_interface/#see-also_4","title":"See also","text":"<ul> <li>Lake</li> </ul>"},{"location":"API/particula_beta/data/loader_setting_builders/","title":"Loader Setting Builders","text":"<p>Particula-beta Index / Particula Beta / Data / Loader Setting Builders</p> <p>Auto-generated documentation for particula_beta.data.loader_setting_builders module.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#datachecksbuilder","title":"DataChecksBuilder","text":"<p>Show source in loader_setting_builders.py:107</p> <p>Builder class for constructing the data checks dictionary.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature","title":"Signature","text":"<pre><code>class DataChecksBuilder(\n    BuilderABC,\n    ChecksCharactersMixin,\n    ChecksCharCountsMixin,\n    ChecksReplaceCharsMixin,\n    ChecksSkipRowsMixin,\n    ChecksSkipEndMixin,\n):\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#see-also","title":"See also","text":"<ul> <li>ChecksCharCountsMixin</li> <li>ChecksCharactersMixin</li> <li>ChecksReplaceCharsMixin</li> <li>ChecksSkipEndMixin</li> <li>ChecksSkipRowsMixin</li> </ul>"},{"location":"API/particula_beta/data/loader_setting_builders/#datachecksbuilderbuild","title":"DataChecksBuilder().build","text":"<p>Show source in loader_setting_builders.py:132</p> <p>Build and return the data checks dictionary.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_1","title":"Signature","text":"<pre><code>def build(self) -&gt; Dict[str, Any]: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#datelocationbuilder","title":"DateLocationBuilder","text":"<p>Show source in loader_setting_builders.py:144</p> <p>Builder class for non standard date location.</p> <p>For example the there only one date, at the start of the file, and only time is in the rows.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_2","title":"Signature","text":"<pre><code>class DateLocationBuilder(BuilderABC, DelimiterMixin):\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#see-also_1","title":"See also","text":"<ul> <li>DelimiterMixin</li> </ul>"},{"location":"API/particula_beta/data/loader_setting_builders/#datelocationbuilderbuild","title":"DateLocationBuilder().build","text":"<p>Show source in loader_setting_builders.py:205</p> <p>Build and return the non standard date location dictionary.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_3","title":"Signature","text":"<pre><code>def build(self) -&gt; Dict[str, Any]: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#datelocationbuilderset_index","title":"DateLocationBuilder().set_index","text":"<p>Show source in loader_setting_builders.py:191</p> <p>Set the index for the date location.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#arguments","title":"Arguments","text":"<ul> <li><code>index</code> - The index number where the date is located, after splitting based on the delimiter. \"sampling, 02/01/2023, active\" will be date at index 1.</li> </ul>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_4","title":"Signature","text":"<pre><code>def set_index(self, index: int): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#datelocationbuilderset_method","title":"DateLocationBuilder().set_method","text":"<p>Show source in loader_setting_builders.py:167</p> <p>Set the method for the date location.</p> <p>Agrs:     method: The current methods are \"file_header_block\"</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_5","title":"Signature","text":"<pre><code>def set_method(self, method: str): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#datelocationbuilderset_row","title":"DateLocationBuilder().set_row","text":"<p>Show source in loader_setting_builders.py:179</p> <p>Set the row for the date location.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#arguments_1","title":"Arguments","text":"<ul> <li><code>row</code> - The row number where the date is located.</li> </ul>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_6","title":"Signature","text":"<pre><code>def set_row(self, row: int): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#loader1dsettingsbuilder","title":"Loader1DSettingsBuilder","text":"<p>Show source in loader_setting_builders.py:36</p> <p>Builder class for creating settings for loading data from NetCDF files.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_7","title":"Signature","text":"<pre><code>class Loader1DSettingsBuilder(\n    BuilderABC,\n    RelativeFolderMixin,\n    FilenameRegexMixin,\n    FileMinSizeBytesMixin,\n    HeaderRowMixin,\n    DataChecksMixin,\n    DataColumnMixin,\n    DataHeaderMixin,\n    TimeColumnMixin,\n    TimeFormatMixin,\n    DelimiterMixin,\n    TimeShiftSecondsMixin,\n    TimezoneIdentifierMixin,\n    DateLocationMixin,\n):\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#see-also_2","title":"See also","text":"<ul> <li>DataChecksMixin</li> <li>DataColumnMixin</li> <li>DataHeaderMixin</li> <li>DateLocationMixin</li> <li>DelimiterMixin</li> <li>FileMinSizeBytesMixin</li> <li>FilenameRegexMixin</li> <li>HeaderRowMixin</li> <li>RelativeFolderMixin</li> <li>TimeColumnMixin</li> <li>TimeFormatMixin</li> <li>TimeShiftSecondsMixin</li> <li>TimezoneIdentifierMixin</li> </ul>"},{"location":"API/particula_beta/data/loader_setting_builders/#loader1dsettingsbuilderbuild","title":"Loader1DSettingsBuilder().build","text":"<p>Show source in loader_setting_builders.py:84</p> <p>Build and return the settings dictionary for NetCDF data loading.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_8","title":"Signature","text":"<pre><code>def build(self) -&gt; Dict[str, Any]: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#loadersizersettingsbuilder","title":"LoaderSizerSettingsBuilder","text":"<p>Show source in loader_setting_builders.py:243</p> <p>Builder class for creating settings for loading and checking sizer 1D and 2D data from CSV files.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_9","title":"Signature","text":"<pre><code>class LoaderSizerSettingsBuilder(\n    BuilderABC,\n    RelativeFolderMixin,\n    FilenameRegexMixin,\n    FileMinSizeBytesMixin,\n    HeaderRowMixin,\n    DataChecksMixin,\n    DataColumnMixin,\n    DataHeaderMixin,\n    TimeColumnMixin,\n    TimeFormatMixin,\n    DelimiterMixin,\n    TimeShiftSecondsMixin,\n    TimezoneIdentifierMixin,\n    SizerDataReaderMixin,\n    DateLocationMixin,\n):\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#see-also_3","title":"See also","text":"<ul> <li>DataChecksMixin</li> <li>DataColumnMixin</li> <li>DataHeaderMixin</li> <li>DateLocationMixin</li> <li>DelimiterMixin</li> <li>FileMinSizeBytesMixin</li> <li>FilenameRegexMixin</li> <li>HeaderRowMixin</li> <li>RelativeFolderMixin</li> <li>SizerDataReaderMixin</li> <li>TimeColumnMixin</li> <li>TimeFormatMixin</li> <li>TimeShiftSecondsMixin</li> <li>TimezoneIdentifierMixin</li> </ul>"},{"location":"API/particula_beta/data/loader_setting_builders/#loadersizersettingsbuilderbuild","title":"LoaderSizerSettingsBuilder().build","text":"<p>Show source in loader_setting_builders.py:296</p> <p>Build and return the two dictionaries for 1D and 2D sizer data loading .</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_10","title":"Signature","text":"<pre><code>def build(self) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#netcdfreader1dbuilder","title":"NetcdfReader1dBuilder","text":"<p>Show source in loader_setting_builders.py:338</p> <p>Builder class for constructing the NetCDF 1D reader dictionary.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_11","title":"Signature","text":"<pre><code>class NetcdfReader1dBuilder(BuilderABC):\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#netcdfreader1dbuilderbuild","title":"NetcdfReader1dBuilder().build","text":"<p>Show source in loader_setting_builders.py:366</p> <p>Build and return the NetCDF reader dictionary.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_12","title":"Signature","text":"<pre><code>def build(self) -&gt; Dict[str, Any]: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#netcdfreader1dbuilderset_data_1d","title":"NetcdfReader1dBuilder().set_data_1d","text":"<p>Show source in loader_setting_builders.py:352</p> <p>Set the data headers to read from the NetCDF file.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_13","title":"Signature","text":"<pre><code>def set_data_1d(self, data_1d: list[str] | str): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#netcdfreader1dbuilderset_header_1d","title":"NetcdfReader1dBuilder().set_header_1d","text":"<p>Show source in loader_setting_builders.py:359</p> <p>Set the header for 1D data for the Stream file.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_14","title":"Signature","text":"<pre><code>def set_header_1d(self, header_1d: list[str] | str): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#netcdfreader2dbuilder","title":"NetcdfReader2dBuilder","text":"<p>Show source in loader_setting_builders.py:376</p> <p>Builder class for constructing the NetCDF 2D reader dictionary.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_15","title":"Signature","text":"<pre><code>class NetcdfReader2dBuilder(BuilderABC):\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#netcdfreader2dbuilderbuild","title":"NetcdfReader2dBuilder().build","text":"<p>Show source in loader_setting_builders.py:406</p> <p>Build and return the NetCDF reader dictionary.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_16","title":"Signature","text":"<pre><code>def build(self) -&gt; Dict[str, Any]: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#netcdfreader2dbuilderset_data_2d","title":"NetcdfReader2dBuilder().set_data_2d","text":"<p>Show source in loader_setting_builders.py:390</p> <p>Set the data headers for 2D data in the NetCDF file.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_17","title":"Signature","text":"<pre><code>def set_data_2d(self, data_2d: list[str] | str): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#netcdfreader2dbuilderset_header_2d","title":"NetcdfReader2dBuilder().set_header_2d","text":"<p>Show source in loader_setting_builders.py:397</p> <p>Set the header for 2D data for the Stream file. set to str(\"None\") to used a 0-nth index for the header.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_18","title":"Signature","text":"<pre><code>def set_header_2d(self, header_2d: list[str] | str): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#netcdfsettingsbuilder","title":"NetcdfSettingsBuilder","text":"<p>Show source in loader_setting_builders.py:417</p> <p>Builder class for creating settings for loading and checking data from ARM NetCDF files.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_19","title":"Signature","text":"<pre><code>class NetcdfSettingsBuilder(\n    BuilderABC,\n    RelativeFolderMixin,\n    FilenameRegexMixin,\n    FileMinSizeBytesMixin,\n    TimeColumnMixin,\n    TimeFormatMixin,\n    TimeShiftSecondsMixin,\n    TimezoneIdentifierMixin,\n):\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#see-also_4","title":"See also","text":"<ul> <li>FileMinSizeBytesMixin</li> <li>FilenameRegexMixin</li> <li>RelativeFolderMixin</li> <li>TimeColumnMixin</li> <li>TimeFormatMixin</li> <li>TimeShiftSecondsMixin</li> <li>TimezoneIdentifierMixin</li> </ul>"},{"location":"API/particula_beta/data/loader_setting_builders/#netcdfsettingsbuilderbuild","title":"NetcdfSettingsBuilder().build","text":"<p>Show source in loader_setting_builders.py:462</p> <p>Build and return the settings dictionary for 1D data loading.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_20","title":"Signature","text":"<pre><code>def build(self) -&gt; Dict[str, Any]: ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#netcdfsettingsbuilderset_netcdf_reader","title":"NetcdfSettingsBuilder().set_netcdf_reader","text":"<p>Show source in loader_setting_builders.py:452</p> <p>Set the NetCDF reader settings.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_21","title":"Signature","text":"<pre><code>def set_netcdf_reader(self, netcdf_reader: Dict[str, Any]): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#sizerdatareaderbuilder","title":"SizerDataReaderBuilder","text":"<p>Show source in loader_setting_builders.py:215</p> <p>Builder class for constructing the sizer data reader dictionary.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_22","title":"Signature","text":"<pre><code>class SizerDataReaderBuilder(\n    BuilderABC,\n    SizerConcentrationConvertFromMixin,\n    SizerStartKeywordMixin,\n    SizerEndKeywordMixin,\n):\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/loader_setting_builders/#see-also_5","title":"See also","text":"<ul> <li>SizerConcentrationConvertFromMixin</li> <li>SizerEndKeywordMixin</li> <li>SizerStartKeywordMixin</li> </ul>"},{"location":"API/particula_beta/data/loader_setting_builders/#sizerdatareaderbuilderbuild","title":"SizerDataReaderBuilder().build","text":"<p>Show source in loader_setting_builders.py:233</p> <p>Build and return the sizer data reader dictionary.</p>"},{"location":"API/particula_beta/data/loader_setting_builders/#signature_23","title":"Signature","text":"<pre><code>def build(self) -&gt; Dict[str, Any]: ...\n</code></pre>"},{"location":"API/particula_beta/data/merger/","title":"Merger","text":"<p>Particula-beta Index / Particula Beta / Data / Merger</p> <p>Auto-generated documentation for particula_beta.data.merger module.</p>"},{"location":"API/particula_beta/data/merger/#combine_data","title":"combine_data","text":"<p>Show source in merger.py:17</p> <p>\" Merge or adds processed data together. Accounts for data shape miss matches and duplicate timestamps. If the data is a different shape than the existing data, it will be reshaped to match the existing data.</p>"},{"location":"API/particula_beta/data/merger/#arguments","title":"Arguments","text":"<p>data : np.array     Existing data stream. time : np.array     Time array for the existing data. header_list : List[str]     List of headers for the existing data. data_new : np.array     Processed data to add to the data stream. time_new : np.array     Time array for the new data. header_new : List[str]     List of headers for the new data.</p>"},{"location":"API/particula_beta/data/merger/#returns","title":"Returns","text":"<p>Tuple[np.array, List[str], Dict[str, int]]     A tuple containing the updated data stream, the updated header list,     and     a dictionary mapping the header names to their corresponding indices in     the data stream.</p>"},{"location":"API/particula_beta/data/merger/#signature","title":"Signature","text":"<pre><code>def combine_data(\n    data: np.ndarray,\n    time: np.ndarray,\n    header_list: list,\n    data_new: np.ndarray,\n    time_new: np.ndarray,\n    header_new: list,\n) -&gt; Tuple[np.ndarray, list]: ...\n</code></pre>"},{"location":"API/particula_beta/data/merger/#stream_add_data","title":"stream_add_data","text":"<p>Show source in merger.py:100</p> <p>Adds a new data stream and corresponding time stream to the existing data.</p>"},{"location":"API/particula_beta/data/merger/#arguments_1","title":"Arguments","text":"<p>stream : object     A Stream object, containing the existing data. new_time : np.ndarray (m,)     An array of time values for the new data stream. new_data : np.ndarray     An array of data values for the new data stream. header_check : bool, optional     If True, checks whether the header in the new data matches the     header in the existing data. Defaults to False. new_header : list of str, optional     A list of header names for the new data stream. Required if     header_check is True.</p>"},{"location":"API/particula_beta/data/merger/#returns_1","title":"Returns","text":"<p>stream : object     A Stream object, containing the updated data.</p>"},{"location":"API/particula_beta/data/merger/#raises","title":"Raises","text":"<p>ValueError     If header_check is True and header is not provided or     header does not match the existing header.</p>"},{"location":"API/particula_beta/data/merger/#notes","title":"Notes","text":"<p>If header_check is True, the method checks whether the header in the new data matches the header in the existing data. If they do not match, the method attempts to merge the headers and updates the header dictionary.</p> <p>If header_check is False or the headers match, the new data is appended to the existing data.</p> <p>The function also checks whether the time stream is increasing, and if not, sorts the time stream and corresponding data.</p>"},{"location":"API/particula_beta/data/merger/#signature_1","title":"Signature","text":"<pre><code>def stream_add_data(\n    stream: Stream,\n    time_new: np.ndarray,\n    data_new: np.ndarray,\n    header_check: Optional[bool] = False,\n    header_new: Optional[list] = None,\n) -&gt; Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/merger/#see-also","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/mixin/","title":"Mixin","text":"<p>Particula-beta Index / Particula Beta / Data / Mixin</p> <p>Auto-generated documentation for particula_beta.data.mixin module.</p>"},{"location":"API/particula_beta/data/mixin/#checkscharcountsmixin","title":"ChecksCharCountsMixin","text":"<p>Show source in mixin.py:429</p> <p>Mixin class for setting the character counts for data checks.</p>"},{"location":"API/particula_beta/data/mixin/#signature","title":"Signature","text":"<pre><code>class ChecksCharCountsMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#checkscharcountsmixinset_char_counts","title":"ChecksCharCountsMixin().set_char_counts","text":"<p>Show source in mixin.py:435</p> <p>Set the required character counts for the data checks. This is the number of times a character should appear in a line of the data file, for it to be considered valid, and proceed with data parsing.</p>"},{"location":"API/particula_beta/data/mixin/#arguments","title":"Arguments","text":"<ul> <li><code>char_counts</code> - Dictionary of characters and their required counts     for the data checks. The keys are the characters, and the     values are the required counts. e.g. {\",\": 4, \":\": 0}.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples","title":"Examples","text":"Set number of commas<pre><code>char_counts = {\",\": 4}\n# valid line: '1,2,3,4'\n# invalid line removed: '1,2,3'\n</code></pre> Filter out specific words<pre><code>char_counts = {\"Temp1 Error\": 0}\n# valid line: '23.4, 0.1, 0.2, no error'\n# invalid line removed: '23.4, 0.1, 0.2, Temp1 Error'\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#signature_1","title":"Signature","text":"<pre><code>def set_char_counts(self, char_counts: dict[str, int]): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#checkscharactersmixin","title":"ChecksCharactersMixin","text":"<p>Show source in mixin.py:394</p> <p>Mixin class for setting the character length range for data checks.</p>"},{"location":"API/particula_beta/data/mixin/#signature_2","title":"Signature","text":"<pre><code>class ChecksCharactersMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#checkscharactersmixinset_characters","title":"ChecksCharactersMixin().set_characters","text":"<p>Show source in mixin.py:400</p> <p>Set the character length range for the data checks. This is how many characters are expected a line of the data file, for it to be considered valid, and proceed with data parsing.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_1","title":"Arguments","text":"<ul> <li><code>characters</code> - List of one (or two) integers for the minimum (and     maximum) number of characters expected in a line of the data     file. e.g. [10, 100] for 10 to 100 characters. or [10] for     10 or more characters.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_1","title":"Examples","text":"Set minimum characters<pre><code>characters = [5]\n# valid line: '1,2,3,4,5'\n# invalid line: '1,2'\n</code></pre> Set range of characters<pre><code>characters = [5, 10]\n# valid line: '1,2,3,4,5'\n# invalid line: '1,2,3,4,5,6,7,8,9,10,11'\n# invalid line: '1,2'\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#signature_3","title":"Signature","text":"<pre><code>def set_characters(self, characters: list[int]): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#checksreplacecharsmixin","title":"ChecksReplaceCharsMixin","text":"<p>Show source in mixin.py:507</p> <p>Mixin class for setting the characters to replace in the data lines.</p>"},{"location":"API/particula_beta/data/mixin/#signature_4","title":"Signature","text":"<pre><code>class ChecksReplaceCharsMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#checksreplacecharsmixinset_replace_chars","title":"ChecksReplaceCharsMixin().set_replace_chars","text":"<p>Show source in mixin.py:513</p> <p>Set the characters to replace in the data lines.</p> <p>This is useful to replace unwanted characters from the data lines before converting the data to the required format. Each key in the replace_dict represents the character to replace, and the corresponding value is the replacement target.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_2","title":"Arguments","text":"<ul> <li><code>replace_dict</code> dict - Dictionary with keys as characters to replace     and values as the replacement targets.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_2","title":"Examples","text":"Replace brackets with empty string<pre><code>replace_dict = {\"[\": \"\", \"]\": \"\"}\n# data: '[1], [2], [3]' -&gt; '1, 2, 3'\n</code></pre> Replace spaces with underscores<pre><code>replace_dict = {\" \": \"_\"}\n# data: '1, 2, 3' -&gt; '1,_2,_3'\n</code></pre> Replace multiple characters<pre><code>replace_dict = {\"[\": \"\", \"]\": \"\", \"\\n\": \" \"}\n# data: '[1]\\n[2]\\n[3]' -&gt; '1 2 3'\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#returns","title":"Returns","text":"<ul> <li><code>self</code> - The instance of the class to allow for method chaining.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#references","title":"References","text":"<p>Python str.replace</p>"},{"location":"API/particula_beta/data/mixin/#signature_5","title":"Signature","text":"<pre><code>def set_replace_chars(self, replace_chars: dict[str, str]): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#checksskipendmixin","title":"ChecksSkipEndMixin","text":"<p>Show source in mixin.py:485</p> <p>Mixin class for setting the number of rows to skip at the end.</p>"},{"location":"API/particula_beta/data/mixin/#signature_6","title":"Signature","text":"<pre><code>class ChecksSkipEndMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#checksskipendmixinset_skip_end","title":"ChecksSkipEndMixin().set_skip_end","text":"<p>Show source in mixin.py:491</p> <p>Set the number of rows to skip at the end of the file.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_3","title":"Arguments","text":"<ul> <li><code>skip_end</code> int - Number of rows to skip at the end of the file.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_3","title":"Examples","text":"Skip last row<pre><code>skip_end = 10\n# Skip the last 10 row of the file.\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#signature_7","title":"Signature","text":"<pre><code>def set_skip_end(self, skip_end: int = 0): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#checksskiprowsmixin","title":"ChecksSkipRowsMixin","text":"<p>Show source in mixin.py:462</p> <p>Mixin class for setting the number of rows to skip at the beginning.</p>"},{"location":"API/particula_beta/data/mixin/#signature_8","title":"Signature","text":"<pre><code>class ChecksSkipRowsMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#checksskiprowsmixinset_skip_rows","title":"ChecksSkipRowsMixin().set_skip_rows","text":"<p>Show source in mixin.py:468</p> <p>Set the number of rows to skip at the beginning of the file.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_4","title":"Arguments","text":"<ul> <li><code>skip_rows</code> int - Number of rows to skip at the beginning of the     file.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_4","title":"Examples","text":"Skip the first 2 rows<pre><code>skip_rows = 2\n# Skip the first 2 rows of the file.\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#signature_9","title":"Signature","text":"<pre><code>def set_skip_rows(self, skip_rows: int = 0): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#datachecksmixin","title":"DataChecksMixin","text":"<p>Show source in mixin.py:120</p> <p>Mixin class for setting the data checks.</p>"},{"location":"API/particula_beta/data/mixin/#signature_10","title":"Signature","text":"<pre><code>class DataChecksMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#datachecksmixinset_data_checks","title":"DataChecksMixin().set_data_checks","text":"<p>Show source in mixin.py:126</p> <p>Dictionary of data checks to perform on the loaded data.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_5","title":"Arguments","text":"<ul> <li><code>checks</code> dict - Dictionary of data checks to perform on the loaded     data. The keys are the names of the checks, and the values are     the parameters for the checks.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#signature_11","title":"Signature","text":"<pre><code>def set_data_checks(self, data_checks: Dict[str, Any]): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#datacolumnmixin","title":"DataColumnMixin","text":"<p>Show source in mixin.py:138</p> <p>Mixin class for setting the data column.</p>"},{"location":"API/particula_beta/data/mixin/#signature_12","title":"Signature","text":"<pre><code>class DataColumnMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#datacolumnmixinset_data_column","title":"DataColumnMixin().set_data_column","text":"<p>Show source in mixin.py:144</p> <p>The data columns for the data files to load. Build with <code>DataChecksBuilder</code>.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_6","title":"Arguments","text":"<ul> <li><code>data_columns</code> - List of column numbers or names for the data columns     to load from the data files. The columns are indexed from 0.     e.g. [3, 5] or ['data 1', 'data 3'].</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_5","title":"Examples","text":"Single data column, index<pre><code>data_columns = [3]\n# header: 'Time, Temp, data 1, data 2, data 3'\n# line: '2021-01-01T12:00:00, 25.8, 1.2, 3.4' # load 1.2\n</code></pre> Single data column, name<pre><code>data_columns = ['data 1']\n# header: 'Time, Temp, data 1, data 3, data 5'\n# line: '2021-01-01T12:00:00, 25.8, 1.2, 3.4' # load 25.8\n</code></pre> Multiple data columns, index<pre><code>data_columns = [1, 3]\n# header: 'Time, Temp, data 1, data 3, data 5'\n# line: '2021-01-01T12:00:00, 25.8, 1.2, 3.4' # load 25.8, 3.4\n</code></pre> Multiple data columns, name<pre><code>data_columns = ['Temp', 'data 3']\n# header: 'Time, Temp, data 1, data 3, data 5'\n# line: '2021-01-01T12:00:00, 25.8, 1.2, 3.4' # load 25.8, 3.4\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#signature_13","title":"Signature","text":"<pre><code>def set_data_column(self, data_columns: Union[List[str], List[int]]): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#dataheadermixin","title":"DataHeaderMixin","text":"<p>Show source in mixin.py:182</p> <p>Mixin class for setting the data header for the Stream.</p>"},{"location":"API/particula_beta/data/mixin/#signature_14","title":"Signature","text":"<pre><code>class DataHeaderMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#dataheadermixinset_data_header","title":"DataHeaderMixin().set_data_header","text":"<p>Show source in mixin.py:188</p> <p>Set the Stream headers corresponding to the data columns. This is to improve the readability of the Stream data. The headers should be in the same order as the data columns. These are also the same headers that will be written to the output file or csv.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_7","title":"Arguments","text":"<ul> <li><code>headers</code> - List of headers corresponding to the data     columns to load. e.g. ['data-1[m/s]', 'data_3[L]'].</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_6","title":"Examples","text":"Single header<pre><code>headers = ['data-1[m/s]']\n# Name the only data column as 'data-1[m/s]'.\n</code></pre> Multiple headers<pre><code>headers = ['data-1[m/s]', 'data-3[L]']\n# Name the data columns as 'data-1[m/s]' and 'data-3[L]'.\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#signature_15","title":"Signature","text":"<pre><code>def set_data_header(self, headers: List[str]): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#datelocationmixin","title":"DateLocationMixin","text":"<p>Show source in mixin.py:670</p> <p>Mixin class for setting the location of the date in the data files. When the date is not in each row of the data file, but in a different location, e.g. in the first row of the file only.</p>"},{"location":"API/particula_beta/data/mixin/#signature_16","title":"Signature","text":"<pre><code>class DateLocationMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#datelocationmixinset_date_location","title":"DateLocationMixin().set_date_location","text":"<p>Show source in mixin.py:679</p> <p>Set the location of the date in the data files.</p> <p>Created dictionary from <code>DateLocationBuilder</code>.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_8","title":"Arguments","text":"<ul> <li><code>date_location</code> - Dictionary with the date location settings. The     keys are the names of the settings, and the values are the     parameters for the settings.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_7","title":"Examples","text":"Date location<pre><code>\"date_location\": {\n    \"method\": \"file_header_block\",\n    \"delimiter\": \",\",\n    \"row\": 1,\n    \"index\": 1\n}\n# Date is in the first row and first column of the file.\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#signature_17","title":"Signature","text":"<pre><code>def set_date_location(self, date_location: Dict[str, Any]): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#delimitermixin","title":"DelimiterMixin","text":"<p>Show source in mixin.py:296</p> <p>Mixin class for setting the delimiter.</p>"},{"location":"API/particula_beta/data/mixin/#signature_18","title":"Signature","text":"<pre><code>class DelimiterMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#delimitermixinset_delimiter","title":"DelimiterMixin().set_delimiter","text":"<p>Show source in mixin.py:302</p> <p>Set the delimiter for the data files to load.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_9","title":"Arguments","text":"<ul> <li><code>delimiter</code> str - Delimiter for the data columns in the data files.     e.g. ',' for CSV files or '\\t' for tab-separated files.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_8","title":"Examples","text":"CSV delimiter<pre><code>delimiter = \",\"\n# CSV file with columns separated by commas.\n</code></pre> Tab delimiter<pre><code>delimiter = \"\\t\"\n# Tab-separated file with columns separated by tabs.\n</code></pre> Space delimiter<pre><code>delimiter = \" \"\n# Space-separated file with columns separated by spaces.\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#signature_19","title":"Signature","text":"<pre><code>def set_delimiter(self, delimiter: str): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#fileminsizebytesmixin","title":"FileMinSizeBytesMixin","text":"<p>Show source in mixin.py:74</p> <p>Mixin class for setting the minimum file size in bytes.</p>"},{"location":"API/particula_beta/data/mixin/#signature_20","title":"Signature","text":"<pre><code>class FileMinSizeBytesMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#fileminsizebytesmixinset_file_min_size_bytes","title":"FileMinSizeBytesMixin().set_file_min_size_bytes","text":"<p>Show source in mixin.py:80</p> <p>Set the minimum file size in bytes for the data files to load.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_10","title":"Arguments","text":"<ul> <li><code>size</code> int - Minimum file size in bytes. Default is 10000 bytes.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#signature_21","title":"Signature","text":"<pre><code>def set_file_min_size_bytes(self, size: int = 10000): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#filenameregexmixin","title":"FilenameRegexMixin","text":"<p>Show source in mixin.py:37</p> <p>Mixin class for setting the filename regex.</p>"},{"location":"API/particula_beta/data/mixin/#signature_22","title":"Signature","text":"<pre><code>class FilenameRegexMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#filenameregexmixinset_filename_regex","title":"FilenameRegexMixin().set_filename_regex","text":"<p>Show source in mixin.py:43</p> <p>Set the filename regex for the data files to load.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_11","title":"Arguments","text":"<ul> <li><code>regex</code> str - Regular expression for the filenames, e.g.     'data_*.csv'.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_9","title":"Examples","text":"Match all files<pre><code>regex = \".*\"\n# Match all files in the folder.\n</code></pre> Match CSV files<pre><code>regex = \".*.csv\"\n# Match all CSV files in the folder.\n</code></pre> Match specific files<pre><code>regex = \"data_*.csv\"\n# Match files starting with 'data_' and ending with '.csv'.\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#references_1","title":"References","text":"<p>Explore Regex Python Regex Doc</p>"},{"location":"API/particula_beta/data/mixin/#signature_23","title":"Signature","text":"<pre><code>def set_filename_regex(self, regex: str): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#headerrowmixin","title":"HeaderRowMixin","text":"<p>Show source in mixin.py:90</p> <p>Mixin class for setting the header row.</p>"},{"location":"API/particula_beta/data/mixin/#signature_24","title":"Signature","text":"<pre><code>class HeaderRowMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#headerrowmixinset_header_row","title":"HeaderRowMixin().set_header_row","text":"<p>Show source in mixin.py:96</p> <p>Set the header row for the data files to load.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_12","title":"Arguments","text":"<ul> <li><code>row</code> int - Row number for the header row in the data file, indexed     from 0.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_10","title":"Examples","text":"Header row at the top<pre><code>row = 0\n# line 0: 'Time, Temp, data 1, data 2, data 3'\n</code></pre> Header is third row<pre><code>row = 2\n# line 0: \"Experiment 1\"\n# line 1: \"Date: 2021-01-01\"\n# line 2: 'Time, Temp, data 1, data 2, data 3'\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#signature_25","title":"Signature","text":"<pre><code>def set_header_row(self, row: int): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#relativefoldermixin","title":"RelativeFolderMixin","text":"<p>Show source in mixin.py:8</p> <p>Mixin class for setting the relative data folder.</p>"},{"location":"API/particula_beta/data/mixin/#signature_26","title":"Signature","text":"<pre><code>class RelativeFolderMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#relativefoldermixinset_relative_data_folder","title":"RelativeFolderMixin().set_relative_data_folder","text":"<p>Show source in mixin.py:14</p> <p>Set the relative data folder for the folder with the data loading.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_13","title":"Arguments","text":"<ul> <li><code>folder</code> str - Relative path to the data folder.     e.g. 'data_folder'. Where the data folder is located in     project_path/data_folder.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_11","title":"Examples","text":"Set data folder<pre><code>folder = \"data_folder\"\n# Set the data folder to 'data_folder'.\n</code></pre> Set a subfolder<pre><code>folder = \"subfolder/data_folder\"\n# Set the data folder to 'subfolder/data_folder'.\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#signature_27","title":"Signature","text":"<pre><code>def set_relative_data_folder(self, folder: str): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#sizerconcentrationconvertfrommixin","title":"SizerConcentrationConvertFromMixin","text":"<p>Show source in mixin.py:617</p> <p>Mixin class for setting to convert the sizer concentration to a different scale.</p>"},{"location":"API/particula_beta/data/mixin/#signature_28","title":"Signature","text":"<pre><code>class SizerConcentrationConvertFromMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#sizerconcentrationconvertfrommixinset_sizer_concentration_convert_from","title":"SizerConcentrationConvertFromMixin().set_sizer_concentration_convert_from","text":"<p>Show source in mixin.py:625</p> <p>Set to convert the sizer concentration from dw or (pmf) scale to dN/dlogDp scale.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_14","title":"Arguments","text":"<ul> <li><code>convert_from</code> - Conversion flag to convert the sizer concentration     from dw or (pmf) scale to dN/dlogDp scale. The option is only     \"dw\" all other values are ignored.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_12","title":"Examples","text":"Convert from dw scale<pre><code>convert_from = \"dw\"\n# Convert the sizer concentration from dw scale to dN/dlogDp scale.\n</code></pre> Convert Ignored<pre><code>convert_from = \"pmf\"\n# Ignored, no conversion is performed, when loading the sizer data.\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#signature_29","title":"Signature","text":"<pre><code>def set_sizer_concentration_convert_from(self, convert_from: Optional[str] = None): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#sizerdatareadermixin","title":"SizerDataReaderMixin","text":"<p>Show source in mixin.py:651</p> <p>Mixin class for the dictionary of the sizer data reader settings.</p>"},{"location":"API/particula_beta/data/mixin/#signature_30","title":"Signature","text":"<pre><code>class SizerDataReaderMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#sizerdatareadermixinset_data_sizer_reader","title":"SizerDataReaderMixin().set_data_sizer_reader","text":"<p>Show source in mixin.py:657</p> <p>Dictionary of the sizer data reader settings for the data files. Build with <code>SizerDataReaderBuilder</code>.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_15","title":"Arguments","text":"<ul> <li><code>data_sizer_reader</code> - Dictionary of the sizer data reader settings     for the data files. The keys are the names of the settings,     and the values are the parameters for the settings.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#signature_31","title":"Signature","text":"<pre><code>def set_data_sizer_reader(self, data_sizer_reader: Dict[str, Any]): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#sizerendkeywordmixin","title":"SizerEndKeywordMixin","text":"<p>Show source in mixin.py:586</p> <p>Mixin class for setting the end key for the sizer data.</p>"},{"location":"API/particula_beta/data/mixin/#signature_32","title":"Signature","text":"<pre><code>class SizerEndKeywordMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#sizerendkeywordmixinset_sizer_end_keyword","title":"SizerEndKeywordMixin().set_sizer_end_keyword","text":"<p>Show source in mixin.py:592</p> <p>Set the end keyword for the sizer data, to identify the end of the sizer data block in the data files. This can be a string or an integer (column index) to identify the end of the sizer data block.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_16","title":"Arguments","text":"<ul> <li><code>end_keyword</code> - End key for the sizer data in the data files.     e.g. '789.3' or -3 for the 3<sup>rd</sup> column from the end.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_13","title":"Examples","text":"End key as a string<pre><code>end_key = \"789.3\"\n# header: '... 689.1, 750.2, 789.3, Total Conc, Comments'\n</code></pre> End key as a column index<pre><code>end_key = -3\n# header: '... 689.1, 750.2, 789.3, Total Conc, Comments'\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#signature_33","title":"Signature","text":"<pre><code>def set_sizer_end_keyword(self, end_key: Union[str, int]): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#sizerstartkeywordmixin","title":"SizerStartKeywordMixin","text":"<p>Show source in mixin.py:555</p> <p>Mixin class for setting the start key for the sizer data.</p>"},{"location":"API/particula_beta/data/mixin/#signature_34","title":"Signature","text":"<pre><code>class SizerStartKeywordMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#sizerstartkeywordmixinset_sizer_start_keyword","title":"SizerStartKeywordMixin().set_sizer_start_keyword","text":"<p>Show source in mixin.py:561</p> <p>Set the start keyword for the sizer data, to identify the start of the sizer data block in the data files. This can be a string or an integer (column index) to identify the start of the sizer data block.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_17","title":"Arguments","text":"<ul> <li><code>start_keyword</code> - Start key for the sizer data in the data files.     e.g. '25.8' or 3 for the 4<sup>th</sup> column</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_14","title":"Examples","text":"Start key as a string<pre><code>start_key = \"35.8\"\n# header: 'Time, Temp, 35.8, 36.0, 36.2, ...'\n</code></pre> Start key as a column index<pre><code>start_key = 2\n# header: 'Time, Temp, 35.8, 36.0, 36.2, ...'\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#signature_35","title":"Signature","text":"<pre><code>def set_sizer_start_keyword(self, start_key: Union[str, int]): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#timecolumnmixin","title":"TimeColumnMixin","text":"<p>Show source in mixin.py:213</p> <p>Mixin class for setting the time column.</p>"},{"location":"API/particula_beta/data/mixin/#signature_36","title":"Signature","text":"<pre><code>class TimeColumnMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#timecolumnmixinset_time_column","title":"TimeColumnMixin().set_time_column","text":"<p>Show source in mixin.py:219</p> <p>The time column for the data files to load. The time column is used to convert the time data to an Unix-Epoch timestamp.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_18","title":"Arguments","text":"<ul> <li><code>columns</code> - List of column indexes for the time columns to     load from the data files. The columns are indexed from 0.     e.g. [0] or [1, 2] to combine 1 and 2 columns.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_15","title":"Examples","text":"Single time column<pre><code>columns = [0]\n# Load the time data from the first column.\n# line: '2021-01-01T12:00:00, 1.2, 3.4'\n</code></pre> Multiple time columns<pre><code>columns = [1, 2]\n# Load the time data from the second and third columns.\n# line: '1.2, 2021-01-01, 12:00:00'\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#signature_37","title":"Signature","text":"<pre><code>def set_time_column(self, columns: List[int]): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#timeformatmixin","title":"TimeFormatMixin","text":"<p>Show source in mixin.py:245</p> <p>Mixin class for setting the time format.</p>"},{"location":"API/particula_beta/data/mixin/#signature_38","title":"Signature","text":"<pre><code>class TimeFormatMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#timeformatmixinset_time_format","title":"TimeFormatMixin().set_time_format","text":"<p>Show source in mixin.py:251</p> <p>Set the time format for the time data in the data files.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_19","title":"Arguments","text":"<ul> <li><code>time_format_str</code> str - Time format string for the time data in the     data files. Default is ISO \"%Y-%m-%dT%H:%M:%S\", list \"epoch\"     if the time data is in Unix-Epoch format. Use the Python time     format codes otherwise,     e.g. \"%Y-%m-%dT%H:%M:%S\" for '2021-01-01T12:00:00'.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_16","title":"Examples","text":"USA date format<pre><code>time_format_str = \"%m/%d/%Y %H:%M:%S\"\n# e.g. '01/01/2021 12:00:00'\n</code></pre> European date format<pre><code>time_format_str = \"%d/%m/%Y %H:%M:%S\"\n# e.g. '01/01/2021 12:00:00'\n</code></pre> ISO date format<pre><code>time_format_str = \"%Y-%m-%dT%H:%M:%S\"\n# e.g. '2021-01-01T12:00:00'\n</code></pre> AM/PM time format<pre><code>time_format_str = \"%Y-%m-%d %I:%M:%S %p\"\n# e.g. '2021-01-01 12:00:00 PM'\n</code></pre> Fractional seconds<pre><code>time_format_str = \"%Y-%m-%dT%H:%M:%S.%f\"\n# e.g. '2021-01-01T12:00:00.123456'\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#references_2","title":"References","text":"<ul> <li>Python Docs</li> <li>Python Time Format</li> </ul>"},{"location":"API/particula_beta/data/mixin/#signature_39","title":"Signature","text":"<pre><code>def set_time_format(self, time_format_str: str = \"%Y-%m-%dT%H:%M:%S\"): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#timeshiftsecondsmixin","title":"TimeShiftSecondsMixin","text":"<p>Show source in mixin.py:329</p> <p>Mixin class for setting the time shift in seconds.</p>"},{"location":"API/particula_beta/data/mixin/#signature_40","title":"Signature","text":"<pre><code>class TimeShiftSecondsMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#timeshiftsecondsmixinset_time_shift_seconds","title":"TimeShiftSecondsMixin().set_time_shift_seconds","text":"<p>Show source in mixin.py:335</p> <p>Set the time shift in seconds for the time data in the data files. This is helpful to match the time stamps of two data folders. This shift is applied to all files loaded with this builder.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_20","title":"Arguments","text":"<ul> <li><code>shift</code> int - Time shift in seconds for the time data in the data     files. Default is 0 seconds.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_17","title":"Examples","text":"Shift by 1 hour<pre><code>shift = 3600\n# Shift the time data by 1 hour (3600 seconds).\n</code></pre> Shift by 1 day<pre><code>shift = 86400\n# Shift the time data by 1 day (86400 seconds).\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#signature_41","title":"Signature","text":"<pre><code>def set_time_shift_seconds(self, shift: int = 0): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#timezoneidentifiermixin","title":"TimezoneIdentifierMixin","text":"<p>Show source in mixin.py:359</p> <p>Mixin class for setting the timezone identifier.</p>"},{"location":"API/particula_beta/data/mixin/#signature_42","title":"Signature","text":"<pre><code>class TimezoneIdentifierMixin:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#timezoneidentifiermixinset_timezone_identifier","title":"TimezoneIdentifierMixin().set_timezone_identifier","text":"<p>Show source in mixin.py:365</p> <p>Set the timezone identifier for the time data in the data files. The timezone shift is handled by the pytz library.</p>"},{"location":"API/particula_beta/data/mixin/#arguments_21","title":"Arguments","text":"<ul> <li><code>timezone</code> str - Timezone identifier for the time data in the data     files. Default is 'UTC'.</li> </ul>"},{"location":"API/particula_beta/data/mixin/#examples_18","title":"Examples","text":"List of Timezones<pre><code>timezone = \"Europe/London\"  # or \"GMT\"\n</code></pre> Mountain Timezone<pre><code>timezone = \"America/Denver\"  # or \"MST7MDT\"\n</code></pre> ETH Zurich Timezone<pre><code>timezone = \"Europe/Zurich\"  # or \"CET\"\n</code></pre>"},{"location":"API/particula_beta/data/mixin/#references_3","title":"References","text":"<p>List of Timezones</p>"},{"location":"API/particula_beta/data/mixin/#signature_43","title":"Signature","text":"<pre><code>def set_timezone_identifier(self, timezone: str = \"UTC\"): ...\n</code></pre>"},{"location":"API/particula_beta/data/settings_generator/","title":"Settings Generator","text":"<p>Particula-beta Index / Particula Beta / Data / Settings Generator</p> <p>Auto-generated documentation for particula_beta.data.settings_generator module.</p>"},{"location":"API/particula_beta/data/settings_generator/#for_general_1d_load","title":"for_general_1d_load","text":"<p>Show source in settings_generator.py:14</p> <p>Generate a settings dictionary for loading and checking 1D data from CSV files.</p>"},{"location":"API/particula_beta/data/settings_generator/#arguments","title":"Arguments","text":"<ul> <li>relative_data_folder (str): The folder path relative to the main script     where data files are located. Default is 'instrument_data'.</li> <li>filename_regex (str): Regular expression pattern to match filenames in     the data folder. Default is '*.csv'.</li> <li>file_min_size_bytes (int): Minimum size in bytes for files to be     considered valid. Default is 10.</li> <li>header_row (int): The index of the row containing column headers     (0-indexed). Default is 0.</li> <li>data_checks (Optional[dict]): A dictionary containing data quality     checks such as character length, required character counts, rows to     skip at the beginning or end. Defaults to basic checks if None.</li> <li>data_column (list of int): List of indices for columns containing data     points to be loaded. Default is [3, 5].</li> <li>data_header (List[str]): List of strings representing the header names     for data columns. Default is ['data 1', 'data 3'].</li> <li>time_column (List[int]): List of indices for columns containing time     information. Default is [0, 1].</li> <li>time_format (str): String format for parsing time columns, using     strftime conventions. Default is '%Y-%m-%d %H:%M:%S.%f'.</li> <li>delimiter (str): Character used to separate values in the file.     Default is ','.</li> <li>time_shift_seconds (int): Number of seconds by which to shift time data     (positive or negative). Default is 0.</li> <li>timezone_identifier (str): Timezone identifier for time conversion.     Default is 'UTC'.</li> </ul>"},{"location":"API/particula_beta/data/settings_generator/#returns","title":"Returns","text":"<ul> <li><code>-</code> dict - A dictionary with settings for data loading procedures including     file paths, size requirements, header information, and data check     parameters.</li> </ul>"},{"location":"API/particula_beta/data/settings_generator/#signature","title":"Signature","text":"<pre><code>def for_general_1d_load(\n    relative_data_folder: str = \"instrument_data\",\n    filename_regex: str = \"*.csv\",\n    file_min_size_bytes: int = 10,\n    header_row: int = 0,\n    data_checks: Optional[dict] = None,\n    data_column: list = None,\n    data_header: List[str] = None,\n    time_column: List[int] = None,\n    time_format: str = \"%Y-%m-%d %H:%M:%S.%f\",\n    delimiter: str = \",\",\n    time_shift_seconds: int = 0,\n    timezone_identifier: str = \"UTC\",\n) -&gt; dict: ...\n</code></pre>"},{"location":"API/particula_beta/data/settings_generator/#for_general_sizer_1d_2d_load","title":"for_general_sizer_1d_2d_load","text":"<p>Show source in settings_generator.py:94</p> <p>Generate settings for the 1D general file loader and the 2D general sizer file loader.</p>"},{"location":"API/particula_beta/data/settings_generator/#arguments_1","title":"Arguments","text":"<ul> <li>relative_data_folder (str): Path to the folder containing data files,     relative to the script's location.</li> <li>filename_regex (str): Regex pattern to match filenames for loading.</li> <li>file_min_size_bytes (int): Minimum file size in bytes for a file to be     considered valid for loading.</li> <li>header_row (int): Row index for the header (0-based) in the data files.</li> <li>data_checks (dict, optional): Specifications for data integrity checks     to apply when loading data.</li> <li>data_1d_column (list of int): Column indices for 1D data extraction.</li> <li>data_1d_header (list of str): Header names corresponding to the     <code>data_1d_column</code> indices.</li> <li>data_2d_dp_start_keyword (str): Keyword indicating the start of 2D data     points in a file.</li> <li>data_2d_dp_end_keyword (str): Keyword indicating the end of 2D data     points in a file.</li> <li>data_2d_convert_concentration_from (str, optional): Unit to convert from     if concentration scaling is needed for 2D data.</li> <li>time_column (list of int): Column indices for time data extraction.</li> <li>time_format (str): Format string for parsing time data.</li> <li>delimiter (str): Delimiter character for splitting data in the file.</li> <li>time_shift_seconds (int): Seconds to shift the time data by.</li> <li>timezone_identifier (str): Timezone ID for time data interpretation.</li> </ul>"},{"location":"API/particula_beta/data/settings_generator/#returns_1","title":"Returns","text":"<ul> <li>tuple of (dict, dict): A tuple containing two dictionaries with settings     for the 1D and 2D data loaders.</li> </ul> <p>The function defaults <code>data_checks</code> to basic validation criteria if not     provided. It returns separate dictionaries for settings applicable to     1D and 2D data loaders, which include file paths, size checks, and     data parsing rules.</p>"},{"location":"API/particula_beta/data/settings_generator/#signature_1","title":"Signature","text":"<pre><code>def for_general_sizer_1d_2d_load(\n    relative_data_folder: str = \"instrument_data\",\n    filename_regex: str = \"*.csv\",\n    file_min_size_bytes: int = 10,\n    header_row: int = 0,\n    data_checks: Optional[dict] = None,\n    data_1d_column: list = None,\n    data_1d_header: List[str] = None,\n    data_2d_dp_start_keyword: str = \"Date Time\",\n    data_2d_dp_end_keyword: str = \"Total Conc\",\n    data_2d_convert_concentration_from: str = \"dw/dlogdp\",\n    time_column: List[int] = None,\n    time_format: str = \"%Y-%m-%d %H:%M:%S.%f\",\n    delimiter: str = \",\",\n    time_shift_seconds: int = 0,\n    timezone_identifier: str = \"UTC\",\n) -&gt; tuple: ...\n</code></pre>"},{"location":"API/particula_beta/data/settings_generator/#load_settings_for_lake","title":"load_settings_for_lake","text":"<p>Show source in settings_generator.py:268</p> <p>Load settings for Lake data from a JSON file. The settings file is a dictionary of stream settings dictionaries.</p> <p>Given a path and subfolder, this function searches for a JSON file named 'lake_settings' with an optional suffix. It returns the settings as a dictionary. If no file is found, or multiple files are found, appropriate errors or warnings are raised.</p>"},{"location":"API/particula_beta/data/settings_generator/#arguments_2","title":"Arguments","text":"<ul> <li><code>-</code> path - The path where the subfolder is located.</li> <li><code>-</code> subfolder - The subfolder where the settings file is expected.</li> <li><code>-</code> settings_suffix - An optional suffix for the settings     file name. Default is an empty string.</li> </ul>"},{"location":"API/particula_beta/data/settings_generator/#returns_2","title":"Returns","text":"<ul> <li><code>-</code> dict - A dictionary of settings loaded from the file.</li> </ul>"},{"location":"API/particula_beta/data/settings_generator/#raises","title":"Raises","text":"<ul> <li><code>-</code> FileNotFoundError - If no settings file is found.</li> <li><code>-</code> Warning - If more than one settings file is found.</li> </ul>"},{"location":"API/particula_beta/data/settings_generator/#signature_2","title":"Signature","text":"<pre><code>def load_settings_for_lake(\n    path: str, subfolder: str = \"\", settings_suffix: str = \"\"\n) -&gt; dict: ...\n</code></pre>"},{"location":"API/particula_beta/data/settings_generator/#load_settings_for_stream","title":"load_settings_for_stream","text":"<p>Show source in settings_generator.py:196</p> <p>Load settings for Stream data from a JSON file.</p> <p>Given a path and subfolder, this function searches for a JSON file named 'stream_settings' with an optional suffix. It returns the settings as a dictionary. If no file is found, or multiple files are found, appropriate errors or warnings are raised.</p>"},{"location":"API/particula_beta/data/settings_generator/#arguments_3","title":"Arguments","text":"<ul> <li><code>-</code> path - The path where the subfolder is located.</li> <li><code>-</code> subfolder - The subfolder where the settings file is expected.</li> <li><code>-</code> settings_suffix - An optional suffix for the settings     file name. Default is an empty string.</li> </ul>"},{"location":"API/particula_beta/data/settings_generator/#returns_3","title":"Returns","text":"<ul> <li><code>-</code> dict - A dictionary of settings loaded from the file.</li> </ul>"},{"location":"API/particula_beta/data/settings_generator/#raises_1","title":"Raises","text":"<ul> <li><code>-</code> FileNotFoundError - If no settings file is found.</li> <li><code>-</code> Warning - If more than one settings file is found.</li> </ul>"},{"location":"API/particula_beta/data/settings_generator/#signature_3","title":"Signature","text":"<pre><code>def load_settings_for_stream(\n    path: str, subfolder: str, settings_suffix: str = \"\"\n) -&gt; dict: ...\n</code></pre>"},{"location":"API/particula_beta/data/settings_generator/#save_settings_for_lake","title":"save_settings_for_lake","text":"<p>Show source in settings_generator.py:314</p> <p>Save settings for lake data to a JSON file.</p> <p>Given a dictionary of settings, this function saves it to a JSON file named 'lake_settings' with an optional suffix in the specified filename. The JSON file is formatted with a 4-space indentation.</p>"},{"location":"API/particula_beta/data/settings_generator/#arguments_4","title":"Arguments","text":"<ul> <li><code>-</code> settings - The settings dictionary to be saved.</li> <li><code>-</code> path - The path where the subfolder is located.</li> <li><code>-</code> subfolder - The subfolder where the settings file will be saved.</li> <li><code>-</code> settings_suffix - An optional suffix for the settings     file name. Default is an empty string.</li> </ul>"},{"location":"API/particula_beta/data/settings_generator/#returns_4","title":"Returns","text":"<ul> <li>None</li> </ul>"},{"location":"API/particula_beta/data/settings_generator/#signature_4","title":"Signature","text":"<pre><code>def save_settings_for_lake(\n    settings: dict, path: str, subfolder: str = \"\", settings_suffix: str = \"\"\n) -&gt; None: ...\n</code></pre>"},{"location":"API/particula_beta/data/settings_generator/#save_settings_for_stream","title":"save_settings_for_stream","text":"<p>Show source in settings_generator.py:241</p> <p>Save settings for lake data to a JSON file.</p> <p>Given a dictionary of settings, this function saves it to a JSON file named 'stream_settings' with an optional suffix in the specified filename. The JSON file is formatted with a 4-space indentation.</p>"},{"location":"API/particula_beta/data/settings_generator/#arguments_5","title":"Arguments","text":"<ul> <li><code>-</code> settings - The settings dictionary to be saved.</li> <li><code>-</code> path - The path where the subfolder is located.</li> <li><code>-</code> subfolder - The subfolder where the settings file will be saved.</li> <li><code>-</code> settings_suffix - An optional suffix for the settings     file name. Default is an empty string.</li> </ul>"},{"location":"API/particula_beta/data/settings_generator/#returns_5","title":"Returns","text":"<ul> <li>None</li> </ul>"},{"location":"API/particula_beta/data/settings_generator/#signature_5","title":"Signature","text":"<pre><code>def save_settings_for_stream(\n    settings: dict, path: str, subfolder: str, settings_suffix: str = \"\"\n) -&gt; None: ...\n</code></pre>"},{"location":"API/particula_beta/data/stream/","title":"Stream","text":"<p>Particula-beta Index / Particula Beta / Data / Stream</p> <p>Auto-generated documentation for particula_beta.data.stream module.</p>"},{"location":"API/particula_beta/data/stream/#stream_1","title":"Stream","text":"<p>Show source in stream.py:14</p> <p>Consistent format for storing data.</p> <p>Represents a consistent format for storing and managing data streams within a list. Similar to pandas but with tighter control over the data allowed and expected format.</p>"},{"location":"API/particula_beta/data/stream/#attributes","title":"Attributes","text":"<ul> <li><code>header</code> - Headers of the data stream, each a string.</li> <li><code>data</code> - 2D numpy array where rows are timepoints and columns     correspond to headers.</li> <li><code>time</code> - 1D numpy array representing the time points of the data stream.</li> <li><code>files</code> - List of filenames that contain the data stream.</li> </ul>"},{"location":"API/particula_beta/data/stream/#methods","title":"Methods","text":"<ul> <li><code>validate_inputs</code> - Validates the types of class inputs.</li> <li><code>__getitem__(index)</code> - Returns the data at the specified index.</li> <li><code>__setitem__(index,</code> value) - Sets or updates data at the specified index.</li> <li><code>__len__()</code> - Returns the length of the time stream.</li> <li><code>datetime64</code> - Converts time stream to numpy datetime64 array for plots.</li> <li><code>header_dict</code> - Provides a dictionary mapping of header indices to names.</li> <li><code>header_float</code> - Converts header names to a numpy array of floats.</li> </ul>"},{"location":"API/particula_beta/data/stream/#signature","title":"Signature","text":"<pre><code>class Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/stream/#streamgetitem","title":"Stream().getitem","text":"<p>Show source in stream.py:56</p> <p>Gets data at a specified index or header name.</p> <p>Allows indexing of the data stream using an integer index or a string corresponding to the header. If a string is used, the header index is retrieved and used to return the data array. Only one str argument is allowed. A list of int is allowed.</p>"},{"location":"API/particula_beta/data/stream/#arguments","title":"Arguments","text":"<ul> <li><code>index</code> - The index or name of the data column to     retrieve.</li> </ul>"},{"location":"API/particula_beta/data/stream/#returns","title":"Returns","text":"<ul> <li><code>np.ndarray</code> - The data array at the specified index.</li> </ul>"},{"location":"API/particula_beta/data/stream/#signature_1","title":"Signature","text":"<pre><code>def __getitem__(self, index: Union[int, str]) -&gt; NDArray[np.float64]: ...\n</code></pre>"},{"location":"API/particula_beta/data/stream/#streamlen","title":"Stream().len","text":"<p>Show source in stream.py:101</p> <p>Returns the number of time points in the data stream.</p>"},{"location":"API/particula_beta/data/stream/#returns_1","title":"Returns","text":"<ul> <li><code>int</code> - Length of the time stream.</li> </ul>"},{"location":"API/particula_beta/data/stream/#signature_2","title":"Signature","text":"<pre><code>def __len__(self) -&gt; int: ...\n</code></pre>"},{"location":"API/particula_beta/data/stream/#streampop","title":"Stream().pop","text":"<p>Show source in stream.py:109</p> <p>Removes data at a specified index or header name.</p> <p>Allows indexing of the data stream using an integer index or a string corresponding to the header. If a string is used, the header index is retrieved and used to return the data array. Only one str argument is allowed. A list of int is allowed.</p>"},{"location":"API/particula_beta/data/stream/#arguments_1","title":"Arguments","text":"<ul> <li><code>index</code> - The index or name of the data column to     retrieve.</li> </ul>"},{"location":"API/particula_beta/data/stream/#signature_3","title":"Signature","text":"<pre><code>def __pop__(self, index: Union[int, str]) -&gt; None: ...\n</code></pre>"},{"location":"API/particula_beta/data/stream/#streamsetitem","title":"Stream().setitem","text":"<p>Show source in stream.py:75</p> <p>Sets or adds data at a specified index.</p> <p>If index is a string and not in headers, it is added. This is used to add new data columns to the stream.</p>"},{"location":"API/particula_beta/data/stream/#arguments_2","title":"Arguments","text":"<ul> <li><code>index</code> - The index or name of the data column to set.</li> <li><code>value</code> - The data to set at the specified index.</li> </ul>"},{"location":"API/particula_beta/data/stream/#notes","title":"Notes","text":"<p>Support setting multiple rows by accepting a list of values.</p>"},{"location":"API/particula_beta/data/stream/#signature_4","title":"Signature","text":"<pre><code>def __setitem__(self, index: Union[int, str], value: NDArray[np.float64]): ...\n</code></pre>"},{"location":"API/particula_beta/data/stream/#streamdatetime64","title":"Stream().datetime64","text":"<p>Show source in stream.py:126</p> <p>Converts the epoch time array to a datetime64 for plotting.</p> <p>This method converts the time array to a datetime64 array, which can be used for plotting time series data. This generally assumes that the time array is in seconds since the epoch.</p>"},{"location":"API/particula_beta/data/stream/#returns_2","title":"Returns","text":"<ul> <li><code>np.ndarray</code> - Datetime64 array representing the time stream.</li> </ul>"},{"location":"API/particula_beta/data/stream/#signature_5","title":"Signature","text":"<pre><code>@property\ndef datetime64(self) -&gt; NDArray[np.float64]: ...\n</code></pre>"},{"location":"API/particula_beta/data/stream/#streamheader_dict","title":"Stream().header_dict","text":"<p>Show source in stream.py:139</p> <p>Provides a dictionary mapping from index to header names.</p>"},{"location":"API/particula_beta/data/stream/#returns_3","title":"Returns","text":"<ul> <li><code>dict</code> - Dictionary with indices as keys and header names as values.</li> </ul>"},{"location":"API/particula_beta/data/stream/#signature_6","title":"Signature","text":"<pre><code>@property\ndef header_dict(self) -&gt; dict[int, str]: ...\n</code></pre>"},{"location":"API/particula_beta/data/stream/#streamheader_float","title":"Stream().header_float","text":"<p>Show source in stream.py:148</p> <p>Attempts to convert header names to a float array, where possible.</p>"},{"location":"API/particula_beta/data/stream/#returns_4","title":"Returns","text":"<ul> <li><code>np.ndarray</code> - Array of header names converted to floats.</li> </ul>"},{"location":"API/particula_beta/data/stream/#signature_7","title":"Signature","text":"<pre><code>@property\ndef header_float(self) -&gt; NDArray[np.float64]: ...\n</code></pre>"},{"location":"API/particula_beta/data/stream/#streamvalidate_inputs","title":"Stream().validate_inputs","text":"<p>Show source in stream.py:47</p> <p>Validates that header is a list.</p>"},{"location":"API/particula_beta/data/stream/#raises","title":"Raises","text":"<ul> <li><code>TypeError</code> - If header is not a list.</li> </ul>"},{"location":"API/particula_beta/data/stream/#signature_8","title":"Signature","text":"<pre><code>def validate_inputs(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/stream/#streamaveraged","title":"StreamAveraged","text":"<p>Show source in stream.py:159</p> <p>Stream Class with Averaged Data and Standard Deviation.</p> <p>Extends the Stream class with functionalities specific to handling averaged data streams. Mainly adding standard deviation to the data stream.</p>"},{"location":"API/particula_beta/data/stream/#attributes_1","title":"Attributes","text":"<ul> <li><code>average_interval</code> - The interval in units (e.g., seconds, minutes) over     which data is averaged.</li> <li><code>start_time</code> - The start time from which data begins to be averaged.</li> <li><code>stop_time</code> - The time at which data ceases to be averaged.</li> <li><code>standard_deviation</code> - A numpy array storing the standard deviation of     data streams.</li> </ul>"},{"location":"API/particula_beta/data/stream/#signature_9","title":"Signature","text":"<pre><code>class StreamAveraged(Stream): ...\n</code></pre>"},{"location":"API/particula_beta/data/stream/#see-also","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/stream/#streamaveragedget_std","title":"StreamAveraged().get_std","text":"<p>Show source in stream.py:211</p> <p>Retrieves the standard deviation.</p> <p>In the averaged data stream, the standard deviation of the data is stored in a separate array that mirrors the same indices as the data stream. This method allows retrieval of the standard deviation at a specified index.</p>"},{"location":"API/particula_beta/data/stream/#arguments_3","title":"Arguments","text":"<ul> <li><code>index</code> - The index or header name of the data stream for which standard deviation is needed.</li> </ul>"},{"location":"API/particula_beta/data/stream/#returns_5","title":"Returns","text":"<ul> <li><code>np.ndarray</code> - The standard deviation values at the specified index.</li> </ul>"},{"location":"API/particula_beta/data/stream/#raises_1","title":"Raises","text":"<ul> <li><code>ValueError</code> - If the specified index does not exist in the header.</li> </ul>"},{"location":"API/particula_beta/data/stream/#signature_10","title":"Signature","text":"<pre><code>def get_std(self, index: Union[int, str]) -&gt; NDArray[np.float64]: ...\n</code></pre>"},{"location":"API/particula_beta/data/stream/#streamaveragedvalidate_averaging_params","title":"StreamAveraged().validate_averaging_params","text":"<p>Show source in stream.py:186</p> <p>Ensures that averaging parameters are valid.</p>"},{"location":"API/particula_beta/data/stream/#raises_2","title":"Raises","text":"<ul> <li><code>ValueError</code> - If average_interval is not a positive number.</li> <li><code>ValueError</code> - If start_time or stop_time are not numerical or if     start_time is greater than or equal to stop_time.</li> </ul>"},{"location":"API/particula_beta/data/stream/#signature_11","title":"Signature","text":"<pre><code>def validate_averaging_params(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/stream_stats/","title":"Stream Stats","text":"<p>Particula-beta Index / Particula Beta / Data / Stream Stats</p> <p>Auto-generated documentation for particula_beta.data.stream_stats module.</p>"},{"location":"API/particula_beta/data/stream_stats/#average_std","title":"average_std","text":"<p>Show source in stream_stats.py:34</p> <p>Calculate the average and standard deviation of data within a given 'stream' object over specified intervals.</p> <p>This function takes a 'stream' object, which should contain time-series data, and computes the average and standard deviation of the data at intervals specified by 'average_interval'. If data.time is in seconds then the units of the interval are seconds (hour in hours etc). The results are returned as a new 'StreamAveraged' object containing the processed data.</p>"},{"location":"API/particula_beta/data/stream_stats/#arguments","title":"Arguments","text":"<ul> <li>stream (object): The input stream object containing 'time' and 'data'     arrays along with other associated metadata.</li> <li>average_interval (float|int, optional): The time interval over which the     averaging is to be performed.</li> <li>new_time_array (np.ndarray, optional): An optional array of time points     at which the average and standard deviation are computed.     If not provided, a new time array is generated based on the start and     end times within the 'stream.time' object.</li> </ul>"},{"location":"API/particula_beta/data/stream_stats/#returns","title":"Returns","text":"<ul> <li>StreamAveraged (object): An object of type 'StreamAveraged' containing     the averaged data, time array, start and stop times, the standard     deviation of the averaged data, and other metadata from the original     'stream' object.</li> </ul> <p>The function checks for an existing 'new_time_array' and generates one if needed. It then calculates the average and standard deviation for each interval and constructs a 'StreamAveraged' object with the results and metadata from the original 'stream' object.</p>"},{"location":"API/particula_beta/data/stream_stats/#signature","title":"Signature","text":"<pre><code>def average_std(\n    stream: Stream,\n    average_interval: Union[float, int] = 60,\n    new_time_array: Optional[np.ndarray] = None,\n) -&gt; StreamAveraged: ...\n</code></pre>"},{"location":"API/particula_beta/data/stream_stats/#see-also","title":"See also","text":"<ul> <li>StreamAveraged</li> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/stream_stats/#drop_masked","title":"drop_masked","text":"<p>Show source in stream_stats.py:14</p> <p>Drop rows where mask is false, and return data stream.</p>"},{"location":"API/particula_beta/data/stream_stats/#arguments_1","title":"Arguments","text":"<p>stream : object     data stream object mask : np.ndarray     mask to apply to data stream</p>"},{"location":"API/particula_beta/data/stream_stats/#returns_1","title":"Returns","text":"<p>object     stream object</p>"},{"location":"API/particula_beta/data/stream_stats/#signature_1","title":"Signature","text":"<pre><code>def drop_masked(stream: Stream, mask: ignore) -&gt; Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/stream_stats/#see-also_1","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/stream_stats/#filtering","title":"filtering","text":"<p>Show source in stream_stats.py:104</p> <p>Filters the data of the given 'stream' object based on the specified bounds or specific value. The filtered data can be either dropped or replaced with a specified value.  Note, not all parameters need to be specified, but at least one must be provided (top, bottom, value).</p>"},{"location":"API/particula_beta/data/stream_stats/#arguments_2","title":"Arguments","text":"<ul> <li>stream (Stream): The input stream object containing 'data' and 'time'     attributes.</li> <li>bottom (float, optional): The lower bound for filtering data. Defaults     to None.</li> <li>top (float, optional): The upper bound for filtering data.     Defaults to None.</li> <li>value (float, optional): Specific value to filter from data.     Defaults to None.</li> <li>invert (bool): If True, inverts the filter criteria.     Defaults to False.</li> <li>clone (bool): If True, returns a copy of the 'stream' object, with     filtered data. If False, modifies the 'stream' object in-place.     Defaults to True.</li> <li>replace_with (float|int, optional): Value to replace filtered-out data.     Defaults to None.</li> <li>drop (bool, optional): If True, filtered-out data points are dropped     from the dataset. Defaults to False.</li> <li>header (list, optional): The header of the data to filter on. This can     same as calling Stream['header']     Defaults to None.</li> </ul>"},{"location":"API/particula_beta/data/stream_stats/#returns_2","title":"Returns","text":"<ul> <li><code>-</code> Stream - The 'stream' object with data filtered as specified.</li> </ul> <p>If 'drop' is True, 'replace_with' is ignored and filtered data points are removed from the 'stream' object. Otherwise, filtered data points are replaced with 'replace_with' value.</p> <p>add specific data row to filter on</p>"},{"location":"API/particula_beta/data/stream_stats/#signature_2","title":"Signature","text":"<pre><code>def filtering(\n    stream: Stream,\n    bottom: Optional[float] = None,\n    top: Optional[float] = None,\n    value: Optional[float] = None,\n    invert: Optional[bool] = False,\n    clone: Optional[bool] = True,\n    replace_with: Optional[Union[float, int]] = None,\n    drop: Optional[bool] = False,\n    header: Optional[Union[list, int, str]] = None,\n) -&gt; Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/stream_stats/#see-also_2","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/stream_stats/#remove_time_window","title":"remove_time_window","text":"<p>Show source in stream_stats.py:176</p> <p>Remove a time window from a stream object.</p>"},{"location":"API/particula_beta/data/stream_stats/#arguments_3","title":"Arguments","text":"<ul> <li><code>stream</code> - The input stream object containing 'data' and 'time'     attributes.</li> <li><code>epoch_start</code> - The start time of the time window to be     removed.</li> <li><code>epoch_end</code> - The end time of the time window to be     removed. If not provided, the time window is the closest time     point to 'epoch_start'.</li> </ul>"},{"location":"API/particula_beta/data/stream_stats/#returns_3","title":"Returns","text":"<ul> <li><code>Stream</code> - The 'stream' object with the specified time window removed.</li> </ul>"},{"location":"API/particula_beta/data/stream_stats/#signature_3","title":"Signature","text":"<pre><code>def remove_time_window(\n    stream: Stream,\n    epoch_start: Union[float, int],\n    epoch_end: Optional[Union[float, int]] = None,\n) -&gt; Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/stream_stats/#see-also_3","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/stream_stats/#select_time_window","title":"select_time_window","text":"<p>Show source in stream_stats.py:210</p> <p>Keep only a specified time window in a stream object and remove all other data.</p>"},{"location":"API/particula_beta/data/stream_stats/#arguments_4","title":"Arguments","text":"<ul> <li><code>stream</code> - The input stream object containing 'data' and 'time'     attributes.</li> <li><code>epoch_start</code> - The start time of the time window to be kept.</li> <li><code>epoch_end</code> - The end time of the time window to be kept. If not provided,     only the closest time point to 'epoch_start' will be kept.</li> </ul>"},{"location":"API/particula_beta/data/stream_stats/#returns_4","title":"Returns","text":"<ul> <li><code>Stream</code> - The stream object with only the specified time window retained.</li> </ul>"},{"location":"API/particula_beta/data/stream_stats/#signature_4","title":"Signature","text":"<pre><code>def select_time_window(\n    stream: Stream,\n    epoch_start: Union[float, int],\n    epoch_end: Optional[Union[float, int]] = None,\n    clone: Optional[bool] = True,\n) -&gt; Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/stream_stats/#see-also_4","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/stream_stats/#time_derivative_of_stream","title":"time_derivative_of_stream","text":"<p>Show source in stream_stats.py:248</p> <p>Calculate the rate of change of the concentration PMF over time and return a new stream.</p> <p>Uses a linear regression model to fit the slope over a time window. The edge cases are handled by using a smaller window size.</p>"},{"location":"API/particula_beta/data/stream_stats/#arguments_5","title":"Arguments","text":"<ul> <li><code>pmf_fitted_stream</code> - Stream object containing the fitted concentration     PMF data.</li> <li><code>window_size</code> - Size of the time window for fitting the slope.</li> </ul>"},{"location":"API/particula_beta/data/stream_stats/#returns_5","title":"Returns","text":"<ul> <li><code>rate_of_change_stream</code> - Stream object containing the rate of     change of the concentration PMF.</li> </ul>"},{"location":"API/particula_beta/data/stream_stats/#signature_5","title":"Signature","text":"<pre><code>def time_derivative_of_stream(\n    stream: Stream, liner_slope_window_size: int = 12\n) -&gt; Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/stream_stats/#see-also_5","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/process/","title":"Process","text":"<p>Particula-beta Index / Particula Beta / Data / Process</p> <p>Auto-generated documentation for particula_beta.data.process module.</p>"},{"location":"API/particula_beta/data/process/#modules","title":"Modules","text":"<ul> <li>Aerodynamic Convert</li> <li>Chamber Rate Fitting</li> <li>Kappa Via Extinction</li> <li>Lognormal 2mode</li> <li>Mie Angular</li> <li>Mie Bulk</li> <li>Ml Analysis</li> <li>Optical Instrument</li> <li>Sample Distribution</li> <li>Scattering Truncation</li> <li>Size Distribution</li> <li>Stats</li> </ul>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/","title":"Aerodynamic Convert","text":"<p>Particula-beta Index / Particula Beta / Data / Process / Aerodynamic Convert</p> <p>Auto-generated documentation for particula_beta.data.process.aerodynamic_convert module.</p>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/#_cost_aerodynamic_radius","title":"_cost_aerodynamic_radius","text":"<p>Show source in aerodynamic_convert.py:12</p> <p>Optimization cost function to determine the aerodynamic radius of a particle.</p>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/#arguments","title":"Arguments","text":"<ul> <li><code>guess_aerodynamic_radius</code> - The initial guess for the aerodynamic radius.</li> <li><code>mean_free_path_air</code> - The mean free path of air molecules.</li> <li><code>particle_radius</code> - The known physical radius of the particle.</li> <li><code>kwargs</code> - Additional keyword arguments for the optimization.<ul> <li>density (float): The density of the particle. Default is     1500 kg/m^3.</li> <li>reference_density (float): The reference density for the     aerodynamic radius calculation. Default is 1000 kg/m^3.</li> <li>aerodynamic_shape_factor (float): The aerodynamic shape factor.     Default is 1.0.</li> </ul> </li> </ul>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/#returns","title":"Returns","text":"<p>The squared error between the guessed aerodynamic radius and     the calculated aerodynamic radius.</p>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/#signature","title":"Signature","text":"<pre><code>def _cost_aerodynamic_radius(\n    guess_aerodynamic_radius: Union[float, NDArray[np.float64]],\n    mean_free_path_air: Union[float, NDArray[np.float64]],\n    particle_radius: Union[float, NDArray[np.float64]],\n    **kwargs\n) -&gt; Union[float, NDArray[np.float64]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/#_cost_physical_radius","title":"_cost_physical_radius","text":"<p>Show source in aerodynamic_convert.py:72</p> <p>Optimization cost function to determine the physical radius of a particle.</p>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/#arguments_1","title":"Arguments","text":"<ul> <li><code>guess_physical_radius</code> - The initial guess for the physical radius.</li> <li><code>mean_free_path_air</code> - The mean free path of air molecules.</li> <li><code>aerodynamic_radius</code> - The known aerodynamic radius of the particle.</li> <li><code>kwargs</code> - Additional keyword arguments for the optimization<ul> <li>density (float): The density of the particle. Default is     1500 kg/m^3.</li> <li>reference_density (float): The reference density for the     aerodynamic radius calculation. Default is 1000 kg/m^3.</li> <li>aerodynamic_shape_factor (float): The aerodynamic shape factor.     Default is 1.0.</li> </ul> </li> </ul>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/#returns_1","title":"Returns","text":"<p>The squared error between the guessed physical radius and the calculated aerodynamic radius.</p>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/#signature_1","title":"Signature","text":"<pre><code>def _cost_physical_radius(\n    guess_physical_radius: Union[float, NDArray[np.float64]],\n    mean_free_path_air: Union[float, NDArray[np.float64]],\n    aerodynamic_radius: Union[float, NDArray[np.float64]],\n    **kwargs\n) -&gt; Union[float, NDArray[np.float64]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/#convert_aerodynamic_to_physical_radius","title":"convert_aerodynamic_to_physical_radius","text":"<p>Show source in aerodynamic_convert.py:132</p> <p>Convert aerodynamic radius to physical radius for a particle or an array of particles.</p>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/#arguments_2","title":"Arguments","text":"<ul> <li><code>aerodynamic_radius</code> - The aerodynamic radius or array of radii to be     converted.</li> <li><code>pressure</code> - The ambient pressure in Pascals.</li> <li><code>temperature</code> - The ambient temperature in Kelvin.</li> <li><code>particle_density</code> - The density of the particles in kg/m^3.</li> <li><code>aerodynamic_shape_factor</code> - The aerodynamic shape factor. Default is 1.0.</li> <li><code>reference_density</code> - The reference density for the aerodynamic radius     in kg/m^3. Default is 1000 kg/m^3.</li> </ul>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/#returns_2","title":"Returns","text":"<p>The physical radius or array of radii corresponding to the aerodynamic radius/radii.</p>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/#signature_2","title":"Signature","text":"<pre><code>def convert_aerodynamic_to_physical_radius(\n    aerodynamic_radius: Union[float, NDArray[np.float64]],\n    pressure: float,\n    temperature: float,\n    particle_density: float,\n    aerodynamic_shape_factor: float = 1.0,\n    reference_density: float = 1000.0,\n) -&gt; Union[float, NDArray[np.float64]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/#convert_physical_to_aerodynamic_radius","title":"convert_physical_to_aerodynamic_radius","text":"<p>Show source in aerodynamic_convert.py:189</p> <p>Convert physical radius to aerodynamic radius for a particle or an array of particles.</p>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/#arguments_3","title":"Arguments","text":"<ul> <li><code>physical_radius</code> - The physical radius or array of radii to be converted.</li> <li><code>pressure</code> - The ambient pressure in Pascals.</li> <li><code>temperature</code> - The ambient temperature in Kelvin.</li> <li><code>particle_density</code> - The density of the particles in kg/m^3.</li> <li><code>aerodynamic_shape_factor</code> - The aerodynamic shape factor. Default is 1.0.</li> <li><code>reference_density</code> - The reference density for the aerodynamic radius     in kg/m^3. Default is 1000 kg/m^3.</li> </ul>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/#returns_3","title":"Returns","text":"<p>The aerodynamic radius or array of radii corresponding to the physical radius/radii.</p>"},{"location":"API/particula_beta/data/process/aerodynamic_convert/#signature_3","title":"Signature","text":"<pre><code>def convert_physical_to_aerodynamic_radius(\n    physical_radius: Union[float, NDArray[np.float64]],\n    pressure: float,\n    temperature: float,\n    particle_density: float,\n    aerodynamic_shape_factor: float = 1.0,\n    reference_density: float = 1000.0,\n) -&gt; Union[float, NDArray[np.float64]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/","title":"Chamber Rate Fitting","text":"<p>Particula-beta Index / Particula Beta / Data / Process / Chamber Rate Fitting</p> <p>Auto-generated documentation for particula_beta.data.process.chamber_rate_fitting module.</p>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#chamberparameters","title":"ChamberParameters","text":"<p>Show source in chamber_rate_fitting.py:284</p> <p>Data class for the chamber parameters.</p>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#signature","title":"Signature","text":"<pre><code>class ChamberParameters: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#calculate_optimized_rates","title":"calculate_optimized_rates","text":"<p>Show source in chamber_rate_fitting.py:423</p> <p>Calculate the coagulation rates using the optimized parameters and return the rates and R2 score.</p>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#arguments","title":"Arguments","text":"<ul> <li><code>radius_bins</code> - Array of particle radii in meters.</li> <li><code>concentration_pmf</code> - 2D array of concentration PMF values.</li> <li><code>wall_eddy_diffusivity</code> - Optimized wall eddy diffusivity.</li> <li><code>alpha_collision_efficiency</code> - Optimized alpha collision efficiency.</li> <li><code>chamber_params</code> - ChamberParameters object containing chamber-related     parameters.</li> <li><code>time_derivative_concentration_pmf</code> - Array of observed rate of change     of the concentration PMF (optional).</li> <li><code>w_correction</code> - W correction of the particles (optional).</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#returns","title":"Returns","text":"<ul> <li><code>coagulation_loss</code> - Loss rate due to coagulation.</li> <li><code>coagulation_gain</code> - Gain rate due to coagulation.</li> <li><code>dilution_loss</code> - Loss rate due to dilution.</li> <li><code>wall_loss_rate</code> - Loss rate due to wall deposition.</li> <li><code>net_rate</code> - Net rate considering all effects.</li> <li><code>r2_value</code> - R2 score between the net rate and the observed rate.</li> <li><code>chi_squared</code> - Chi-squared error between the net rate and the     observed rate.</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#signature_1","title":"Signature","text":"<pre><code>def calculate_optimized_rates(\n    radius_bins: NDArray[np.float64],\n    concentration_pmf: NDArray[np.float64],\n    wall_eddy_diffusivity: float,\n    alpha_collision_efficiency: float,\n    w_correction: float,\n    fractional_uncertainty: NDArray[np.float64],\n    chamber_parameters: ChamberParameters,\n    time_derivative_concentration_pmf: Optional[NDArray[np.float64]] = None,\n) -&gt; Tuple[float, float, float, float, float, float]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#see-also","title":"See also","text":"<ul> <li>ChamberParameters</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#calculate_pmf_rates","title":"calculate_pmf_rates","text":"<p>Show source in chamber_rate_fitting.py:118</p> <p>Calculate the coagulation, dilution, and wall loss rates, and return the net rate.</p>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#arguments_1","title":"Arguments","text":"<ul> <li><code>radius_bins</code> - Array of particle radii.</li> <li><code>concentration_pmf</code> - Array of particle concentration     probability mass function.</li> <li><code>temperature</code> - Temperature in Kelvin.</li> <li><code>pressure</code> - Pressure in Pascals.</li> <li><code>particle_density</code> - Density of the particles in kg/m^3.</li> <li><code>alpha_collision_efficiency</code> - Collision efficiency factor.</li> <li><code>volume</code> - Volume of the chamber in m^3.</li> <li><code>input_flow_rate</code> - Input flow rate in m^3/s.</li> <li><code>wall_eddy_diffusivity</code> - Eddy diffusivity for wall loss in m^2/s.</li> <li><code>chamber_dimensions</code> - Dimensions of the chamber     (length, width, height) in meters.</li> <li><code>w_correction</code> - W correction of the particles (optional).</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#returns_1","title":"Returns","text":"<ul> <li><code>coagulation_loss</code> - Loss rate due to coagulation.</li> <li><code>coagulation_gain</code> - Gain rate due to coagulation.</li> <li><code>dilution_loss</code> - Loss rate due to dilution.</li> <li><code>wall_loss_rate</code> - Loss rate due to wall deposition.</li> <li><code>net_rate</code> - Net rate considering all effects.</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#signature_2","title":"Signature","text":"<pre><code>def calculate_pmf_rates(\n    radius_bins: NDArray[np.float64],\n    concentration_pmf: NDArray[np.float64],\n    temperature: float = 293.15,\n    pressure: float = 101325,\n    particle_density: float = 1000,\n    alpha_collision_efficiency: float = 1,\n    w_correction: float = 1,\n    volume: float = 1,\n    input_flow_rate: float = 1.6e-07,\n    wall_eddy_diffusivity: float = 0.1,\n    chamber_dimensions: Tuple[float, float, float] = (1, 1, 1),\n) -&gt; Tuple[\n    NDArray[np.float64],\n    NDArray[np.float64],\n    NDArray[np.float64],\n    NDArray[np.float64],\n    NDArray[np.float64],\n]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#chi_squared_error","title":"chi_squared_error","text":"<p>Show source in chamber_rate_fitting.py:20</p> <p>Calculate the chi-squared error between the observed and predicted values, accounting for uncertainties.</p>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#arguments_2","title":"Arguments","text":"<ul> <li><code>observed</code> - Array of observed values.</li> <li><code>predicted</code> - Array of predicted values.</li> <li><code>fractional_uncertainty</code> - Array or float of fractional uncertainty     (e.g., 0.2 for 20%).</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#returns_2","title":"Returns","text":"<ul> <li><code>chi_squared</code> - Chi-squared error between the observed and predicted     values.</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#signature_3","title":"Signature","text":"<pre><code>def chi_squared_error(\n    observed: np.ndarray, predicted: np.ndarray, fractional_uncertainty: np.ndarray\n) -&gt; float: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#coagulation_rates_cost_function","title":"coagulation_rates_cost_function","text":"<p>Show source in chamber_rate_fitting.py:222</p> <p>Cost function for the optimization of the eddy diffusivity and alpha collision efficiency.</p>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#signature_4","title":"Signature","text":"<pre><code>def coagulation_rates_cost_function(\n    parameters: NDArray[np.float64],\n    radius_bins: NDArray[np.float64],\n    concentration_pmf: NDArray[np.float64],\n    time_derivative_concentration_pmf: NDArray[np.float64],\n    fractional_uncertainty: NDArray[np.float64],\n    temperature: float = 293.15,\n    pressure: float = 101325,\n    particle_density: float = 1000,\n    volume: float = 1,\n    input_flow_rate: float = 1.6e-07,\n    chamber_dimensions: Tuple[float, float, float] = (1, 1, 1),\n) -&gt; float: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#create_guess_and_bounds","title":"create_guess_and_bounds","text":"<p>Show source in chamber_rate_fitting.py:295</p> <p>Create the initial guess array and bounds list for the optimization.</p>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#arguments_3","title":"Arguments","text":"<ul> <li><code>guess_eddy_diffusivity</code> - Initial guess for eddy diffusivity.</li> <li><code>guess_alpha_collision_efficiency</code> - Initial guess for alpha collision     efficiency.</li> <li><code>bounds_eddy_diffusivity</code> - Bounds for eddy diffusivity.</li> <li><code>bounds_alpha_collision_efficiency</code> - Bounds for alpha collision     efficiency.</li> <li><code>guess_w_correction</code> - Initial guess for the W correction,     optional. Typical range between 1 and 3.</li> <li><code>bounds_w_correction</code> - Bounds for the W correction.</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#returns_3","title":"Returns","text":"<ul> <li><code>initial_guess</code> - Numpy array of the initial guess values.</li> <li><code>bounds</code> - List of tuples representing the bounds for each parameter.</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#signature_5","title":"Signature","text":"<pre><code>def create_guess_and_bounds(\n    guess_eddy_diffusivity: float,\n    guess_alpha_collision_efficiency: float,\n    bounds_eddy_diffusivity: Tuple[float, float],\n    bounds_alpha_collision_efficiency: Tuple[float, float],\n    guess_w_correction: float,\n    bounds_w_correction: Tuple[float, float],\n) -&gt; Tuple[NDArray[np.float64], List[Tuple[float, float]]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#hessian_standard_error","title":"hessian_standard_error","text":"<p>Show source in chamber_rate_fitting.py:48</p> <p>Calculate the standard error for the optimized parameters using Hessian estimation with regularization and fallback to diagonal approximation.</p>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#arguments_4","title":"Arguments","text":"<ul> <li><code>objective_function</code> - The objective function.</li> <li><code>parameters</code> - Array of optimized parameters.</li> <li><code>epsilon</code> - Step size for numerical differentiation.</li> <li><code>regularization</code> - Regularization term for Hessian inversion.</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#returns_4","title":"Returns","text":"<ul> <li><code>standard_errors</code> - Standard errors of the optimized parameters.</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#signature_6","title":"Signature","text":"<pre><code>def hessian_standard_error(\n    objective_function: Callable,\n    parameters: np.ndarray,\n    epsilon: float = 0.0001,\n    regularization: float = 1e-08,\n) -&gt; np.ndarray: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#optimize_and_calculate_rates_looped","title":"optimize_and_calculate_rates_looped","text":"<p>Show source in chamber_rate_fitting.py:507</p> <p>Perform optimization and calculate rates for each time point in the stream.</p>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#arguments_5","title":"Arguments","text":"<ul> <li><code>pmf_stream</code> - Stream object containing the fitted PMF data.</li> <li><code>pmf_derivative_stream</code> - Stream object containing the derivative of the     PMF data.</li> <li><code>chamber_parameters</code> - ChamberParameters object containing     chamber-related parameters.</li> <li><code>fit_guess</code> - Initial guess for the optimization.</li> <li><code>fit_bounds</code> - Bounds for the optimization parameters.</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#returns_5","title":"Returns","text":"<ul> <li><code>result_stream</code> - Stream containing the optimization results for     each time point.</li> <li><code>coagulation_loss_stream</code> - Stream containing coagulation loss rates.</li> <li><code>coagulation_gain_stream</code> - Stream containing coagulation gain rates.</li> <li><code>coagulation_net_stream</code> - Stream containing net coagulation rates.</li> <li><code>dilution_loss_stream</code> - Stream containing dilution loss rates.</li> <li><code>wall_loss_rate_stream</code> - Stream containing wall loss rates.</li> <li><code>total_rate_stream</code> - Stream containing total rates.</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#signature_7","title":"Signature","text":"<pre><code>def optimize_and_calculate_rates_looped(\n    pmf_stream: Stream,\n    pmf_derivative_stream: Stream,\n    chamber_parameters: ChamberParameters,\n    fit_guess: NDArray[np.float64],\n    fit_bounds: List[Tuple[float, float]],\n    fractional_uncertainty: NDArray[np.float64],\n    epsilon_hessian_estimation: float = 0.0001,\n) -&gt; Tuple[Stream, Stream, Stream, Stream, Stream, Stream, Stream]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#see-also_1","title":"See also","text":"<ul> <li>ChamberParameters</li> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#optimize_chamber_parameters","title":"optimize_chamber_parameters","text":"<p>Show source in chamber_rate_fitting.py:353</p> <p>Optimize the eddy diffusivity and alpha collision efficiency parameters for a given particle size distribution and its time derivative.</p>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#arguments_6","title":"Arguments","text":"<ul> <li><code>radius_bins</code> - Array of particle size bins in meters.</li> <li><code>concentration_pmf</code> - Array of particle mass fractions (PMF)     concentrations at each radius bin.</li> <li><code>time_derivative_concentration_pmf</code> - Array of time derivatives of     the PMF concentrations, representing the rate of change     in concentration over time.</li> <li><code>chamber_params</code> - ChamberParameters object containing the physical     properties of the chamber, including temperature, pressure,     particle density, volume, input flow rate, and chamber dimensions.</li> <li><code>fit_guess</code> - Initial guess for the optimization parameters     (eddy diffusivity and alpha collision efficiency).</li> <li><code>fit_bounds</code> - List of tuples specifying the bounds for the     optimization parameters (lower and upper bounds     for each parameter).</li> <li><code>minimize_method</code> - Optimization method to be used. Default is \"L-BFGS-B\".     The following methods from <code>scipy.optimize.minimize</code> accept bounds,     \"L-BFGS-B\", \"TNC\", \"SLSQP\", \"Powell\", \"trust-constr\".</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#returns_6","title":"Returns","text":"<ul> <li><code>wall_eddy_diffusivity_optimized</code> - Optimized value of the wall eddy     diffusivity (in 1/s).</li> <li><code>alpha_collision_efficiency_optimized</code> - Optimized value of the alpha     collision efficiency (dimensionless).</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#signature_8","title":"Signature","text":"<pre><code>def optimize_chamber_parameters(\n    radius_bins: NDArray[np.float64],\n    concentration_pmf: NDArray[np.float64],\n    time_derivative_concentration_pmf: NDArray[np.float64],\n    fractional_uncertainty: NDArray[np.float64],\n    chamber_parameters: ChamberParameters,\n    fit_guess: NDArray[np.float64],\n    fit_bounds: List[Tuple[float, float]],\n    minimize_method: str = \"L-BFGS-B\",\n    epsilon_hessian_estimation: float = 0.0001,\n) -&gt; Union[Tuple[float, float, float], Tuple[float, float, float]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#see-also_2","title":"See also","text":"<ul> <li>ChamberParameters</li> </ul>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#optimize_parameters","title":"optimize_parameters","text":"<p>Show source in chamber_rate_fitting.py:336</p> <p>Get the optimized parameters using the given cost function.</p>"},{"location":"API/particula_beta/data/process/chamber_rate_fitting/#signature_9","title":"Signature","text":"<pre><code>def optimize_parameters(\n    cost_function: ignore,\n    initial_guess: NDArray[np.float64],\n    bounds: List[Tuple[float, float]],\n    method: str,\n) -&gt; Tuple[float, float, float]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/kappa_via_extinction/","title":"Kappa Via Extinction","text":"<p>Particula-beta Index / Particula Beta / Data / Process / Kappa Via Extinction</p> <p>Auto-generated documentation for particula_beta.data.process.kappa_via_extinction module.</p>"},{"location":"API/particula_beta/data/process/kappa_via_extinction/#extinction_ratio_wet_dry","title":"extinction_ratio_wet_dry","text":"<p>Show source in kappa_via_extinction.py:24</p> <p>Calculate the extinction ratio between wet and dry aerosols, considering water uptake through the kappa parameter.</p> <p>This function uses Mie theory to determine the optical properties of aerosols with varying water content, allowing for analysis of hygroscopic growth and its impact on aerosol optical characteristics.</p>"},{"location":"API/particula_beta/data/process/kappa_via_extinction/#arguments","title":"Arguments","text":"<ul> <li><code>kappa</code> - Hygroscopicity parameter, defining the water uptake ability     of particles.</li> <li><code>number_per_cm3</code> - Number concentration of particles per cubic     centimeter for each size bin.</li> <li><code>diameters</code> - Diameters of particles in nanometers for each size bin.</li> <li><code>water_activity_sizer</code> - Water activity of the aerosol size distribution.</li> <li><code>water_activity_dry</code> - Water activity for the calculation of \"dry\"     aerosol properties.</li> <li><code>water_activity_wet</code> - Water activity for the calculation of \"wet\"     aerosol properties.</li> <li><code>refractive_index_dry</code> - Refractive index of the dry aerosol particles.     Default is 1.45.</li> <li><code>water_refractive_index</code> - Refractive index of water. Default is 1.33.</li> <li><code>wavelength</code> - Wavelength of the incident light in nanometers.     Default is 450 nm.</li> <li><code>discretize</code> - If True, discretizes input arguments for Mie calculations     to enable caching. Default is True.</li> <li><code>return_coefficients</code> - If True, returns the individual extinction     coefficients for wet and dry aerosols instead of their ratio.     Default is False.</li> <li><code>return_all_optics</code> - If True, returns all optical properties calculated     by Mie theory, not just extinction. Default is False.</li> </ul>"},{"location":"API/particula_beta/data/process/kappa_via_extinction/#returns","title":"Returns","text":"<p>By default, returns the ratio of wet to dry aerosol extinction. If <code>return_coefficients</code> is True, returns a tuple of NDArrays containing the extinction coefficients for wet and dry aerosols, respectively.</p>"},{"location":"API/particula_beta/data/process/kappa_via_extinction/#signature","title":"Signature","text":"<pre><code>def extinction_ratio_wet_dry(\n    kappa: Union[float, NDArray[np.float64]],\n    number_per_cm3: NDArray[np.float64],\n    diameters: NDArray[np.float64],\n    water_activity_sizer: NDArray[np.float64],\n    water_activity_dry: NDArray[np.float64],\n    water_activity_wet: NDArray[np.float64],\n    refractive_index_dry: Union[complex, float] = 1.45,\n    water_refractive_index: Union[complex, float] = 1.33,\n    wavelength: float = 450,\n    discretize: bool = True,\n    return_coefficients: bool = False,\n    return_all_optics: bool = False,\n) -&gt; Union[float, Tuple[NDArray, NDArray]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/kappa_via_extinction/#fit_extinction_ratio_with_kappa","title":"fit_extinction_ratio_with_kappa","text":"<p>Show source in kappa_via_extinction.py:139</p> <p>Fit the kappa parameter based on the measured extinction ratios of dry and wet aerosols, considering water uptake effects.</p> <p>This method uses Mie theory to optimize kappa by minimizing the difference between the calculated and observed extinction ratios of wet to dry aerosols.</p>"},{"location":"API/particula_beta/data/process/kappa_via_extinction/#arguments_1","title":"Arguments","text":"<ul> <li><code>b_ext_dry</code> - The measured extinction of the dry aerosol.</li> <li><code>b_ext_wet</code> - The measured extinction of the wet aerosol.</li> <li><code>number_per_cm3</code> - Number concentration of particles per cubic centimeter     for each size bin.</li> <li><code>diameters</code> - Diameters of particles in nanometers for each size bin.</li> <li><code>water_activity_sizer</code> - Water activity corresponding to the aerosol     size distribution.</li> <li><code>water_activity_dry</code> - Water activity for the \"dry\" aerosol condition.</li> <li><code>water_activity_wet</code> - Water activity for the \"wet\" aerosol condition.</li> <li><code>refractive_index_dry</code> - Refractive index of the dry aerosol particles.     Default is 1.45.</li> <li><code>water_refractive_index</code> - Refractive index of water. Default is 1.33.</li> <li><code>wavelength</code> - Wavelength of incident light in nanometers. Default is     450 nm.</li> <li><code>discretize</code> - If True, discretizes input arguments for Mie calculations     to enable caching. Default is True.</li> <li><code>kappa_bounds</code> - Bounds within which to fit the kappa parameter.     Default is (0, 1).</li> <li><code>kappa_tolerance</code> - Tolerance level for the optimization of kappa.     Default is 1e-6.</li> <li><code>kappa_maxiter</code> - Maximum number of iterations allowed in the optimization     process. Default is 200.</li> </ul>"},{"location":"API/particula_beta/data/process/kappa_via_extinction/#returns_1","title":"Returns","text":"<p>The optimized kappa parameter that best fits the observed extinction ratios.</p>"},{"location":"API/particula_beta/data/process/kappa_via_extinction/#signature_1","title":"Signature","text":"<pre><code>def fit_extinction_ratio_with_kappa(\n    b_ext_dry: Union[float, np.float64],\n    b_ext_wet: Union[float, np.float64],\n    number_per_cm3: NDArray[np.float64],\n    diameters: NDArray[np.float64],\n    water_activity_sizer: NDArray[np.float64],\n    water_activity_dry: NDArray[np.float64],\n    water_activity_wet: NDArray[np.float64],\n    refractive_index_dry: Union[complex, float] = 1.45,\n    water_refractive_index: Union[complex, float] = 1.33,\n    wavelength: float = 450,\n    discretize: bool = True,\n    kappa_bounds: Tuple[float, float] = (0, 1),\n    kappa_tolerance: float = 1e-06,\n    kappa_maxiter: int = 200,\n) -&gt; Union[float, np.float64]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/kappa_via_extinction/#kappa_from_extinction_looped","title":"kappa_from_extinction_looped","text":"<p>Show source in kappa_via_extinction.py:231</p> <p>Fit the extinction ratio to the kappa value for a set of measurements, looping over time indexes in <code>number_per_cm3</code>.</p> <p>This function is designed for analyzing data from a CAPS (Cavity Attenuated Phase Shift) instrument under varying humidities.</p>"},{"location":"API/particula_beta/data/process/kappa_via_extinction/#arguments_2","title":"Arguments","text":"<ul> <li><code>extinction_dry</code> - Array of dry aerosol extinction measurements.</li> <li><code>extinction_wet</code> - Array of wet aerosol extinction measurements.</li> <li><code>number_per_cm3</code> - Array of particle number concentrations in #/cm\u00b3.</li> <li><code>diameter</code> - Array of particle diameters in nanometers.</li> <li><code>water_activity_sizer</code> - Water activity (relative humidity/100) of the     sizing instrument's air.</li> <li><code>water_activity_sample_dry</code> - Water activity (relative humidity/100) of     the air for dry measurements.</li> <li><code>water_activity_sample_wet</code> - Water activity (relative humidity/100) of     the air for wet measurements.</li> <li><code>refractive_index_dry</code> - Refractive index of dry particles.     Default is 1.45.</li> <li><code>water_refractive_index</code> - Refractive index of water. Default is 1.33.</li> <li><code>wavelength</code> - Wavelength of the light source in nanometers.     Default is 450 nm.</li> <li><code>discretize</code> - If True, calculations are performed with discretized     parameter values to potentially improve performance.     Default is True.</li> </ul>"},{"location":"API/particula_beta/data/process/kappa_via_extinction/#returns_2","title":"Returns","text":"<p>A 2D array where each row corresponds to the time-indexed kappa value, lower and upper bounds of the kappa estimation, structured as [kappa, lower, upper].</p>"},{"location":"API/particula_beta/data/process/kappa_via_extinction/#signature_2","title":"Signature","text":"<pre><code>def kappa_from_extinction_looped(\n    extinction_dry: NDArray[np.float64],\n    extinction_wet: NDArray[np.float64],\n    number_per_cm3: NDArray[np.float64],\n    diameter: NDArray[np.float64],\n    water_activity_sizer: NDArray[np.float64],\n    water_activity_sample_dry: NDArray[np.float64],\n    water_activity_sample_wet: NDArray[np.float64],\n    refractive_index_dry: Union[complex, float] = 1.45,\n    water_refractive_index: Union[complex, float] = 1.33,\n    wavelength: float = 450,\n    discretize: bool = True,\n) -&gt; NDArray[np.float64]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/lognormal_2mode/","title":"Lognormal 2mode","text":"<p>Particula-beta Index / Particula Beta / Data / Process / Lognormal 2mode</p> <p>Auto-generated documentation for particula_beta.data.process.lognormal_2mode module.</p>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#attributes","title":"Attributes","text":"<ul> <li><code>logger</code> - Set up logging: logging.getLogger('particula')</li> </ul>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#cost_function","title":"cost_function","text":"<p>Show source in lognormal_2mode.py:26</p> <p>Cost function for the lognormal distribution with 2 modes.</p>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#arguments","title":"Arguments","text":"<ul> <li><code>params</code> - Combined array of mode_values, geometric_standard_deviation,     and number_of_particles.</li> <li><code>x_values</code> - The x-values (particle sizes).</li> <li><code>concentration_pdf</code> - The actual concentration PDF to fit.</li> </ul>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#returns","title":"Returns","text":"<p>The mean squared error between the actual and guessed concentration     PDF.</p>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#signature","title":"Signature","text":"<pre><code>def cost_function(\n    params: NDArray[np.float64],\n    x_values: NDArray[np.float64],\n    concentration_pdf: NDArray[np.float64],\n) -&gt; float: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#create_lognormal_2mode_from_fit","title":"create_lognormal_2mode_from_fit","text":"<p>Show source in lognormal_2mode.py:382</p> <p>Create a fitted PMF stream and concentration matrix based on optimized parameters.</p>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#arguments_1","title":"Arguments","text":"<ul> <li><code>parameters_stream</code> - Stream object containing the optimized parameters.</li> <li><code>radius_min</code> - Log10 of the minimum radius value in meters (default: -9).</li> <li><code>radius_max</code> - Log10 of the maximum radius value in meters (default: -6).</li> <li><code>num_radius_bins</code> - Number of radius bins to create between radius_min     and radius_max.</li> </ul>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#returns_1","title":"Returns","text":"<ul> <li><code>fitted_pmf_stream</code> - A Stream object containing the time and fitted     concentration PMF data.</li> <li><code>fitted_concentration_pmf</code> - A numpy array with the fitted     concentration PMF values.</li> </ul>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#signature_1","title":"Signature","text":"<pre><code>def create_lognormal_2mode_from_fit(\n    parameters_stream: Stream,\n    radius_min: float = 1e-09,\n    radius_max: float = 1e-06,\n    num_radius_bins: int = 250,\n) -&gt; Tuple[Stream, NDArray[np.float64]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#see-also","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#evaluate_fit","title":"evaluate_fit","text":"<p>Show source in lognormal_2mode.py:130</p> <p>Evaluate the best fit and calculate R\u00b2 score.</p>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#signature_2","title":"Signature","text":"<pre><code>def evaluate_fit(\n    best_result: dict[str, Any],\n    logspace_x: NDArray[np.float64],\n    concentration_pdf: NDArray[np.float64],\n) -&gt; Tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.float64], float]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#get_bounds","title":"get_bounds","text":"<p>Show source in lognormal_2mode.py:72</p> <p>Provide default bounds for optimization parameters.</p>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#signature_3","title":"Signature","text":"<pre><code>def get_bounds() -&gt; List[Tuple[float, Any]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#get_initial_guesses","title":"get_initial_guesses","text":"<p>Show source in lognormal_2mode.py:84</p> <p>Combine initial guesses into a single array.</p>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#signature_4","title":"Signature","text":"<pre><code>def get_initial_guesses(\n    mode_guess: NDArray[np.float64],\n    geometric_standard_deviation_guess: NDArray[np.float64],\n    number_of_particles_in_mode_guess: NDArray[np.float64],\n) -&gt; NDArray[np.float64]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#guess_and_optimize_looped","title":"guess_and_optimize_looped","text":"<p>Show source in lognormal_2mode.py:301</p> <p>Generate initial guesses using a machine learning model, optimize them, and return a Stream object with the results.</p>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#arguments_2","title":"Arguments","text":"<ul> <li><code>experiment_time</code> - Array of experiment time points.</li> <li><code>radius_m</code> - Array of particle radii in meters.</li> <li><code>concentration_m3_pdf</code> - 2D array of concentration PDFs for each     time point.</li> </ul>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#returns_2","title":"Returns","text":"<ul> <li><code>fitted_stream</code> - A Stream object containing the initial guesses,     optimized values, and R\u00b2 scores.</li> </ul>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#signature_5","title":"Signature","text":"<pre><code>def guess_and_optimize_looped(\n    experiment_time: NDArray[np.float64],\n    radius_m: NDArray[np.float64],\n    concentration_m3_pdf: NDArray[np.float64],\n) -&gt; Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#see-also_1","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#optimize_fit","title":"optimize_fit","text":"<p>Show source in lognormal_2mode.py:164</p> <p>Optimize the lognormal 2-mode distribution parameters using multiple optimization methods.</p>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#signature_6","title":"Signature","text":"<pre><code>def optimize_fit(\n    mode_guess: NDArray[np.float64],\n    geometric_standard_deviation_guess: NDArray[np.float64],\n    number_of_particles_in_mode_guess: NDArray[np.float64],\n    logspace_x: NDArray[np.float64],\n    concentration_pdf: NDArray[np.float64],\n    bounds: Optional[List[Tuple[float, Any]]] = None,\n    list_of_methods: Optional[List[str]] = None,\n) -&gt; Tuple[\n    NDArray[np.float64], NDArray[np.float64], NDArray[np.float64], float, dict[str, Any]\n]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#optimize_fit_looped","title":"optimize_fit_looped","text":"<p>Show source in lognormal_2mode.py:230</p> <p>Loop through the concentration PDFs to get the best optimization.</p>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#arguments_3","title":"Arguments","text":"<ul> <li><code>mode_guess</code> - Array of mode values.</li> <li><code>geometric_standard_deviation_guess</code> - Array of geometric standard     deviations.</li> <li><code>number_of_particles_in_mode_guess</code> - Array of number of particles.</li> <li><code>x_values</code> - Array of x-values (particle sizes).</li> <li><code>concentration_pdf</code> - Matrix of concentration PDF values.</li> <li><code>bounds</code> - List of bounds for optimization.</li> <li><code>list_of_methods</code> - List of optimization methods.</li> </ul>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#returns_3","title":"Returns","text":"<ul> <li><code>optimized_mode_values</code> - Optimized mode values.</li> <li><code>optimized_gsd</code> - Optimized geometric standard deviations.</li> <li><code>optimized_number_of_particles</code> - Optimized number of particles.</li> <li><code>r2</code> - R\u00b2 score.</li> <li><code>optimization_results</code> - Dictionary of optimization results.</li> </ul>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#signature_7","title":"Signature","text":"<pre><code>def optimize_fit_looped(\n    mode_guess: NDArray[np.float64],\n    geometric_standard_deviation_guess: NDArray[np.float64],\n    number_of_particles_in_mode_guess: NDArray[np.float64],\n    logspace_x: NDArray[np.float64],\n    concentration_pdf: NDArray[np.float64],\n    bounds: Optional[List[Tuple[float, Any]]] = None,\n    list_of_methods: Optional[List[str]] = None,\n) -&gt; Tuple[\n    NDArray[np.float64], NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]\n]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#run_optimization","title":"run_optimization","text":"<p>Show source in lognormal_2mode.py:99</p> <p>Perform the optimization using the specified method.</p>"},{"location":"API/particula_beta/data/process/lognormal_2mode/#signature_8","title":"Signature","text":"<pre><code>def run_optimization(\n    method: str,\n    initial_guess: NDArray[np.float64],\n    bounds: List[Tuple[float, Any]],\n    x_values: NDArray[np.float64],\n    concentration_pdf: NDArray[np.float64],\n) -&gt; Optional[dict[str, Any]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/mie_angular/","title":"Mie Angular","text":"<p>Particula-beta Index / Particula Beta / Data / Process / Mie Angular</p> <p>Auto-generated documentation for particula_beta.data.process.mie_angular module.</p>"},{"location":"API/particula_beta/data/process/mie_angular/#assign_scattering_thetas","title":"assign_scattering_thetas","text":"<p>Show source in mie_angular.py:100</p> <p>Assign scattering angles and efficiencies based on the z-axis position within the CAPS instrument.</p>"},{"location":"API/particula_beta/data/process/mie_angular/#arguments","title":"Arguments","text":"<ul> <li><code>alpha</code> - The forward scattering angle in radians.</li> <li><code>beta</code> - The backward scattering angle in radians.</li> <li><code>q_mie</code> - The Mie scattering efficiency.</li> <li><code>z_position</code> - The position along the z-axis in centimeters (cm).</li> <li><code>integrate_sphere_diameter_cm</code> - The diameter of the integrating sphere     in centimeters (cm).</li> </ul>"},{"location":"API/particula_beta/data/process/mie_angular/#returns","title":"Returns","text":"<p>Tuple: - The forward scattering angle (theta1) in radians. - The backward scattering angle (theta2) in radians. - The ideal scattering efficiency (qsca_ideal) for the given z-axis     position.</p>"},{"location":"API/particula_beta/data/process/mie_angular/#signature","title":"Signature","text":"<pre><code>def assign_scattering_thetas(\n    alpha: float,\n    beta: float,\n    q_mie: float,\n    z_position: Union[float, np.float64],\n    integrate_sphere_diameter_cm: float,\n) -&gt; Tuple[float, float, float]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/mie_angular/#calculate_scattering_angles","title":"calculate_scattering_angles","text":"<p>Show source in mie_angular.py:63</p> <p>Calculate forward and backward scattering angles for a given position along the z-axis within the CAPS instrument geometry.</p>"},{"location":"API/particula_beta/data/process/mie_angular/#arguments_1","title":"Arguments","text":"<ul> <li><code>z_position</code> - The position along the z-axis in centimeters (cm).</li> <li><code>integrate_sphere_diameter_cm</code> - The diameter of the integrating sphere     in centimeters (cm).</li> <li><code>tube_diameter_cm</code> - The diameter of the sample tube in centimeters (cm).</li> </ul>"},{"location":"API/particula_beta/data/process/mie_angular/#returns_1","title":"Returns","text":"<p>Tuple: - The forward scattering angle (alpha) in radians. - The backward scattering angle (beta) in radians.</p>"},{"location":"API/particula_beta/data/process/mie_angular/#signature_1","title":"Signature","text":"<pre><code>def calculate_scattering_angles(\n    z_position: Union[float, np.float64],\n    integrate_sphere_diameter_cm: float,\n    tube_diameter_cm: float,\n) -&gt; Tuple[float, float]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/mie_angular/#discretize_scattering_angles","title":"discretize_scattering_angles","text":"<p>Show source in mie_angular.py:17</p> <p>Discretize and cache the scattering function for a spherical particle with specified material properties and size.</p> <p>This function optimizes the performance of scattering calculations by caching results for frequently used parameters, thereby reducing the need for repeated calculations.</p>"},{"location":"API/particula_beta/data/process/mie_angular/#arguments_2","title":"Arguments","text":"<ul> <li><code>m_sphere</code> - The complex or real refractive index of the particle.</li> <li><code>wavelength</code> - The wavelength of the incident light in nanometers (nm).</li> <li><code>diameter</code> - The diameter of the particle in nanometers (nm).</li> <li><code>min_angle</code> - The minimum scattering angle in degrees to be considered in     the calculation. Defaults to 0.</li> <li><code>max_angle</code> - The maximum scattering angle in degrees to be considered in     the calculation. Defaults to 180.</li> <li><code>angular_resolution</code> - The resolution in degrees between calculated     scattering angles. Defaults to 1.</li> </ul>"},{"location":"API/particula_beta/data/process/mie_angular/#returns_2","title":"Returns","text":"<p>Tuple: - <code>-</code> measure - The scattering intensity as a function of angle. - <code>-</code> parallel - The scattering intensity for parallel polarization. - <code>-</code> perpendicular - The scattering intensity for perpendicular     polarization. - <code>-</code> unpolarized - The unpolarized scattering intensity.</p>"},{"location":"API/particula_beta/data/process/mie_angular/#signature_2","title":"Signature","text":"<pre><code>@lru_cache(maxsize=100000)\ndef discretize_scattering_angles(\n    m_sphere: Union[complex, float],\n    wavelength: float,\n    diameter: Union[float, np.float64],\n    min_angle: int = 0,\n    max_angle: int = 180,\n    angular_resolution: float = 1,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/mie_bulk/","title":"Mie Bulk","text":"<p>Particula-beta Index / Particula Beta / Data / Process / Mie Bulk</p> <p>Auto-generated documentation for particula_beta.data.process.mie_bulk module.</p>"},{"location":"API/particula_beta/data/process/mie_bulk/#compute_bulk_optics","title":"compute_bulk_optics","text":"<p>Show source in mie_bulk.py:129</p> <p>Compute bulk optical properties from size-dependent efficiency factors for a size distribution.</p> <p>This function calculates various bulk optical properties such as extinction, scattering, and backscattering coefficients based on the size distribution and corresponding efficiency factors.</p>"},{"location":"API/particula_beta/data/process/mie_bulk/#arguments","title":"Arguments","text":"<ul> <li><code>q_ext</code> - Array of extinction efficiency factors.</li> <li><code>q_sca</code> - Array of scattering efficiency factors.</li> <li><code>q_back</code> - Array of backscatter efficiency factors.</li> <li><code>q_ratio</code> - Array of backscatter-to-extinction ratio efficiency factors.</li> <li><code>g</code> - Array of asymmetry factors.</li> <li><code>area_dist</code> - Area-scaled size distribution array.</li> <li><code>extinction_only</code> - Flag indicating whether to compute only the     extinction coefficient.</li> <li><code>pms</code> - Flag indicating whether a probability mass distribution is used,     where the sum of all bins     represents the total number of particles.</li> <li><code>dp</code> - Array of particle diameters in nanometers.</li> </ul>"},{"location":"API/particula_beta/data/process/mie_bulk/#returns","title":"Returns","text":"<p>If <code>extinction_only</code> is True, returns an array with the bulk extinction coefficient. Otherwise, returns a tuple containing the bulk optical properties, including extinction, scattering, and backscattering coefficients, and possibly others depending on the input flags.</p>"},{"location":"API/particula_beta/data/process/mie_bulk/#signature","title":"Signature","text":"<pre><code>def compute_bulk_optics(\n    q_ext: NDArray[np.float64],\n    q_sca: NDArray[np.float64],\n    q_back: NDArray[np.float64],\n    q_ratio: NDArray[np.float64],\n    g: NDArray[np.float64],\n    area_dist: NDArray[np.float64],\n    extinction_only: bool,\n    pms: bool,\n    dp: NDArray[np.float64],\n) -&gt; Union[NDArray[np.float64], tuple[NDArray[np.float64], ...]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/mie_bulk/#discretize_auto_mieq","title":"discretize_auto_mieq","text":"<p>Show source in mie_bulk.py:21</p> <p>Compute Mie coefficients for a spherical particle based on its material properties, size, and the properties of the surrounding medium.</p> <p>This function uses the PyMieScatt library to calculate various Mie efficiencies and parameters for a single sphere, including extinction efficiency (q_ext), scattering efficiency (q_sca), absorption efficiency (q_abs), asymmetry factor (g), radiation pressure efficiency (q_pr), backscatter efficiency (q_back), and the ratio of backscatter to extinction efficiency (q_ratio).</p> <p>The function is optimized with an LRU (Least Recently Used) cache, which stores up to 100,000 recent computations to improve performance by avoiding repeated calculations for the same inputs.</p>"},{"location":"API/particula_beta/data/process/mie_bulk/#arguments_1","title":"Arguments","text":"<ul> <li><code>m_sphere</code> - The complex refractive index of the sphere. A real     number can be provided for non-absorbing materials.</li> <li><code>wavelength</code> - The wavelength of the incident light in nanometers (nm).</li> <li><code>diameter</code> - The diameter of the sphere in nanometers (nm).</li> <li><code>m_medium</code> - The refractive index of the surrounding medium.     Default is 1.0, corresponding to vacuum.</li> </ul>"},{"location":"API/particula_beta/data/process/mie_bulk/#returns_1","title":"Returns","text":"<p>Tuple:     - q_ext, Extinction efficiency.     - q_sca, Scattering efficiency.     - q_abs, Absorption efficiency.     - g, Asymmetry factor.     - q_pr, Radiation pressure efficiency.     - q_back, Backscatter efficiency.     - q_ratio, Ratio of backscatter to extinction efficiency.</p>"},{"location":"API/particula_beta/data/process/mie_bulk/#signature_1","title":"Signature","text":"<pre><code>@lru_cache(maxsize=100000)\ndef discretize_auto_mieq(\n    m_sphere: Union[complex, float],\n    wavelength: float,\n    diameter: float,\n    m_medium: float = 1.0,\n) -&gt; Tuple[float, ...]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/mie_bulk/#discretize_mie_parameters","title":"discretize_mie_parameters","text":"<p>Show source in mie_bulk.py:65</p> <p>Discretize the refractive index, wavelength, and diameters for Mie scattering calculations.</p> <p>This function improves numerical stability and performance by discretizing the refractive index of the material, the wavelength of incident light, and the diameters of particles. Discretization reduces the variability in input parameters, making Mie scattering computations more efficient by creating a more manageable set of unique calculations.</p>"},{"location":"API/particula_beta/data/process/mie_bulk/#arguments_2","title":"Arguments","text":"<ul> <li><code>m_sphere</code> - The complex or real refractive index of the particles.     This value is discretized to a specified base to reduce     input variability.</li> <li><code>wavelength</code> - The wavelength of incident light in nanometers (nm),     discretized to minimize variations in related computations.</li> <li><code>diameter</code> - The particle diameter or array of diameters in nanometers     (nm), discretized to a specified base to standardize input sizes     for calculations.</li> <li><code>base_m_sphere</code> - Optional; the base value to which the real and     imaginary parts of the refractive index are rounded.     Default is 0.001.</li> <li><code>base_wavelength</code> - Optional; the base value to which the wavelength is     rounded. Default is 1 nm.</li> <li><code>base_diameter</code> - Optional; the base value to which particle diameters     are rounded. Default is 5 nm.</li> </ul>"},{"location":"API/particula_beta/data/process/mie_bulk/#returns_2","title":"Returns","text":"<p>Tuple:     - The discretized refractive index (m_sphere).     - The discretized wavelength.     - The discretized diameter or array of diameters, suitable for use         in Mie scattering calculations with potentially improved         performance and reduced computational overhead.</p>"},{"location":"API/particula_beta/data/process/mie_bulk/#signature_2","title":"Signature","text":"<pre><code>def discretize_mie_parameters(\n    m_sphere: Union[complex, float],\n    wavelength: float,\n    diameter: Union[float, NDArray[np.float64]],\n    base_m_sphere: float = 0.001,\n    base_wavelength: float = 1,\n    base_diameter: float = 5,\n) -&gt; Tuple[Union[complex, float], float, Union[float, list[float]]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/mie_bulk/#format_mie_results","title":"format_mie_results","text":"<p>Show source in mie_bulk.py:193</p> <p>Format the output results of the Mie scattering calculations.</p>"},{"location":"API/particula_beta/data/process/mie_bulk/#arguments_3","title":"Arguments","text":"<ul> <li><code>b_ext</code> - Array of bulk extinction coefficients.</li> <li><code>b_sca</code> - Array of bulk scattering coefficients.</li> <li><code>b_abs</code> - Array of bulk absorption coefficients.</li> <li><code>big_g</code> - Array of asymmetry factors (g).</li> <li><code>b_pr</code> - Array of bulk radiation pressure efficiencies.</li> <li><code>b_back</code> - Array of bulk backscattering coefficients.</li> <li><code>b_ratio</code> - Array of backscatter-to-extinction ratios.</li> <li><code>as_dict</code> - Flag to determine if the results should be returned as a     dictionary.</li> </ul>"},{"location":"API/particula_beta/data/process/mie_bulk/#returns_3","title":"Returns","text":"<p>(dict, Tuple):     - If <code>as_dict</code> is True, returns a dictionary with the bulk optical         properties.     - If <code>as_dict</code> is False, returns a tuple of the bulk         optical properties in the following order,         (b_ext, b_sca, b_abs, big_g, b_pr, b_back, b_ratio).</p>"},{"location":"API/particula_beta/data/process/mie_bulk/#signature_3","title":"Signature","text":"<pre><code>def format_mie_results(\n    b_ext: NDArray[np.float64],\n    b_sca: NDArray[np.float64],\n    b_abs: NDArray[np.float64],\n    big_g: NDArray[np.float64],\n    b_pr: NDArray[np.float64],\n    b_back: NDArray[np.float64],\n    b_ratio: NDArray[np.float64],\n    as_dict: bool,\n) -&gt; Union[dict[str, NDArray[np.float64]], tuple[NDArray[np.float64], ...]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/mie_bulk/#mie_size_distribution","title":"mie_size_distribution","text":"<p>Show source in mie_bulk.py:237</p> <p>Calculate Mie scattering parameters for a size distribution of spherical particles.</p> <p>This function computes optical properties such as extinction, scattering, absorption coefficients, asymmetry factor, backscatter efficiency, and their ratios for a given size distribution of spherical particles. It supports various modes of calculation, including discretization of input parameters and optional truncation of the scattering efficiency.</p>"},{"location":"API/particula_beta/data/process/mie_bulk/#arguments_4","title":"Arguments","text":"<ul> <li><code>m_sphere</code> - The complex refractive index of the particles. Real values     can be used for non-absorbing materials.</li> <li><code>wavelength</code> - The wavelength of the incident light in nanometers (nm).</li> <li><code>diameter</code> - An array of particle diameters in nanometers (nm).</li> <li><code>number_per_cm3</code> - The number distribution of particles per cubic     centimeter (#/cm^3).</li> <li><code>n_medium</code> - The refractive index of the medium. Defaults to 1.0     (air or vacuum).</li> <li><code>pms</code> - Specifies if the size distribution is in probability mass form.     Default is True.</li> <li><code>as_dict</code> - If True, results are returned as a dictionary. Otherwise,     as a tuple. Default is False.</li> <li><code>extinction_only</code> - If True, only the extinction coefficient is     calculated and returned. Default is False.</li> <li><code>discretize</code> - If True, input parameters (m_sphere, wavelength, diameter)     are discretized for computation. Default is False.</li> <li><code>truncation_calculation</code> - Enables truncation of the scattering     efficiency based on a multiple of the backscattering coefficient.     Default is False.</li> <li><code>truncation_b_sca_multiple</code> - The multiple of the backscattering     coefficient used for truncating the scattering efficiency.     Required if <code>truncation_calculation</code> is True.</li> </ul>"},{"location":"API/particula_beta/data/process/mie_bulk/#returns_4","title":"Returns","text":"<p>(NDArray, dict, Tuple):     - An array of extinction coefficients if <code>extinction_only</code> is True.     - A dictionary of computed optical properties if <code>as_dict</code> is True.     - A tuple of computed optical properties otherwise.</p>"},{"location":"API/particula_beta/data/process/mie_bulk/#raises","title":"Raises","text":"<ul> <li><code>ValueError</code> - If <code>truncation_calculation</code> is True but     <code>truncation_b_sca_multiple</code> is not specified.</li> </ul>"},{"location":"API/particula_beta/data/process/mie_bulk/#signature_4","title":"Signature","text":"<pre><code>def mie_size_distribution(\n    m_sphere: Union[complex, float],\n    wavelength: float,\n    diameter: NDArray[np.float64],\n    number_per_cm3: NDArray[np.float64],\n    n_medium: float = 1.0,\n    pms: bool = True,\n    as_dict: bool = False,\n    extinction_only: bool = False,\n    discretize: bool = False,\n    truncation_calculation: bool = False,\n    truncation_b_sca_multiple: Optional[float] = None,\n) -&gt; Union[\n    NDArray[np.float64], dict[str, NDArray[np.float64]], Tuple[NDArray[np.float64], ...]\n]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/optical_instrument/","title":"Optical Instrument","text":"<p>Particula-beta Index / Particula Beta / Data / Process / Optical Instrument</p> <p>Auto-generated documentation for particula_beta.data.process.optical_instrument module.</p>"},{"location":"API/particula_beta/data/process/optical_instrument/#capsinstrumentkeywordbuilder","title":"CapsInstrumentKeywordBuilder","text":"<p>Show source in optical_instrument.py:17</p> <p>Builder class for CAPS Instrument Keywords dictionary.</p>"},{"location":"API/particula_beta/data/process/optical_instrument/#signature","title":"Signature","text":"<pre><code>class CapsInstrumentKeywordBuilder:\n    def __init__(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/process/optical_instrument/#capsinstrumentkeywordbuilderbuild","title":"CapsInstrumentKeywordBuilder().build","text":"<p>Show source in optical_instrument.py:96</p> <p>Validate and return the keywords dictionary.</p>"},{"location":"API/particula_beta/data/process/optical_instrument/#returns","title":"Returns","text":"<ul> <li><code>dict</code> - The validated keywords dictionary.</li> </ul>"},{"location":"API/particula_beta/data/process/optical_instrument/#signature_1","title":"Signature","text":"<pre><code>def build(self) -&gt; dict[str, Union[str, float, int, bool]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/optical_instrument/#capsinstrumentkeywordbuilderpre_build_check","title":"CapsInstrumentKeywordBuilder().pre_build_check","text":"<p>Show source in optical_instrument.py:81</p> <p>Check that all required parameters have been set.</p>"},{"location":"API/particula_beta/data/process/optical_instrument/#raises","title":"Raises","text":"<ul> <li><code>ValueError</code> - If any required keyword has not been set.</li> </ul>"},{"location":"API/particula_beta/data/process/optical_instrument/#signature_2","title":"Signature","text":"<pre><code>def pre_build_check(self): ...\n</code></pre>"},{"location":"API/particula_beta/data/process/optical_instrument/#capsinstrumentkeywordbuilderset_keyword","title":"CapsInstrumentKeywordBuilder().set_keyword","text":"<p>Show source in optical_instrument.py:42</p> <p>Set the keyword parameter for the activity calculation.</p>"},{"location":"API/particula_beta/data/process/optical_instrument/#arguments","title":"Arguments","text":"<ul> <li><code>keyword</code> - The keyword to set.</li> <li><code>value</code> - The value to set the keyword to.</li> </ul>"},{"location":"API/particula_beta/data/process/optical_instrument/#raises_1","title":"Raises","text":"<ul> <li><code>ValueError</code> - If the keyword is not recognized or the value type     is incorrect.</li> </ul>"},{"location":"API/particula_beta/data/process/optical_instrument/#signature_3","title":"Signature","text":"<pre><code>def set_keyword(self, keyword: str, value: Optional[Union[str, float, int, bool]]): ...\n</code></pre>"},{"location":"API/particula_beta/data/process/optical_instrument/#capsinstrumentkeywordbuilderset_keywords","title":"CapsInstrumentKeywordBuilder().set_keywords","text":"<p>Show source in optical_instrument.py:71</p> <p>Set multiple keywords at once.</p>"},{"location":"API/particula_beta/data/process/optical_instrument/#arguments_1","title":"Arguments","text":"<ul> <li><code>kwargs</code> - The keywords and their values to set.</li> </ul>"},{"location":"API/particula_beta/data/process/optical_instrument/#signature_4","title":"Signature","text":"<pre><code>def set_keywords(self, **kwargs: Union[str, float, int, bool]): ...\n</code></pre>"},{"location":"API/particula_beta/data/process/optical_instrument/#albedo_from_ext_scat","title":"albedo_from_ext_scat","text":"<p>Show source in optical_instrument.py:249</p> <p>Calculate the albedo from the extinction and scattering data in the stream.</p> <p>This function computes the absorption as the difference between extinction and scattering, and the single-scattering albedo as the ratio of scattering to extinction. If the extinction values are zero or negative, the albedo is set to <code>np.nan</code>.</p>"},{"location":"API/particula_beta/data/process/optical_instrument/#arguments_2","title":"Arguments","text":"<ul> <li><code>stream</code> - The datastream containing CAPS data.</li> <li><code>extinction_key</code> - The key for the extinction data in the stream.</li> <li><code>scattering_key</code> - The key for the scattering data in the stream.</li> <li><code>new_absorption_key</code> - The key where the calculated absorption will     be stored.</li> <li><code>new_albedo_key</code> - The key where the calculated albedo will     be stored.</li> </ul>"},{"location":"API/particula_beta/data/process/optical_instrument/#returns_1","title":"Returns","text":"<ul> <li><code>Stream</code> - The updated datastream with the new absorption and albedo values.</li> </ul>"},{"location":"API/particula_beta/data/process/optical_instrument/#raises_2","title":"Raises","text":"<ul> <li><code>KeyError</code> - If the provided extinction or scattering keys are not found     in the stream.</li> </ul>"},{"location":"API/particula_beta/data/process/optical_instrument/#signature_5","title":"Signature","text":"<pre><code>def albedo_from_ext_scat(\n    stream: Stream,\n    extinction_key: str,\n    scattering_key: str,\n    new_absorption_key: str,\n    new_albedo_key: str,\n) -&gt; Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/optical_instrument/#see-also","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/process/optical_instrument/#caps_processing","title":"caps_processing","text":"<p>Show source in optical_instrument.py:106</p> <p>Process CAPS data and SMPS data for kappa fitting, apply truncation corrections, and add the results to the caps stream.</p>"},{"location":"API/particula_beta/data/process/optical_instrument/#arguments_3","title":"Arguments","text":"<ul> <li><code>stream_size_distribution</code> - Stream containing size distribution data.</li> <li><code>stream_sizer_properties</code> - Stream containing sizer properties data.</li> <li><code>stream_caps</code> - Stream containing CAPS data.</li> <li><code>keywords</code> - Dictionary containing configuration parameters.</li> </ul>"},{"location":"API/particula_beta/data/process/optical_instrument/#returns_2","title":"Returns","text":"<p>Stream with processed CAPS data, including kappa fitting results and truncation corrections.</p>"},{"location":"API/particula_beta/data/process/optical_instrument/#signature_6","title":"Signature","text":"<pre><code>def caps_processing(\n    stream_size_distribution: Stream,\n    stream_sizer_properties: Stream,\n    stream_caps: Stream,\n    keywords: dict[str, Union[str, float, int, bool]],\n): ...\n</code></pre>"},{"location":"API/particula_beta/data/process/optical_instrument/#see-also_1","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/process/optical_instrument/#enhancement_ratio","title":"enhancement_ratio","text":"<p>Show source in optical_instrument.py:314</p> <p>Calculate the enhancement ratio from two data keys in the stream.</p> <p>This is the ratio between the numerator and the denominator. If the denominator is zero, then the ratio is set to <code>np.nan</code>. This function is useful for f(RH) calculations.</p>"},{"location":"API/particula_beta/data/process/optical_instrument/#arguments_4","title":"Arguments","text":"<ul> <li><code>stream</code> - The datastream containing the data.</li> <li><code>numerator_key</code> - The key for the numerator data in the stream.</li> <li><code>denominator_key</code> - The key for the denominator data in the stream.</li> <li><code>new_key</code> - The key where the calculated enhancement ratio will     be stored.</li> </ul>"},{"location":"API/particula_beta/data/process/optical_instrument/#returns_3","title":"Returns","text":"<ul> <li><code>Stream</code> - The updated datastream with the new enhancement ratio values.</li> </ul>"},{"location":"API/particula_beta/data/process/optical_instrument/#raises_3","title":"Raises","text":"<ul> <li><code>KeyError</code> - If the provided numerator or denominator keys are not found     in the stream.</li> </ul>"},{"location":"API/particula_beta/data/process/optical_instrument/#signature_7","title":"Signature","text":"<pre><code>def enhancement_ratio(\n    stream: Stream, numerator_key: str, denominator_key: str, new_key: str\n) -&gt; Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/optical_instrument/#see-also_2","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/process/sample_distribution/","title":"Sample Distribution","text":"<p>Particula-beta Index / Particula Beta / Data / Process / Sample Distribution</p> <p>Auto-generated documentation for particula_beta.data.process.sample_distribution module.</p>"},{"location":"API/particula_beta/data/process/sample_distribution/#get_samples_from_tabulated_pdf","title":"get_samples_from_tabulated_pdf","text":"<p>Show source in sample_distribution.py:10</p> <p>Draw random samples from a 1-D PDF that is supplied as tabulated values.</p> <p>The function assumes <code>x</code> is strictly increasing and <code>pdf</code> \u2265 0 for all points.  If <code>pdf</code> is not already normalised, it is normalised internally.</p>"},{"location":"API/particula_beta/data/process/sample_distribution/#arguments","title":"Arguments","text":"<ul> <li><code>x</code> - 1-D array of the abscissa values (e.g. particle radii in metres).</li> <li><code>pdf</code> - 1-D array of the unnormalised probability-density values     evaluated at each <code>x</code>.  Must be the same length as <code>x</code>.</li> <li><code>n_samples</code> - Number of random variates to return.</li> <li><code>rng</code> - Optional <code>np.random.Generator</code> for reproducibility.</li> </ul>"},{"location":"API/particula_beta/data/process/sample_distribution/#returns","title":"Returns","text":"<p>1-D NumPy array of length <code>n_samples</code> containing samples drawn from the distribution represented by (<code>x</code>, <code>pdf</code>).</p>"},{"location":"API/particula_beta/data/process/sample_distribution/#examples","title":"Examples","text":"<pre><code># Mixture of two lognormals, tabulated on a log-spaced grid\nimport numpy as np\nfrom scipy.stats import lognorm\n\nx_grid = np.geomspace(1e-9, 5e-7, 10000)\nmodes   = np.array([5e-8, 1e-7])\ngsds    = np.array([1.5,  2.0])\nweights = np.array([1e9, 5e9])\n\npdf_vals = sum(\n    w * lognorm(s=np.log(gsd), scale=mode).pdf(x_grid)\n    for w, gsd, mode in zip(weights, gsds, modes)\n)\n\nradii = sample_from_tabulated_pdf(x_grid, pdf_vals, 10_000)\n</code></pre>"},{"location":"API/particula_beta/data/process/sample_distribution/#signature","title":"Signature","text":"<pre><code>def get_samples_from_tabulated_pdf(\n    distribution_bins: NDArray[np.float64],\n    distribution_pdf: NDArray[np.float64],\n    number_of_samples: int,\n    random_generator: np.random.Generator | None = None,\n) -&gt; np.ndarray: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/scattering_truncation/","title":"Scattering Truncation","text":"<p>Particula-beta Index / Particula Beta / Data / Process / Scattering Truncation</p> <p>Auto-generated documentation for particula_beta.data.process.scattering_truncation module.</p>"},{"location":"API/particula_beta/data/process/scattering_truncation/#correction_for_distribution","title":"correction_for_distribution","text":"<p>Show source in scattering_truncation.py:247</p> <p>Correction for a size distribution of particles.</p> <p>Calculates the correction factor for scattering measurements due to truncation effects in aerosol size distribution measurements. This correction factor is used to adjust the measured scattering coefficient, accounting for the limited angular range of the instrument.</p>"},{"location":"API/particula_beta/data/process/scattering_truncation/#arguments","title":"Arguments","text":"<ul> <li><code>m_sphere</code> - The complex or real refractive index of the particles.</li> <li><code>wavelength</code> - The wavelength of the incident light in nanometers (nm).</li> <li><code>diameter_sizes</code> - An array of particle diameters in nanometers (nm)     corresponding to the size distribution.</li> <li><code>number_per_cm3</code> - An array of particle number concentrations     (#/cm^3) for each diameter in the size distribution.</li> <li><code>discretize</code> - If True, the calculation will use discretized values     for the refractive index, wavelength, and diameters to     potentially improve computation performance. Default is True.</li> </ul>"},{"location":"API/particula_beta/data/process/scattering_truncation/#returns","title":"Returns","text":"<p>Array:     The correction factor for scattering measurements. This factor is     dimensionless and is used to correct the measured scattering     coefficient for truncation effects, calculated as the ratio of     the ideal (full angular range) to truncated scattering coefficient.</p>"},{"location":"API/particula_beta/data/process/scattering_truncation/#examples","title":"Examples","text":"<p>b_sca_corrected = b_sca_measured * bsca_correction</p>"},{"location":"API/particula_beta/data/process/scattering_truncation/#signature","title":"Signature","text":"<pre><code>def correction_for_distribution(\n    m_sphere: Union[complex, float],\n    wavelength: float,\n    diameter_sizes: NDArray[np.float64],\n    number_per_cm3: NDArray[np.float64],\n    discretize: bool = True,\n) -&gt; Union[float, np.float64]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/scattering_truncation/#correction_for_humidified","title":"correction_for_humidified","text":"<p>Show source in scattering_truncation.py:315</p> <p>Truncation Correction for humidified aerosol measurements.</p> <p>Calculates the scattering correction for humidified aerosol measurements, accounting for water uptake by adjusting the aerosol's refractive index. This function requires the kappa values for the particles, which describe their hygroscopic growth.</p>"},{"location":"API/particula_beta/data/process/scattering_truncation/#arguments_1","title":"Arguments","text":"<ul> <li><code>kappa</code> - Hygroscopicity parameter kappa, indicating the water uptake     capability of the particles.</li> <li><code>number_per_cm3</code> - Number concentration of particles per cubic     centimeter (#/cm\u00b3) for each size bin.</li> <li><code>diameter</code> - Array of particle diameters in nanometers (nm).</li> <li><code>water_activity_sizer</code> - Water activity (relative humidity/100) of the     air sample used for sizing.</li> <li><code>water_activity_sample</code> - Water activity (relative humidity/100) of the     air sample in optical measurements.</li> <li><code>refractive_index_dry</code> - Refractive index of the dry particles.     Default is 1.45.</li> <li><code>water_refractive_index</code> - Refractive index of water. Default is 1.33.</li> <li><code>wavelength</code> - Wavelength of the incident light in nanometers (nm).     Default is 450.</li> <li><code>discretize</code> - If True, calculation uses discretized values for     refractive index, wavelength, and diameters to improve performance.     Default is True.</li> </ul>"},{"location":"API/particula_beta/data/process/scattering_truncation/#returns_1","title":"Returns","text":"<p>np.float64:     A numpy array of scattering correction factors for each particle     size in the distribution, to be applied to measured backscatter     coefficients to account for truncation effects due to humidity.</p>"},{"location":"API/particula_beta/data/process/scattering_truncation/#signature_1","title":"Signature","text":"<pre><code>def correction_for_humidified(\n    kappa: Union[float, np.float64],\n    number_per_cm3: NDArray[np.float64],\n    diameter: NDArray[np.float64],\n    water_activity_sizer: np.float64,\n    water_activity_sample: np.float64,\n    refractive_index_dry: Union[complex, float] = 1.45,\n    water_refractive_index: Union[complex, float] = 1.33,\n    wavelength: float = 450,\n    discretize: bool = True,\n) -&gt; np.float64: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/scattering_truncation/#correction_for_humidified_looped","title":"correction_for_humidified_looped","text":"<p>Show source in scattering_truncation.py:389</p> <p>Looped correction for humidified aerosol measurements.</p> <p>Corrects scattering measurements for aerosol particles to account for truncation errors in CAPS instrument. This correction is vital for accurate representation of particle scattering properties under different humidity conditions. The function iterates over time-indexed measurements, calculating corrections based on input parameters reflecting the particles' physical and chemical characteristics.</p>"},{"location":"API/particula_beta/data/process/scattering_truncation/#arguments_2","title":"Arguments","text":"<ul> <li><code>kappa</code> - Hygroscopicity parameter array for the aerosol particles,     indicating water uptake ability.</li> <li><code>number_per_cm3</code> - Time-indexed number concentration of particles     in #/cm\u00b3 for each size.</li> <li><code>diameter</code> - Particle diameters, crucial for calculating scattering     effects.</li> <li><code>water_activity_sizer</code> - Water activity measured by the sizing     instrument, indicating relative humidity.</li> <li><code>water_activity_sample</code> - Sample water activity, corresponding to     the ambient conditions during measurement.</li> <li><code>refractive_index_dry</code> - Refractive index of the dry particles,     affecting their scattering behavior. Default is 1.45.</li> <li><code>water_refractive_index</code> - Refractive index of water, important     for calculations involving humidified conditions. Default is 1.33.</li> <li><code>wavelength</code> - Wavelength of the incident light in nanometers,     which influences scattering intensity. Default is 450.</li> <li><code>discretize</code> - If set to True, performs discretized calculations for     potentially improved computational performance. Default is True.</li> </ul>"},{"location":"API/particula_beta/data/process/scattering_truncation/#returns_2","title":"Returns","text":"<p>An array of corrected scattering multipliers for each time index, accounting for aerosol particle size, composition, and environmental conditions.</p>"},{"location":"API/particula_beta/data/process/scattering_truncation/#notes","title":"Notes","text":"<p>The correction process includes data nan checks for missing values, ensuring robust and reliable correction outcomes.</p>"},{"location":"API/particula_beta/data/process/scattering_truncation/#signature_2","title":"Signature","text":"<pre><code>def correction_for_humidified_looped(\n    kappa: NDArray[np.float64],\n    number_per_cm3: NDArray[np.float64],\n    diameter: NDArray[np.float64],\n    water_activity_sizer: NDArray[np.float64],\n    water_activity_sample: NDArray[np.float64],\n    refractive_index_dry: Union[complex, float] = 1.45,\n    water_refractive_index: Union[complex, float] = 1.33,\n    wavelength: float = 450,\n    discretize: bool = True,\n) -&gt; NDArray[np.float64]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/scattering_truncation/#get_truncated_scattering","title":"get_truncated_scattering","text":"<p>Show source in scattering_truncation.py:22</p> <p>Extracts the truncated scattering intensity and corresponding angles based on the given truncation angles.</p>"},{"location":"API/particula_beta/data/process/scattering_truncation/#arguments_3","title":"Arguments","text":"<ul> <li><code>su</code> - The scattering intensity for unpolarized light as a function of     angle.</li> <li><code>theta</code> - The array of angles corresponding to the scattering intensity     measurements.</li> <li><code>theta1</code> - The lower bound of the angle range for truncation in radians.</li> <li><code>theta2</code> - The upper bound of the angle range for truncation in radians.</li> </ul>"},{"location":"API/particula_beta/data/process/scattering_truncation/#returns_3","title":"Returns","text":"<p>Tuple (np.ndarray, np.ndarray):     A tuple containing the truncated scattering intensity and the     corresponding angles within the truncated range.</p>"},{"location":"API/particula_beta/data/process/scattering_truncation/#signature_3","title":"Signature","text":"<pre><code>def get_truncated_scattering(\n    scattering_unpolarized: np.ndarray, theta: np.ndarray, theta1: float, theta2: float\n) -&gt; Tuple[np.ndarray, np.ndarray]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/scattering_truncation/#trunc_mono","title":"trunc_mono","text":"<p>Show source in scattering_truncation.py:51</p> <p>Truncation correction for monodisperse aerosol particle.</p> <p>Calculates the single scattering albedo (SSA) correction due to truncation for monodisperse aerosol measurements using the CAPS-PM-SSA instrument. The correction accounts for the incomplete angular range of scattering measurements due to the instrument's geometry.</p>"},{"location":"API/particula_beta/data/process/scattering_truncation/#arguments_4","title":"Arguments","text":"<ul> <li><code>m_sphere</code> - Complex or real refractive index of the aerosol.</li> <li><code>wavelength</code> - Wavelength of light in nanometers used in the CAPS     instrument.</li> <li><code>diameter</code> - Diameter of the monodisperse aerosol in nanometers.</li> <li><code>full_output</code> - If True, additional details about the calculation     are returned, including z-axis values, angles of integration,     and both truncated     and ideal scattering efficiencies. calibrated_trunc : bool, optional     If True, applies a numberical calibration factor to the truncation     correction, so 150 is 1. Default is True. discretize : bool, optional     If True, discretizes the input parameters for potentially improved     stability/performance in scattering function calculations. Can not     be done for full_output=True</li> </ul>"},{"location":"API/particula_beta/data/process/scattering_truncation/#returns_4","title":"Returns","text":"<p>Tuple:     If fullOutput is False, returns only the truncation correction     factor. If fullOutput is True, returns a tuple containing the     truncation correction factor, z-axis positions, truncated     scattering efficiency, ideal scattering efficiency, forward     scattering angle, and backward scattering angle.</p>"},{"location":"API/particula_beta/data/process/scattering_truncation/#signature_4","title":"Signature","text":"<pre><code>@lru_cache(maxsize=100000)\ndef trunc_mono(\n    m_sphere: Union[complex, float],\n    wavelength: float,\n    diameter: float,\n    full_output: bool = False,\n    calibrated_trunc: bool = True,\n    discretize: bool = True,\n) -&gt; Union[\n    float, Tuple[float, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/scattering_truncation/#truncation_for_diameters","title":"truncation_for_diameters","text":"<p>Show source in scattering_truncation.py:194</p> <p>Truncation correction for an array of particle diameters.</p> <p>Calculates the truncation correction for an array of particle diameters given a specific refractive index and wavelength. This function is particularly useful for aerosol optical property measurements where truncation effects due to instrument geometry need to be accounted for.</p>"},{"location":"API/particula_beta/data/process/scattering_truncation/#arguments_5","title":"Arguments","text":"<ul> <li><code>m_sphere</code> - The complex or real refractive index of the particles.</li> <li><code>wavelength</code> - The wavelength of the incident light in nanometers (nm).</li> <li><code>diameter_sizes</code> - An array of particle diameters in nanometers (nm) for     which the truncation correction will be calculated.</li> <li><code>discretize</code> - A flag indicating whether to discretize the input     parameters for potentially improved calculation performance.     Default is True.</li> <li><code>calibrated_trunc</code> - If True, applies a numberical calibration factor to     the truncation correction, so 150 is 1. Default is True.</li> </ul>"},{"location":"API/particula_beta/data/process/scattering_truncation/#returns_5","title":"Returns","text":"<p>An array of truncation corrections corresponding to the input array of particle diameters.</p>"},{"location":"API/particula_beta/data/process/scattering_truncation/#signature_5","title":"Signature","text":"<pre><code>def truncation_for_diameters(\n    m_sphere: Union[complex, float],\n    wavelength: float,\n    diameter_sizes: NDArray[np.float64],\n    discretize: bool = True,\n    calibrated_trunc: bool = True,\n) -&gt; NDArray[np.float64]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/size_distribution/","title":"Size Distribution","text":"<p>Particula-beta Index / Particula Beta / Data / Process / Size Distribution</p> <p>Auto-generated documentation for particula_beta.data.process.size_distribution module.</p>"},{"location":"API/particula_beta/data/process/size_distribution/#iterate_merge_distributions","title":"iterate_merge_distributions","text":"<p>Show source in size_distribution.py:359</p> <p>Merge two sets of particle size distributions using linear weighting.</p>"},{"location":"API/particula_beta/data/process/size_distribution/#arguments","title":"Arguments","text":"<ul> <li><code>concentration_lower</code> - The concentration of particles in the lower     distribution.</li> <li><code>diameters_lower</code> - The diameters corresponding to the lower distribution.</li> <li><code>concentration_upper</code> - The concentration of particles in the upper     distribution.</li> <li><code>diameters_upper</code> - The diameters corresponding to the upper distribution.</li> </ul>"},{"location":"API/particula_beta/data/process/size_distribution/#returns","title":"Returns","text":"<p>Tuple: - The merged diameter distribution. - The merged concentration distribution.</p>"},{"location":"API/particula_beta/data/process/size_distribution/#signature","title":"Signature","text":"<pre><code>def iterate_merge_distributions(\n    concentration_lower: np.ndarray,\n    diameters_lower: np.ndarray,\n    concentration_upper: np.ndarray,\n    diameters_upper: np.ndarray,\n) -&gt; tuple[np.ndarray, np.ndarray]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/size_distribution/#mean_properties","title":"mean_properties","text":"<p>Show source in size_distribution.py:19</p> <p>Calculate the mean properties of the size distribution.</p>"},{"location":"API/particula_beta/data/process/size_distribution/#arguments_1","title":"Arguments","text":"<ul> <li><code>sizer_dndlogdp</code> - Array of particle concentrations in each bin.</li> <li><code>sizer_diameter</code> - Array of bin center diameters.</li> <li><code>total_concentration</code> - Optional; the total concentration of particles     in the distribution. If not provided, it will be calculated.</li> <li><code>sizer_limits</code> - Optional; the lower and upper limits of the size     range of interest. If not provided, the full range will be used.</li> </ul>"},{"location":"API/particula_beta/data/process/size_distribution/#returns_1","title":"Returns","text":"<p>Tuple: - Total concentration of particles in the distribution. - Total mass of particles in the distribution. - Mean diameter of the distribution by number. - Mean diameter of the distribution by volume. - Geometric mean diameter of the distribution. - Mode diameter of the distribution by number. - Mode diameter of the distribution by volume.</p>"},{"location":"API/particula_beta/data/process/size_distribution/#signature_1","title":"Signature","text":"<pre><code>def mean_properties(\n    sizer_dndlogdp: np.ndarray,\n    sizer_diameter: np.ndarray,\n    total_concentration: Optional[float] = None,\n    sizer_limits: Optional[list] = None,\n) -&gt; Tuple[float, float, float, float, float, float, float]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/size_distribution/#merge_distributions","title":"merge_distributions","text":"<p>Show source in size_distribution.py:271</p> <p>Merge two particle size distributions using linear weighting, accounting for mobility versus aerodynamic diameters.</p>"},{"location":"API/particula_beta/data/process/size_distribution/#arguments_2","title":"Arguments","text":"<ul> <li><code>concentration_lower</code> - The concentration of particles in the lower     distribution.</li> <li><code>diameters_lower</code> - The diameters corresponding to the lower distribution.</li> <li><code>concentration_upper</code> - The concentration of particles in the upper     distribution.</li> <li><code>diameters_upper</code> - The diameters corresponding to the upper distribution.</li> </ul>"},{"location":"API/particula_beta/data/process/size_distribution/#returns_2","title":"Returns","text":"<p>Tuple: - <code>-</code> new_2d - The merged concentration distribution. - <code>-</code> new_diameter - The merged diameter distribution.</p>"},{"location":"API/particula_beta/data/process/size_distribution/#notes","title":"Notes","text":"<p>Add process the moblity vs aerodynamic diameters</p>"},{"location":"API/particula_beta/data/process/size_distribution/#signature_2","title":"Signature","text":"<pre><code>def merge_distributions(\n    concentration_lower: np.ndarray,\n    diameters_lower: np.ndarray,\n    concentration_upper: np.ndarray,\n    diameters_upper: np.ndarray,\n) -&gt; tuple[np.ndarray, np.ndarray]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/size_distribution/#merge_size_distribution","title":"merge_size_distribution","text":"<p>Show source in size_distribution.py:404</p> <p>Merge two particle size distributions using linear weighting. The concentrations should be in dN/dlogDp.</p>"},{"location":"API/particula_beta/data/process/size_distribution/#arguments_3","title":"Arguments","text":"<ul> <li><code>stream_lower</code> - The stream with the lower size range, e.g., from an SMPS.</li> <li><code>stream_upper</code> - The stream with the upper size range, e.g., from an     OPS or APS.</li> <li><code>lower_units</code> - The units of the lower distribution. Default is 'nm'.</li> <li><code>upper_units</code> - The units of the upper distribution. Default is 'um'.</li> </ul>"},{"location":"API/particula_beta/data/process/size_distribution/#returns_3","title":"Returns","text":"<ul> <li><code>Stream</code> - A stream object containing the merged size distribution.</li> </ul>"},{"location":"API/particula_beta/data/process/size_distribution/#signature_3","title":"Signature","text":"<pre><code>def merge_size_distribution(\n    stream_lower: Stream,\n    stream_upper: Stream,\n    lower_units: str = \"nm\",\n    upper_units: str = \"um\",\n) -&gt; object: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/size_distribution/#see-also","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/process/size_distribution/#resample_distribution","title":"resample_distribution","text":"<p>Show source in size_distribution.py:446</p> <p>Resample a particle size distribution to a new set of diameters using numpy interpolation. Extrapolated values will be set to NaN.</p>"},{"location":"API/particula_beta/data/process/size_distribution/#arguments_4","title":"Arguments","text":"<ul> <li><code>stream</code> - The stream object containing the size distribution to resample.</li> <li><code>new_diameters</code> - The new diameters to which the distribution will be     resampled.</li> <li><code>concentration_scale</code> - The concentration scale of the distribution.     Options are 'dn/dlogdp', 'dn', 'pms'     (which is equivalent to 'dn'), or 'pdf'. Default is 'dn/dlogdp'.</li> <li><code>clone</code> - Whether to clone the stream before resampling. Default is False.</li> </ul>"},{"location":"API/particula_beta/data/process/size_distribution/#returns_4","title":"Returns","text":"<ul> <li><code>Stream</code> - The resampled stream object.</li> </ul>"},{"location":"API/particula_beta/data/process/size_distribution/#signature_4","title":"Signature","text":"<pre><code>def resample_distribution(\n    stream: Stream,\n    new_diameters: np.ndarray,\n    concentration_scale: str = \"dn/dlogdp\",\n    clone: bool = False,\n) -&gt; Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/size_distribution/#see-also_1","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/process/size_distribution/#sizer_mean_properties","title":"sizer_mean_properties","text":"<p>Show source in size_distribution.py:110</p> <p>Calculate the mean properties of the size distribution and return the updated stream.</p>"},{"location":"API/particula_beta/data/process/size_distribution/#arguments_5","title":"Arguments","text":"<ul> <li><code>stream</code> - The stream containing the size distribution data to process.</li> <li><code>sizer_limits</code> - A list specifying the lower and upper limits of the     size range of interest, in the units specified by <code>diameter_units</code>.     Default is None, which means the full range is used.</li> <li><code>density</code> - The density of the particles in g/cm\u00b3. Default is 1.5 g/cm\u00b3.</li> <li><code>diameter_units</code> - The units of the diameter. Default is 'nm'. The     specified units will be converted to nanometers.</li> </ul>"},{"location":"API/particula_beta/data/process/size_distribution/#returns_5","title":"Returns","text":"<ul> <li><code>Stream</code> - The updated stream with the mean properties added.</li> </ul>"},{"location":"API/particula_beta/data/process/size_distribution/#signature_5","title":"Signature","text":"<pre><code>def sizer_mean_properties(\n    stream: Stream,\n    sizer_limits: Optional[List[float]] = None,\n    density: float = 1.5,\n    diameter_units: str = \"nm\",\n) -&gt; Stream: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/size_distribution/#see-also_2","title":"See also","text":"<ul> <li>Stream</li> </ul>"},{"location":"API/particula_beta/data/process/stats/","title":"Stats","text":"<p>Particula-beta Index / Particula Beta / Data / Process / Stats</p> <p>Auto-generated documentation for particula_beta.data.process.stats module.</p>"},{"location":"API/particula_beta/data/process/stats/#average_to_interval","title":"average_to_interval","text":"<p>Show source in stats.py:102</p> <p>Calculate the average of the data stream over the specified time intervals.</p> <p>This function calculates the average of the data stream over a series of time intervals specified by <code>average_interval_array</code>. The average and standard deviation of the data are calculated for each interval, and the results are returned as two arrays.</p>"},{"location":"API/particula_beta/data/process/stats/#arguments","title":"Arguments","text":"<pre><code>- `time_raw` *np.ndarray* - An array of timestamps, sorted in ascending\n    order.\n- `average_interval` *float* - The length of each time interval in seconds.\n- `average_interval_array` *np.ndarray* - An array of timestamps\n    representing\n    the start times of each time interval.\n- `data_raw` *np.ndarray* - An array of data points corresponding to the\n    timestamps in `time_raw`.\n- `average_data` *np.ndarray* - An empty array of shape\n    (num_channels, num_intervals)that will be filled with the\n    average data for each time interval.\n- `average_data_std` *np.ndarray* - An empty array of shape\n    (num_channels, num_intervals) that will be filled with the standard\n    deviation of the data for each time interval.\n</code></pre>"},{"location":"API/particula_beta/data/process/stats/#returns","title":"Returns","text":"<pre><code>- `Tuple[np.ndarray,` *np.ndarray]* - A tuple containing the average data\n    and the standard deviation of the data, both as arrays of shape\n    (num_channels, num_intervals).\n</code></pre>"},{"location":"API/particula_beta/data/process/stats/#signature","title":"Signature","text":"<pre><code>def average_to_interval(\n    time_raw: np.ndarray,\n    data_raw: np.ndarray,\n    average_interval: float,\n    average_interval_array: np.ndarray,\n    average_data: np.ndarray,\n    average_data_std: np.ndarray,\n) -&gt; Tuple[np.ndarray, np.ndarray]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/stats/#distribution_integration","title":"distribution_integration","text":"<p>Show source in stats.py:272</p> <p>Performs either PDF integration or PMS integration based on the input. This function supports broadcasting where x_array has shape (m,) and distribution has shape (n, m).</p>"},{"location":"API/particula_beta/data/process/stats/#arguments_1","title":"Arguments","text":"<pre><code>- `distribution` - The distribution array to integrate.\n    It should have a shape of (n, m).\n- `x_array` - The x-values array for PDF\n    integration. It should have a shape of (m,).\n    If None, PMS integration is performed. Defaults to None.\n- `axis` - The axis along which to perform the integration\n    for PDF or the sum for PMS.\n    Defaults to 0.\n</code></pre>"},{"location":"API/particula_beta/data/process/stats/#returns_1","title":"Returns","text":"<pre><code>- `np.ndarray` - The result of the integration. If PDF integration is\nperformed, the result will have a shape of (n,) if axis=0 or (m,)\nif axis=1. If PMS integration is performed, the result will be a\nsingle value if axis=None, or an array with reduced dimensionality\notherwise.\n</code></pre>"},{"location":"API/particula_beta/data/process/stats/#signature_1","title":"Signature","text":"<pre><code>def distribution_integration(\n    distribution: np.ndarray, x_array: Optional[np.ndarray] = None, axis: int = 0\n) -&gt; np.ndarray: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/stats/#mask_outliers","title":"mask_outliers","text":"<p>Show source in stats.py:221</p> <p>Create a boolean mask for outliers in a data array. Outliers are defined as values that are either above or below a specified threshold, or that are equal to a specified value. Not all parameters need to be specified. If <code>invert</code> is True, the mask will be inverted. The mask will be True for False for outliers and True for non-outliers.</p>"},{"location":"API/particula_beta/data/process/stats/#arguments_2","title":"Arguments","text":"<pre><code>- `data` *np.ndarray* - The data array to be masked.\n- `bottom` *float* - The lower threshold for outliers.\n- `top` *float* - The upper threshold for outliers.\n- `value` *float* - The value to be masked.\n- `invert` *bool* - If True, the mask will be inverted.\n</code></pre>"},{"location":"API/particula_beta/data/process/stats/#returns_2","title":"Returns","text":"<pre><code>- `np.ndarray` - A boolean mask for the outliers in the data array. Mask is\n    True for non-outliers and False for outliers, and the same shape as\n    the data array.\n</code></pre>"},{"location":"API/particula_beta/data/process/stats/#signature_2","title":"Signature","text":"<pre><code>def mask_outliers(\n    data: np.ndarray,\n    bottom: Optional[float] = None,\n    top: Optional[float] = None,\n    value: Optional[float] = None,\n    invert: Optional[bool] = False,\n) -&gt; np.ndarray: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/stats/#merge_formatting","title":"merge_formatting","text":"<p>Show source in stats.py:11</p> <p>Formats two data arrays and their headers so that the data new can be subsiqently added to data current.</p>"},{"location":"API/particula_beta/data/process/stats/#arguments_3","title":"Arguments","text":"<ul> <li><code>data_current</code> np.ndarray - First data array to merge.</li> <li><code>header_current</code> list - Header for the first data array.</li> <li><code>data_new</code> np.ndarray - Second data array to merge.</li> <li><code>header_new</code> list - Header for the second data array.</li> </ul>"},{"location":"API/particula_beta/data/process/stats/#returns_3","title":"Returns","text":"<p>(np.ndarray, list, np.array, list): A tuple formatted data and headers.</p>"},{"location":"API/particula_beta/data/process/stats/#raises","title":"Raises","text":"<ul> <li><code>ValueError</code> - If the data arrays are not the same shape.</li> <li><code>ValueError</code> - If the headers are not the same length.</li> </ul>"},{"location":"API/particula_beta/data/process/stats/#signature_3","title":"Signature","text":"<pre><code>def merge_formatting(\n    data_current: np.ndarray,\n    header_current: list,\n    data_new: np.ndarray,\n    header_new: list,\n) -&gt; Tuple[np.ndarray, list, np.ndarray, list]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/ml_analysis/","title":"Ml Analysis","text":"<p>Particula-beta Index / Particula Beta / Data / Process / Ml Analysis</p> <p>Auto-generated documentation for particula_beta.data.process.ml_analysis module.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/#modules","title":"Modules","text":"<ul> <li>Generate And Train 2mode Sizer</li> <li>Get Ml Folder</li> <li>Run Ml Trainings</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/","title":"Generate And Train 2mode Sizer","text":"<p>Particula-beta Index / Particula Beta / Data / Process / Ml Analysis / Generate And Train 2mode Sizer</p> <p>Auto-generated documentation for particula_beta.data.process.ml_analysis.generate_and_train_2mode_sizer module.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#attributes","title":"Attributes","text":"<ul> <li> <p><code>logger</code> - Set up logging: logging.getLogger('particula')</p> </li> <li> <p><code>TOTAL_NUMBER_SIMULATED</code> - Training parameters: 10000000</p> </li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#create_pipeline","title":"create_pipeline","text":"<p>Show source in generate_and_train_2mode_sizer.py:176</p> <p>Create a pipeline with normalization and MLPRegressor model.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#returns","title":"Returns","text":"<p>A scikit-learn Pipeline object.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#signature","title":"Signature","text":"<pre><code>def create_pipeline() -&gt; Pipeline: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#evaluate_pipeline","title":"evaluate_pipeline","text":"<p>Show source in generate_and_train_2mode_sizer.py:308</p> <p>Evaluate the pipeline and print the mean squared error for each target.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#arguments","title":"Arguments","text":"<ul> <li><code>pipeline</code> - The trained pipeline.</li> <li><code>X_test</code> - The test feature array.</li> <li><code>y_test</code> - The test target array.</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#signature_1","title":"Signature","text":"<pre><code>def evaluate_pipeline(\n    pipeline: Pipeline, x_test: NDArray[np.float64], y_test: NDArray[np.float64]\n) -&gt; None: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#generate_simulated_data","title":"generate_simulated_data","text":"<p>Show source in generate_and_train_2mode_sizer.py:46</p> <p>Generate simulated lognormal aerosol particle size distributions.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#arguments_1","title":"Arguments","text":"<ul> <li><code>total_number_simulated</code> - Total number of simulated distributions.</li> <li><code>number_of_modes_sim</code> - Number of modes to simulate (1, 2, or 3).</li> <li><code>x_array_max_index</code> - Number of size bins in the particle size array.</li> <li><code>lower_bound_gsd</code> - Lower bound for the geometric standard deviation.</li> <li><code>upper_bound_gsd</code> - Upper bound for the geometric standard deviation.</li> <li><code>seed</code> - Random seed for reproducibility.</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#returns_1","title":"Returns","text":"<ul> <li><code>x_values</code> - Array of particle sizes.</li> <li><code>mode_index_sim</code> - Array of simulated mode indices.</li> <li><code>geomertic_standard_deviation_sim</code> - Array of simulated geometric     standard deviations (GSDs).</li> <li><code>number_of_particles_sim</code> - Array of simulated relative number     concentrations.</li> <li><code>number_pdf_sim</code> - Array of simulated probability density     functions (PDFs).</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#signature_2","title":"Signature","text":"<pre><code>def generate_simulated_data(\n    total_number_simulated: int = 10000,\n    number_of_modes_sim: int = 2,\n    x_array_max_index: int = 128,\n    lower_bound_gsd: float = 1.0,\n    upper_bound_gsd: float = 2.0,\n    seed: int = 0,\n) -&gt; Tuple[\n    NDArray[np.float64],\n    NDArray[np.int64],\n    NDArray[np.float64],\n    NDArray[np.float64],\n    NDArray[np.float64],\n]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#load_and_cache_pipeline","title":"load_and_cache_pipeline","text":"<p>Show source in generate_and_train_2mode_sizer.py:357</p> <p>Load and cache the ML pipeline if not already loaded.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#arguments_2","title":"Arguments","text":"<ul> <li><code>filename</code> - Path to the pipeline file.</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#returns_2","title":"Returns","text":"<p>The loaded pipeline.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#signature_3","title":"Signature","text":"<pre><code>def load_and_cache_pipeline(filename: str) -&gt; Pipeline: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#load_pipeline","title":"load_pipeline","text":"<p>Show source in generate_and_train_2mode_sizer.py:345</p> <p>Load a pipeline from a file.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#arguments_3","title":"Arguments","text":"<ul> <li><code>filename</code> - The filename to load the pipeline from.</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#returns_3","title":"Returns","text":"<p>The loaded pipeline.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#signature_4","title":"Signature","text":"<pre><code>def load_pipeline(filename: str) -&gt; Pipeline: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#lognormal_2mode_ml_guess","title":"lognormal_2mode_ml_guess","text":"<p>Show source in generate_and_train_2mode_sizer.py:443</p> <p>Load the machine learning pipeline, interpolate the concentration PDF, and predict lognormal parameters.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#arguments_4","title":"Arguments","text":"<ul> <li><code>file_name</code> - Path to the saved ML pipeline file.</li> <li><code>x_values</code> - Array of x-values (particle sizes).</li> <li><code>concentration_pdf</code> - Array of concentration PDF values.</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#returns_4","title":"Returns","text":"<ul> <li><code>mode_values_guess</code> - Predicted mode values after rescaling.</li> <li><code>geometric_standard_deviation_guess</code> - Predicted geometric standard     deviations after rescaling.</li> <li><code>number_of_particles_guess</code> - Predicted number of particles after     rescaling.</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#signature_5","title":"Signature","text":"<pre><code>def lognormal_2mode_ml_guess(\n    logspace_x: NDArray[np.float64], concentration_pdf: NDArray[np.float64]\n) -&gt; Tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#looped_lognormal_2mode_ml_guess","title":"looped_lognormal_2mode_ml_guess","text":"<p>Show source in generate_and_train_2mode_sizer.py:527</p> <p>Loop through the concentration PDFs to get the best guess.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#arguments_5","title":"Arguments","text":"<ul> <li><code>logspace_x</code> - Array of x-values (particle sizes).</li> <li><code>concentration_pdf</code> - Matrix of concentration PDF values.</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#returns_5","title":"Returns","text":"<p>Tuple: - <code>-</code> mode_values_guess - Predicted mode values after rescaling. - <code>-</code> geometric_standard_deviation_guess - Predicted geometric standard     deviations after rescaling. - <code>-</code> number_of_particles_guess - Predicted number of particles after     rescaling.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#signature_6","title":"Signature","text":"<pre><code>def looped_lognormal_2mode_ml_guess(\n    logspace_x: NDArray[np.float64], concentration_pdf: NDArray[np.float64]\n) -&gt; Tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#normalize_max","title":"normalize_max","text":"<p>Show source in generate_and_train_2mode_sizer.py:130</p> <p>Normalize each sample in X by dividing by its maximum value.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#arguments_6","title":"Arguments","text":"<ul> <li><code>X</code> - The input array to be normalized.</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#returns_6","title":"Returns","text":"<p>The normalized array.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#signature_7","title":"Signature","text":"<pre><code>def normalize_max(x_input: NDArray[np.float64]) -&gt; NDArray[np.float64]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#normalize_targets","title":"normalize_targets","text":"<p>Show source in generate_and_train_2mode_sizer.py:142</p> <p>Normalize the mode index, GSD, and relative number concentration.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#arguments_7","title":"Arguments","text":"<ul> <li><code>mode_index_sim</code> - Array of mode indices.</li> <li><code>geomertic_standard_deviation_sim</code> - Array of geometric standard     deviations (GSDs).</li> <li><code>number_of_particles_sim</code> - Array of relative number concentrations.</li> <li><code>x_array_max_index</code> - Maximum index for the mode.</li> <li><code>lower_bound_gsd</code> - Lower bound for the geometric standard     deviation (GSD).</li> <li><code>upper_bound_gsd</code> - Upper bound for the geometric standard     deviation (GSD).</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#returns_7","title":"Returns","text":"<ul> <li><code>y</code> - Normalized array combining mode indices, GSDs, and relative     number concentrations.</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#signature_8","title":"Signature","text":"<pre><code>def normalize_targets(\n    mode_index_sim: NDArray[np.int64],\n    geomertic_standard_deviation_sim: NDArray[np.float64],\n    number_of_particles_sim: NDArray[np.float64],\n    x_array_max_index: int,\n    lower_bound_gsd: float,\n    upper_bound_gsd: float,\n) -&gt; NDArray[np.float64]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#save_pipeline","title":"save_pipeline","text":"<p>Show source in generate_and_train_2mode_sizer.py:335</p> <p>Save the trained pipeline to a file.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#arguments_8","title":"Arguments","text":"<ul> <li><code>pipeline</code> - The trained pipeline.</li> <li><code>filename</code> - The filename to save the pipeline to.</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#signature_9","title":"Signature","text":"<pre><code>def save_pipeline(pipeline: Pipeline, filename: str) -&gt; None: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#train_network_and_save","title":"train_network_and_save","text":"<p>Show source in generate_and_train_2mode_sizer.py:374</p> <p>Train the neural network and save the pipeline.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#signature_10","title":"Signature","text":"<pre><code>def train_network_and_save(): ...\n</code></pre>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#train_pipeline","title":"train_pipeline","text":"<p>Show source in generate_and_train_2mode_sizer.py:202</p> <p>Train the pipeline and return the trained model along with train/test data.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#arguments_9","title":"Arguments","text":"<ul> <li><code>X</code> - The feature array.</li> <li><code>y</code> - The target array.</li> <li><code>test_size</code> - The proportion of the dataset to include in the test split.</li> <li><code>random_state</code> - Random seed for reproducibility.</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#returns_8","title":"Returns","text":"<ul> <li><code>pipeline</code> - The trained pipeline. X_train, X_test, y_train, y_test: The training and testing data splits.</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#signature_11","title":"Signature","text":"<pre><code>def train_pipeline(\n    x_input: NDArray[np.float64],\n    y: NDArray[np.float64],\n    test_split_size: float = 0.3,\n    random_state: int = 42,\n) -&gt; Tuple[\n    Pipeline,\n    NDArray[np.float64],\n    NDArray[np.float64],\n    NDArray[np.float64],\n    NDArray[np.float64],\n]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#train_pipeline_with_progress","title":"train_pipeline_with_progress","text":"<p>Show source in generate_and_train_2mode_sizer.py:241</p> <p>Train the pipeline in batches with progress tracking, and return the trained model along with train/test data.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#arguments_10","title":"Arguments","text":"<ul> <li><code>X</code> - The feature array.</li> <li><code>y</code> - The target array.</li> <li><code>test_size</code> - The proportion of the dataset to include in the test split.</li> <li><code>random_state</code> - Random seed for reproducibility.</li> <li><code>n_batches</code> - Number of batches to split the training into.</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#returns_9","title":"Returns","text":"<ul> <li><code>pipeline</code> - The trained pipeline. X_train, X_test, y_train, y_test: The training and testing data splits.</li> </ul>"},{"location":"API/particula_beta/data/process/ml_analysis/generate_and_train_2mode_sizer/#signature_12","title":"Signature","text":"<pre><code>def train_pipeline_with_progress(\n    x_input: NDArray[np.float64],\n    y: NDArray[np.float64],\n    test_split_size: float = 0.3,\n    random_state: int = 42,\n    n_batches: int = 10,\n) -&gt; Tuple[\n    Pipeline,\n    NDArray[np.float64],\n    NDArray[np.float64],\n    NDArray[np.float64],\n    NDArray[np.float64],\n]: ...\n</code></pre>"},{"location":"API/particula_beta/data/process/ml_analysis/get_ml_folder/","title":"Get Ml Folder","text":"<p>Particula-beta Index / Particula Beta / Data / Process / Ml Analysis / Get Ml Folder</p> <p>Auto-generated documentation for particula_beta.data.process.ml_analysis.get_ml_folder module.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/get_ml_folder/#get_ml_analysis_folder","title":"get_ml_analysis_folder","text":"<p>Show source in get_ml_folder.py:6</p> <p>Get the location of the data folder.</p>"},{"location":"API/particula_beta/data/process/ml_analysis/get_ml_folder/#signature","title":"Signature","text":"<pre><code>def get_ml_analysis_folder(): ...\n</code></pre>"},{"location":"API/particula_beta/data/process/ml_analysis/run_ml_trainings/","title":"Run Ml Trainings","text":"<p>Particula-beta Index / Particula Beta / Data / Process / Ml Analysis / Run Ml Trainings</p> <p>Auto-generated documentation for particula_beta.data.process.ml_analysis.run_ml_trainings module.</p>"},{"location":"API/particula_beta/data/util/","title":"Util","text":"<p>Particula-beta Index / Particula Beta / Data / Util</p> <p>Auto-generated documentation for particula_beta.data.util module.</p>"},{"location":"API/particula_beta/data/util/#modules","title":"Modules","text":"<ul> <li>Convert Length</li> </ul>"},{"location":"API/particula_beta/data/util/convert_length/","title":"Convert Length","text":"<p>Particula-beta Index / Particula Beta / Data / Util / Convert Length</p> <p>Auto-generated documentation for particula_beta.data.util.convert_length module.</p>"},{"location":"API/particula_beta/data/util/convert_length/#get_length_from_volume","title":"get_length_from_volume","text":"<p>Show source in convert_length.py:9</p> <p>Calculates a length (radius or diameter) from a given volume for a sphere.</p>"},{"location":"API/particula_beta/data/util/convert_length/#arguments","title":"Arguments","text":"<ul> <li><code>volume</code> - The volume of the shape.</li> <li><code>dimension</code> - The dimension to return \u2013 \"radius\" or \"diameter\".</li> </ul>"},{"location":"API/particula_beta/data/util/convert_length/#returns","title":"Returns","text":"<p>The requested length (radius or diameter) as a float.</p>"},{"location":"API/particula_beta/data/util/convert_length/#signature","title":"Signature","text":"<pre><code>def get_length_from_volume(\n    volume: Union[float, NDArray[np.float64]], dimension: str = \"radius\"\n) -&gt; float: ...\n</code></pre>"},{"location":"API/particula_beta/data/util/convert_length/#get_volume_from_length","title":"get_volume_from_length","text":"<p>Show source in convert_length.py:27</p> <p>Calculates a volume from a given length (radius or diameter) for a sphere.</p>"},{"location":"API/particula_beta/data/util/convert_length/#arguments_1","title":"Arguments","text":"<ul> <li><code>length</code> - The length specifying the shape's size (radius or diameter).</li> <li><code>dimension</code> - Specifies whether the input is a \"radius\" or a \"diameter\".</li> </ul>"},{"location":"API/particula_beta/data/util/convert_length/#returns_1","title":"Returns","text":"<p>The corresponding volume as a float.</p>"},{"location":"API/particula_beta/data/util/convert_length/#signature_1","title":"Signature","text":"<pre><code>def get_volume_from_length(\n    length: Union[float, NDArray[np.float64]], dimension: str = \"radius\"\n) -&gt; float: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/","title":"Lagrangian","text":"<p>Particula-beta Index / Particula Beta / Lagrangian</p> <p>Auto-generated documentation for particula_beta.lagrangian module.</p>"},{"location":"API/particula_beta/lagrangian/#modules","title":"Modules","text":"<ul> <li>Boundary</li> <li>Collisions</li> <li>Integration</li> <li>Particle Pairs</li> <li>Particle Property</li> </ul>"},{"location":"API/particula_beta/lagrangian/boundary/","title":"Boundary","text":"<p>Particula-beta Index / Particula Beta / Lagrangian / Boundary</p> <p>Auto-generated documentation for particula_beta.lagrangian.boundary module.</p>"},{"location":"API/particula_beta/lagrangian/boundary/#wrapped_cube","title":"wrapped_cube","text":"<p>Show source in boundary.py:6</p> <p>Apply cubic boundary conditions with wrap-around, to the position tensor.</p> <p>This function modifies positions that exceed the cubic domain side, wrapping them around to the opposite side of the domain. It handles both positive and negative overflows. The center of the cube is assumed to be at zero. If a particle is way outside the cube, it is wrapped around to the opposite side of the cube.</p>"},{"location":"API/particula_beta/lagrangian/boundary/#arguments","title":"Arguments","text":"<ul> <li>position (torch.Tensor): A tensor representing positions that might     exceed the domain boundaries. [3, num_particles]</li> <li>cube_side (float): The cube side length of the domain.</li> </ul>"},{"location":"API/particula_beta/lagrangian/boundary/#returns","title":"Returns","text":"<ul> <li><code>-</code> torch.Tensor - The modified position tensor with boundary conditions     applied.</li> </ul>"},{"location":"API/particula_beta/lagrangian/boundary/#examples","title":"Examples","text":"<p>position = torch.tensor([...])  # Position tensor cube_side = 10.0  # Define the domain wrapped_position = boundary.wrapped_cube(position,     cube_side)</p>"},{"location":"API/particula_beta/lagrangian/boundary/#signature","title":"Signature","text":"<pre><code>def wrapped_cube(position: torch.Tensor, cube_side: float) -&gt; torch.Tensor: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/collisions/","title":"Collisions","text":"<p>Particula-beta Index / Particula Beta / Lagrangian / Collisions</p> <p>Auto-generated documentation for particula_beta.lagrangian.collisions module.</p>"},{"location":"API/particula_beta/lagrangian/collisions/#coalescence","title":"coalescence","text":"<p>Show source in collisions.py:72</p> <p>Update mass and velocity of particles based on collision pairs, conserving mass and momentum.</p> <p>This function processes collision pairs, sorts them to avoid duplicate handling, and then updates the mass and velocity of colliding particles according to the conservation of mass and momentum.</p>"},{"location":"API/particula_beta/lagrangian/collisions/#arguments","title":"Arguments","text":"<ul> <li><code>position</code> torch.Tensor - A 2D tensor of shape [n_dimensions, n_particles]     representing the positions of particles.</li> <li><code>velocity</code> torch.Tensor - A 2D tensor of shape [n_dimensions, n_particles]     representing the velocities of particles.</li> <li><code>mass</code> torch.Tensor - A 1D tensor containing the mass of each particle.</li> <li><code>radius</code> torch.Tensor - A 1D tensor containing the radius of each particle.</li> <li><code>collision_indices_pairs</code> torch.Tensor - A 2D tensor containing pairs of     indices representing colliding particles.</li> <li><code>remove_duplicates_func</code> function - A function to remove duplicate entries     from a tensor of index pairs.</li> </ul>"},{"location":"API/particula_beta/lagrangian/collisions/#returns","title":"Returns","text":"<ul> <li><code>-</code> torch.Tensor - A 2D tensor of shape [n_dimensions, n_particles]     representing the updated velocities of particles.</li> </ul>"},{"location":"API/particula_beta/lagrangian/collisions/#notes","title":"Notes","text":"<ul> <li>This function modifies the <code>velocity</code> and <code>mass</code> tensors in-place.</li> <li>It assumes that the mass and momentum are transferred from the right     particle to the left in each collision pair.</li> <li>The subtraction approach for the right-side particles ensures no mass is     lost in multi-particle collisions (e.g., A&lt;-B and B&lt;-D).</li> </ul>"},{"location":"API/particula_beta/lagrangian/collisions/#signature","title":"Signature","text":"<pre><code>def coalescence(\n    position: torch.Tensor,\n    velocity: torch.Tensor,\n    mass: torch.Tensor,\n    radius: torch.Tensor,\n    collision_indices_pairs: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/collisions/#elastic_collision","title":"elastic_collision","text":"<p>Show source in collisions.py:139</p> <p>Update velocities of particles based on elastic collision pairs using matrix operations, conserving kinetic energy and momentum.</p>"},{"location":"API/particula_beta/lagrangian/collisions/#arguments_1","title":"Arguments","text":"<ul> <li><code>velocity</code> torch.Tensor - A 2D tensor of shape [n_dimensions, n_particles]     representing the velocities of particles.</li> <li><code>mass</code> torch.Tensor - A 1D tensor containing the mass of each particle.</li> <li><code>collision_indices_pairs</code> torch.Tensor - A 2D tensor containing pairs of     indices representing colliding particles.</li> <li><code>remove_duplicates_func</code> function - A function to remove duplicate entries     from a tensor of index pairs.</li> </ul>"},{"location":"API/particula_beta/lagrangian/collisions/#returns_1","title":"Returns","text":"<ul> <li><code>torch.Tensor</code> - A 2D tensor of shape [n_dimensions, n_particles]     representing the updated velocities of particles.</li> </ul>"},{"location":"API/particula_beta/lagrangian/collisions/#notes_1","title":"Notes","text":"<ul> <li>This function modifies the <code>velocity</code> tensor in-place.</li> <li>Mass remains unchanged in elastic collisions.</li> </ul>"},{"location":"API/particula_beta/lagrangian/collisions/#examples","title":"Examples","text":"<ul> <li><code>2d</code> - https://www.wolframalpha.com/input?i=elastic+collision&amp;assumption=%7B%22F%22%2C+%22ElasticCollision%22%2C+%22m2%22%7D+-%3E%221+kg%22&amp;assumption=%7B%22F%22%2C+%22ElasticCollision%22%2C+%22m1%22%7D+-%3E%221+kg%22&amp;assumption=%22FSelect%22+-%3E+%7B%7B%22ElasticCollision2D%22%7D%7D&amp;assumption=%7B%22F%22%2C+%22ElasticCollision%22%2C+%22v1i%22%7D+-%3E%221+m%2Fs%22&amp;assumption=%7B%22F%22%2C+%22ElasticCollision%22%2C+%22v2i%22%7D+-%3E%22-0.5+m%2Fs%22</li> <li><code>3d</code> fortran - https://www.plasmaphysics.org.uk/programs/coll3d_for.htm https://www.plasmaphysics.org.uk/collision3d.htm</li> </ul> <p>I think the approach is take a pair, and rotate the coordinate system so that the collision is in the x-y plane. Then, the z component of the velocity is a 1d problem, and the x-y component is a 2d problem. Then, rotate back to the original coordinate system.</p>"},{"location":"API/particula_beta/lagrangian/collisions/#signature_1","title":"Signature","text":"<pre><code>def elastic_collision(\n    velocity: torch.Tensor, mass: torch.Tensor, collision_indices_pairs: torch.Tensor\n) -&gt; torch.Tensor: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/collisions/#find_collisions","title":"find_collisions","text":"<p>Show source in collisions.py:10</p> <p>Find the collision pairs from a distance matrix, given the mass and indices of particles.</p> <p>This function identifies pairs of particles that are within a certain distance threshold (&lt;0), indicating a collision. It filters out pairs involving particles with zero mass.</p>"},{"location":"API/particula_beta/lagrangian/collisions/#arguments_2","title":"Arguments","text":"<ul> <li><code>distance_matrix</code> torch.Tensor - A 2D tensor containing the pairwise     distances between particles.</li> <li><code>indices</code> torch.Tensor - A 1D tensor containing the indices of the     particles.</li> <li><code>mass</code> torch.Tensor - A 1D tensor containing the mass of each particle.</li> <li><code>k</code> int, optional - The number of closest neighbors to consider for each     particle. Defaults to 1.</li> </ul>"},{"location":"API/particula_beta/lagrangian/collisions/#returns_2","title":"Returns","text":"<ul> <li><code>torch.Tensor</code> - A 2D tensor of shape [n_collisions, 2] containing the indices of colliding pairs of particles.</li> </ul>"},{"location":"API/particula_beta/lagrangian/collisions/#notes_2","title":"Notes","text":"<ul> <li>The function assumes that the diagonal elements of the distance matrix (distances of particles to themselves) are less than zero.</li> <li>Particles with zero mass are excluded from the collision pairs.</li> </ul>"},{"location":"API/particula_beta/lagrangian/collisions/#signature_2","title":"Signature","text":"<pre><code>def find_collisions(\n    distance_matrix: torch.Tensor, indices: torch.Tensor, mass: torch.Tensor, k: int = 1\n) -&gt; torch.Tensor: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/integration/","title":"Integration","text":"<p>Particula-beta Index / Particula Beta / Lagrangian / Integration</p> <p>Auto-generated documentation for particula_beta.lagrangian.integration module.</p>"},{"location":"API/particula_beta/lagrangian/integration/#leapfrog","title":"leapfrog","text":"<p>Show source in integration.py:8</p> <p>Perform a single step of leapfrog integration on the position and velocity of a particle.</p> <p>Leapfrog integration is a numerical method used for solving differential equations typically found in molecular dynamics and astrophysics. It is symplectic, hence conserves energy over long simulations, and is known for its simple implementation and stability over large time steps.</p>"},{"location":"API/particula_beta/lagrangian/integration/#arguments","title":"Arguments","text":"<ul> <li>position (Tensor): The current position of the particle.</li> <li>velocity (Tensor): The current velocity of the particle.</li> <li>force (Tensor): The current force acting on the particle.</li> <li>mass (float): The mass of the particle.</li> <li>time_step (float): The time step for the integration.</li> </ul>"},{"location":"API/particula_beta/lagrangian/integration/#returns","title":"Returns","text":"<ul> <li><code>-</code> tuple - Updated position and velocity of the particle after one time step.</li> </ul> <p>Reference: - https://en.wikipedia.org/wiki/Leapfrog_integration</p>"},{"location":"API/particula_beta/lagrangian/integration/#signature","title":"Signature","text":"<pre><code>def leapfrog(\n    position: torch.Tensor,\n    velocity: torch.Tensor,\n    force: torch.Tensor,\n    mass: torch.Tensor,\n    time_step: float,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/particle_pairs/","title":"Particle Pairs","text":"<p>Particula-beta Index / Particula Beta / Lagrangian / Particle Pairs</p> <p>Auto-generated documentation for particula_beta.lagrangian.particle_pairs module.</p>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#calculate_pairwise_distance","title":"calculate_pairwise_distance","text":"<p>Show source in particle_pairs.py:48</p> <p>Need to test this:</p> <p>Calculate the pairwise Euclidean distances between points in a given position tensor.</p> <p>This function computes the pairwise distances between points represented in the input tensor. Each row of the input tensor is considered a point in n-dimensional space.</p>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#arguments","title":"Arguments","text":"<ul> <li><code>position</code> torch.Tensor - A 2D tensor of shape [n_dimensions, n_points]</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#returns","title":"Returns","text":"<ul> <li><code>torch.Tensor</code> - A 2D tensor of shape [n_points, n_points] containing the pairwise Euclidean distances between each pair of points. The element at [i, j] in the output tensor represents the distance between the i-th and j-th points in the input tensor.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#examples","title":"Examples","text":"<p>position = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])</p>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#output-will-be-a-3x3-tensor-with-the-pairwise-distances-between-these","title":"Output will be a 3x3 tensor with the pairwise distances between these","text":"<p>3 points.</p>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#signature","title":"Signature","text":"<pre><code>def calculate_pairwise_distance(position: torch.Tensor) -&gt; torch.Tensor: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#full_sweep_and_prune","title":"full_sweep_and_prune","text":"<p>Show source in particle_pairs.py:161</p> <p>Sweep and prune algorithm for collision detection along all three axes (x, y, z). This function identifies pairs of particles that are close enough to potentially collide in 3D space.</p>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#arguments_1","title":"Arguments","text":"<ul> <li><code>position</code> torch.Tensor - The 2D tensor of particle positions,     where each row represents an axis (x, y, z).</li> <li><code>radius</code> torch.Tensor - The radius of particles.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#returns_1","title":"Returns","text":"<ul> <li><code>torch.Tensor</code> - A tensor containing pairs of indices of potentially     colliding particles.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#signature_1","title":"Signature","text":"<pre><code>def full_sweep_and_prune(\n    position: torch.Tensor, radius: torch.Tensor\n) -&gt; torch.Tensor: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#full_sweep_and_prune_simplified","title":"full_sweep_and_prune_simplified","text":"<p>Show source in particle_pairs.py:245</p> <p>A simplified version of the full sweep and prune algorithm for collision written above, it is not working yet. there is an error in the update of the indices in the y and z axis.</p> <p>Sweep and prune algorithm for collision detection along all three axes (x, y, z). This function identifies pairs of particles that are close enough to potentially collide in 3D space.</p>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#arguments_2","title":"Arguments","text":"<ul> <li><code>position</code> torch.Tensor - The 2D tensor of particle positions,     where each row represents an axis (x, y, z).</li> <li><code>radius</code> torch.Tensor - The radius of particles.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#returns_2","title":"Returns","text":"<ul> <li><code>torch.Tensor</code> - A tensor containing pairs of indices of potentially     colliding particles.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#signature_2","title":"Signature","text":"<pre><code>def full_sweep_and_prune_simplified(\n    position: torch.Tensor, radius: torch.Tensor, working_yet: bool = False\n) -&gt; torch.Tensor: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#remove_duplicates","title":"remove_duplicates","text":"<p>Show source in particle_pairs.py:8</p> <p>Removes duplicate entries from a specified column in a tensor of index pairs.</p> <p>This function is designed to work with tensors where each row represents a pair of indices. It removes rows containing duplicate entries in the specified column.</p>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#arguments_3","title":"Arguments","text":"<ul> <li>index_pairs (torch.Tensor): A 2D tensor of shape [n, 2], where n is the     number of index pairs.</li> <li>index_to_remove (int): The column index (0 or 1) from which to remove     duplicate entries.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#returns_3","title":"Returns","text":"<ul> <li><code>-</code> torch.Tensor - A 2D tensor of index pairs with duplicates removed from     the specified column.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#examples_1","title":"Examples","text":"<p>index_pairs = torch.tensor([[1, 2], [3, 4], [1, 2]]) index_to_remove = 0</p>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#output-will-be-1-2-3-4-assuming-column-0-is-chosen-for-removing","title":"Output will be [[1, 2], [3, 4]] assuming column 0 is chosen for removing","text":"<pre><code>duplicates.\n</code></pre>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#signature_3","title":"Signature","text":"<pre><code>def remove_duplicates(\n    index_pairs: torch.Tensor, index_to_remove: int\n) -&gt; torch.Tensor: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#single_axis_sweep_and_prune","title":"single_axis_sweep_and_prune","text":"<p>Show source in particle_pairs.py:120</p> <p>Sweep and prune algorithm for collision detection along a single axis. This function identifies pairs of particles that are close enough to potentially collide.</p>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#arguments_4","title":"Arguments","text":"<ul> <li><code>position_axis</code> torch.Tensor - The position of particles along a single     axis.</li> <li><code>radius</code> torch.Tensor - The radius of particles.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#returns_4","title":"Returns","text":"<ul> <li><code>Tuple[torch.Tensor,</code> torch.Tensor] - Two tensors containing the indices of potentially colliding particles.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#signature_4","title":"Signature","text":"<pre><code>def single_axis_sweep_and_prune(\n    position_axis: torch.Tensor, radius: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor]: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#validate_pair_distance","title":"validate_pair_distance","text":"<p>Show source in particle_pairs.py:78</p> <p>Validates if the Euclidean distances between pairs of points are smaller than the sum of their radii.</p>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#arguments_5","title":"Arguments","text":"<ul> <li><code>collision_indices_pairs</code> torch.Tensor - A tensor containing pairs of     indices of potentially colliding particles.</li> <li><code>position</code> torch.Tensor - A 2D tensor of particle positions, where each     column represents a particle, and each row represents an axis.</li> <li><code>radius</code> torch.Tensor - A 1D tensor representing the radius of each     particle.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#returns_5","title":"Returns","text":"<ul> <li><code>torch.Tensor</code> - A tensor containing the indices of the pairs of     particles that are actually colliding.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_pairs/#signature_5","title":"Signature","text":"<pre><code>def validate_pair_distance(\n    collision_indices_pairs: torch.Tensor, position: torch.Tensor, radius: torch.Tensor\n) -&gt; torch.Tensor: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/particle_property/","title":"Particle Property","text":"<p>Particula-beta Index / Particula Beta / Lagrangian / Particle Property</p> <p>Auto-generated documentation for particula_beta.lagrangian.particle_property module.</p>"},{"location":"API/particula_beta/lagrangian/particle_property/#friction_factor_wrapper","title":"friction_factor_wrapper","text":"<p>Show source in particle_property.py:65</p> <p>Calculate the friction factor for a given radius, temperature, and pressure.</p> <p>This function wraps several underlying calculations related to dynamic viscosity, mean free path, Knudsen number, and slip correction factor to compute the particle friction factor.</p>"},{"location":"API/particula_beta/lagrangian/particle_property/#arguments","title":"Arguments","text":"<ul> <li><code>radius_meter</code> - A tensor representing the radius of the sphere(s) in meters. Can be a scalar or a vector.</li> <li><code>temperature_kelvin</code> - A tensor of the temperature in Kelvin.</li> <li><code>pressure_pascal</code> - A tensor of the pressure in Pascals.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#returns","title":"Returns","text":"<ul> <li><code>torch.Tensor</code> - A tensor of the same shape as <code>radius_meter</code>, representing the particle friction factor.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#signature","title":"Signature","text":"<pre><code>def friction_factor_wrapper(\n    radius_meter: torch.Tensor, temperature_kelvin: float, pressure_pascal: float\n) -&gt; torch.Tensor: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/particle_property/#generate_particle_masses","title":"generate_particle_masses","text":"<p>Show source in particle_property.py:116</p> <p>Generate an array of particle masses based on a log-normal distribution of particle radii and a given density.</p>"},{"location":"API/particula_beta/lagrangian/particle_property/#arguments_1","title":"Arguments","text":"<ul> <li><code>mean_radius</code> float - Mean radius of the particles. The units are     specified by <code>radius_input_units</code>.</li> <li><code>std_dev_radius</code> float - Standard deviation of the particle radii. The     units are specified by <code>radius_input_units</code>.</li> <li><code>density</code> torch.Tensor - Density of the particles in kg/m^3.</li> <li><code>num_particles</code> int - Number of particles to generate.</li> <li><code>radius_input_units</code> str, optional - Units of <code>mean_radius</code> and     <code>std_dev_radius</code>. Defaults to 'nm' (nanometers).</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#returns_1","title":"Returns","text":"<ul> <li><code>torch.Tensor</code> - A tensor of particle masses in kg, corresponding to each     particle.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#raises","title":"Raises","text":"<ul> <li><code>ValueError</code> - If <code>mean_radius</code> or <code>std_dev_radius</code> are non-positive.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#signature_1","title":"Signature","text":"<pre><code>def generate_particle_masses(\n    mean_radius: float,\n    std_dev_radius: float,\n    density: torch.Tensor,\n    num_particles: int,\n    radius_input_units: str = \"nm\",\n) -&gt; torch.Tensor: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/particle_property/#mass_calculation","title":"mass_calculation","text":"<p>Show source in particle_property.py:41</p> <p>Calculate the mass of a sphere given its radius and density using the formula for the volume of a sphere.</p> <p>This function assumes a uniform density and spherical shape to compute the mass based on the mass-density relationship: Volume = Mass / Density. The volume of a sphere is given by (4/3) * pi * radius^3.</p>"},{"location":"API/particula_beta/lagrangian/particle_property/#arguments_2","title":"Arguments","text":"<ul> <li>radius (torch.Tensor): A tensor containing the radius of the sphere(s).     Can be a scalar or a vector.</li> <li>density (torch.Tensor): A tensor containing the density of the sphere(s).     Can be a scalar or a vector.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#returns_2","title":"Returns","text":"<ul> <li><code>torch.Tensor</code> - A tensor of the same shape as <code>radius</code> and <code>density</code>     representing the mass of the sphere(s).</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#signature_2","title":"Signature","text":"<pre><code>def mass_calculation(radius: torch.Tensor, density: torch.Tensor) -&gt; torch.Tensor: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/particle_property/#nearest_match","title":"nearest_match","text":"<p>Show source in particle_property.py:247</p> <p>Perform nearest neighbor interpolation (on torch objects) to find y-values corresponding to new x-values. The function identifies the nearest x-value for each value in x_new and returns the corresponding y-value.</p>"},{"location":"API/particula_beta/lagrangian/particle_property/#arguments_3","title":"Arguments","text":"<ul> <li><code>x_values</code> torch.Tensor - The original x-values of shape (n,).</li> <li><code>y_values</code> torch.Tensor - The original y-values of shape (n,).     Each y-value corresponds to an x-value.</li> <li><code>x_new</code> torch.Tensor - The new x-values for which y-values are to be     interpolated, of shape (m,).</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#returns_3","title":"Returns","text":"<ul> <li><code>torch.Tensor</code> - The interpolated y-values of shape (m,). Each value     corresponds to the nearest match from x_values.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#signature_3","title":"Signature","text":"<pre><code>def nearest_match(\n    x_values: torch.Tensor, y_values: torch.Tensor, x_new: torch.Tensor\n) -&gt; torch.Tensor: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/particle_property/#radius_calculation","title":"radius_calculation","text":"<p>Show source in particle_property.py:11</p> <p>Calculate the radius of a sphere given its mass and density using the formula for the volume of a sphere.</p> <p>This function assumes a uniform density and spherical shape to compute the radius based on the mass-density relationship: Volume = Mass / Density. The volume of a sphere is given by (4/3) * pi * radius^3.</p>"},{"location":"API/particula_beta/lagrangian/particle_property/#arguments_4","title":"Arguments","text":"<ul> <li>mass (torch.Tensor): A tensor containing the mass of the sphere(s). Can     be a scalar or a vector.</li> <li>density (torch.Tensor): A tensor containing the density of the sphere(s).     Can be a scalar or a vector.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#returns_4","title":"Returns","text":"<ul> <li><code>torch.Tensor</code> - A tensor of the same shape as <code>mass</code> and <code>density</code>     representing the radius of the sphere(s).</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#notes","title":"Notes","text":"<ul> <li>The function supports broadcasting, so <code>mass</code> and <code>density</code> can be of     different shapes, as long as they are broadcastable to a common shape.</li> <li>Units of mass and density should be consistent to obtain a radius in     meaningful units.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#signature_4","title":"Signature","text":"<pre><code>def radius_calculation(mass: torch.Tensor, density: torch.Tensor) -&gt; torch.Tensor: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/particle_property/#random_thermal_velocity","title":"random_thermal_velocity","text":"<p>Show source in particle_property.py:215</p> <p>Generate a random thermal velocity for each particle.</p>"},{"location":"API/particula_beta/lagrangian/particle_property/#arguments_5","title":"Arguments","text":"<ul> <li><code>temperature_kelvin</code> torch.Tensor - Temperature of the fluid in Kelvin.</li> <li><code>mass_kg</code> torch.Tensor - Mass of the particle in kilograms.</li> <li><code>number_of_particles</code> int - Number of particles.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#returns_5","title":"Returns","text":"<ul> <li><code>torch.Tensor</code> - Thermal speed of the particle in meters per second.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#signature_5","title":"Signature","text":"<pre><code>def random_thermal_velocity(\n    temperature_kelvin: float,\n    mass_kg: torch.Tensor,\n    number_of_particles: int,\n    t_type=torch.float,\n    random_seed: int = 0,\n) -&gt; torch.Tensor: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/particle_property/#speed","title":"speed","text":"<p>Show source in particle_property.py:201</p> <p>Calculate the speed of a particle.</p>"},{"location":"API/particula_beta/lagrangian/particle_property/#arguments_6","title":"Arguments","text":"<ul> <li><code>velocity</code> torch.Tensor - Velocity of the particle.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#returns_6","title":"Returns","text":"<ul> <li><code>torch.Tensor</code> - Speed of the particle.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#signature_6","title":"Signature","text":"<pre><code>def speed(velocity: torch.Tensor) -&gt; torch.Tensor: ...\n</code></pre>"},{"location":"API/particula_beta/lagrangian/particle_property/#thermal_speed","title":"thermal_speed","text":"<p>Show source in particle_property.py:168</p> <p>Calculate the thermal speed of a particle based on its temperature and mass.</p> <p>The thermal speed is computed using the formula: sqrt(8 * k * T / (pi * m)) where k is the Boltzmann constant, T is the temperature in Kelvin, and m is the particle mass in kilograms.</p>"},{"location":"API/particula_beta/lagrangian/particle_property/#arguments_7","title":"Arguments","text":"<ul> <li><code>temperature_kelvin</code> float - Temperature of the environment in Kelvin.</li> <li><code>mass_kg</code> torch.Tensor - Mass of the particle(s) in kilograms.     Can be a scalar or a vector.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#returns_7","title":"Returns","text":"<ul> <li><code>torch.Tensor</code> - The thermal speed of the particle(s) in meters per second</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#raises_1","title":"Raises","text":"<ul> <li><code>ValueError</code> - If <code>temperature_kelvin</code> is less than or equal to zero or if any element of <code>mass_kg</code> is non-positive.</li> </ul>"},{"location":"API/particula_beta/lagrangian/particle_property/#signature_7","title":"Signature","text":"<pre><code>def thermal_speed(temperature_kelvin: float, mass_kg: torch.Tensor) -&gt; torch.Tensor: ...\n</code></pre>"},{"location":"Discussions/","title":"Discussions","text":""},{"location":"Discussions/#technical-topics","title":"Technical Topics","text":"<ul> <li>Condensation Equations</li> <li>Dynamic Viscosity</li> <li>Mean Free Path</li> <li>Ion-Particle Coagulation</li> </ul>"},{"location":"Discussions/#particula-concepts","title":"Particula Concepts","text":"<ul> <li>Maps</li> </ul>"},{"location":"How-To-Guides/","title":"How-To Guides","text":"<ul> <li> <p>Setup Particula</p> <p>How to setup python and install <code>Particula</code> via pip.</p> <p> Tutorial</p> </li> <li> <p>Chamber Wall Loss</p> <p>How to simulate and analyze experiments for the loss of particles to the chamber walls.</p> <p> Tutorial</p> </li> <li> <p>Equilibria</p> <p>How to simulate aerosol thermodynamic equilibria using the Binary Activity Thermodynamic <code>BAT</code> Model. Useful for water uptake and cloud droplet activation.</p> <p> Tutorial</p> </li> <li> <p>Light Scattering</p> <p>How to simulate light scattering from aerosol particles and refractive index mixing for humidified aerosol. Useful for understanding how particles scatter light.</p> <p> Tutorial</p> </li> <li> <p>Nucleation</p> <p>How to simulate aerosol nucleation using a custom process. Showing how to add a custom process for a nucleation event.</p> <p> Tutorial</p> </li> <li> <p>Data Streams and Lakes </p> <p>in BETA How to organize and analyze data from multiple instruments using <code>Streams</code> and <code>Lakes</code> organization. Useful for bulk and repeatable data analysis.</p> <p> Tutorial</p> </li> <li> <p>Lagrangian </p> <p>in BETA How to simulate aerosol coagulation dynamics using the Lagrangian method. Useful for tracking individual particles.</p> <p> Tutorial</p> </li> </ul>"},{"location":"How-To-Guides/Chamber_Wall_Loss/","title":"Index: Chamber Wall Loss BETA","text":"<p>In this example we'll go through the steps of combining the data analysis, and modeling capabilities of particula to analyze the wall loss in a chamber.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/#notebooks","title":"Notebooks","text":"<ul> <li>Wall Loss Forward Simulation</li> <li>Chamber Data Observations</li> <li>Chamber Rates Fit</li> </ul>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Forward_Simulation/","title":"Chamber Forward Simulation","text":"In\u00a0[6]: Copied! <pre>import numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom particula.dynamics import dilution, wall_loss, coagulation\nfrom particula.particles.properties import lognormal_pmf_distribution\n</pre> import numpy as np from matplotlib import pyplot as plt  from particula.dynamics import dilution, wall_loss, coagulation from particula.particles.properties import lognormal_pmf_distribution In\u00a0[7]: Copied! <pre># Define initial simulation parameters\nmode = np.array([100e-9, 500e-9])  # Median diameter of the particles in meters\ngeometric_standard_deviation = np.array(\n    [1.3, 1.5]\n)  # Geometric standard deviation of particle size distribution\nnumber_in_mode = (\n    np.array([5e4, 5e3]) * 1e6\n)  # Number of particles in each mode  1/m^3\n\n\n# define the radius bins for the simulation\nradius_bins = np.logspace(-8, -5, 250)\n\n\n# Create particle distribution using the defined parameters\n\nconcentraiton_pmf = lognormal_pmf_distribution(\n    x_values=radius_bins,\n    mode=mode,\n    geometric_standard_deviation=geometric_standard_deviation,\n    number_of_particles=number_in_mode,\n)\n\n\n# plot the initial particle distribution\nfig, ax = plt.subplots()\nax.plot(\n    radius_bins, concentraiton_pmf, label=\"Initial distribution\", marker=\".\"\n)\nax.set_xscale(\"log\")\nax.set_xlabel(\"Particle radius (m)\")\nax.set_ylabel(r\"Particle Concentration (dN), $\\dfrac{1}{m^{3}}$\")\nplt.show()\n</pre> # Define initial simulation parameters mode = np.array([100e-9, 500e-9])  # Median diameter of the particles in meters geometric_standard_deviation = np.array(     [1.3, 1.5] )  # Geometric standard deviation of particle size distribution number_in_mode = (     np.array([5e4, 5e3]) * 1e6 )  # Number of particles in each mode  1/m^3   # define the radius bins for the simulation radius_bins = np.logspace(-8, -5, 250)   # Create particle distribution using the defined parameters  concentraiton_pmf = lognormal_pmf_distribution(     x_values=radius_bins,     mode=mode,     geometric_standard_deviation=geometric_standard_deviation,     number_of_particles=number_in_mode, )   # plot the initial particle distribution fig, ax = plt.subplots() ax.plot(     radius_bins, concentraiton_pmf, label=\"Initial distribution\", marker=\".\" ) ax.set_xscale(\"log\") ax.set_xlabel(\"Particle radius (m)\") ax.set_ylabel(r\"Particle Concentration (dN), $\\dfrac{1}{m^{3}}$\") plt.show() In\u00a0[8]: Copied! <pre># coagulation rate\n\nmass_particle = (\n    4 / 3 * np.pi * radius_bins**3 * 1000\n)  # mass of the particles in kg\n\nkernel = coagulation.brownian_coagulation_kernel_via_system_state(\n    radius_particle=radius_bins,\n    mass_particle=mass_particle,\n    temperature=293.15,\n    pressure=101325,\n    alpha_collision_efficiency=1,\n)\ncoagulation_loss = coagulation.discrete_loss(\n    concentration=concentraiton_pmf,\n    kernel=kernel,\n)\ncoagulation_gain = coagulation.discrete_gain(\n    radius=radius_bins,\n    concentration=concentraiton_pmf,\n    kernel=kernel,\n)\ncoagulation_net = coagulation_gain - coagulation_loss\n\n# dilution rate\ndilution_coefficent = dilution.volume_dilution_coefficient(\n    volume=1,  # m^3\n    input_flow_rate=2 * 1e-6,  # m^3/s\n)\ndilution_loss = dilution.dilution_rate(\n    coefficient=dilution_coefficent,\n    concentration=concentraiton_pmf,\n)\n\n# wall loss rate\nchamber_wall_loss_rate = wall_loss.rectangle_wall_loss_rate(\n    wall_eddy_diffusivity=0.1,\n    particle_radius=radius_bins,\n    particle_density=1000,\n    particle_concentration=concentraiton_pmf,\n    temperature=293.15,\n    pressure=101325,\n    chamber_dimensions=(1, 1, 1),  # m\n)\n\n# plot rates\nfig, ax = plt.subplots()\nax.plot(\n    radius_bins,\n    coagulation_net,\n    label=\"Coagulation Net\",\n)\nax.plot(\n    radius_bins,\n    dilution_loss,\n    label=\"Dilution Loss\",\n)\nax.plot(\n    radius_bins,\n    chamber_wall_loss_rate,\n    label=\"Chamber Wall Loss\",\n)\nax.plot(\n    radius_bins,\n    coagulation_net + dilution_loss + chamber_wall_loss_rate,\n    label=\"Net Rate\",\n    linestyle=\"--\",\n)\nax.set_xscale(\"log\")\n# ax.set_yscale(\"log\")\nax.set_xlabel(\"Particle radius (m)\")\nax.set_ylabel(r\"Rate $\\dfrac{1}{m^{3} s^{1}}$\")\nax.grid()\nplt.legend()\nplt.show()\n</pre> # coagulation rate  mass_particle = (     4 / 3 * np.pi * radius_bins**3 * 1000 )  # mass of the particles in kg  kernel = coagulation.brownian_coagulation_kernel_via_system_state(     radius_particle=radius_bins,     mass_particle=mass_particle,     temperature=293.15,     pressure=101325,     alpha_collision_efficiency=1, ) coagulation_loss = coagulation.discrete_loss(     concentration=concentraiton_pmf,     kernel=kernel, ) coagulation_gain = coagulation.discrete_gain(     radius=radius_bins,     concentration=concentraiton_pmf,     kernel=kernel, ) coagulation_net = coagulation_gain - coagulation_loss  # dilution rate dilution_coefficent = dilution.volume_dilution_coefficient(     volume=1,  # m^3     input_flow_rate=2 * 1e-6,  # m^3/s ) dilution_loss = dilution.dilution_rate(     coefficient=dilution_coefficent,     concentration=concentraiton_pmf, )  # wall loss rate chamber_wall_loss_rate = wall_loss.rectangle_wall_loss_rate(     wall_eddy_diffusivity=0.1,     particle_radius=radius_bins,     particle_density=1000,     particle_concentration=concentraiton_pmf,     temperature=293.15,     pressure=101325,     chamber_dimensions=(1, 1, 1),  # m )  # plot rates fig, ax = plt.subplots() ax.plot(     radius_bins,     coagulation_net,     label=\"Coagulation Net\", ) ax.plot(     radius_bins,     dilution_loss,     label=\"Dilution Loss\", ) ax.plot(     radius_bins,     chamber_wall_loss_rate,     label=\"Chamber Wall Loss\", ) ax.plot(     radius_bins,     coagulation_net + dilution_loss + chamber_wall_loss_rate,     label=\"Net Rate\",     linestyle=\"--\", ) ax.set_xscale(\"log\") # ax.set_yscale(\"log\") ax.set_xlabel(\"Particle radius (m)\") ax.set_ylabel(r\"Rate $\\dfrac{1}{m^{3} s^{1}}$\") ax.grid() plt.legend() plt.show() In\u00a0[9]: Copied! <pre># time steps\ntime_array = np.linspace(start=0, stop=3600, num=1000)\ndt = time_array[1] - time_array[0]\n\n# create a matrix to store the particle distribution at each time step\nconcentration_matrix = np.zeros((len(time_array), len(radius_bins)))\ncoagulation_net_matrix = np.zeros((len(time_array), len(radius_bins)))\ndilution_loss_matrix = np.zeros((len(time_array), len(radius_bins)))\nchamber_wall_loss_rate_matrix = np.zeros((len(time_array), len(radius_bins)))\n\n# set the initial concentration\nconcentration_matrix[0, :] = concentraiton_pmf\n\nkernel = coagulation.brownian_coagulation_kernel_via_system_state(\n    radius_particle=radius_bins,\n    mass_particle=mass_particle,\n    temperature=293.15,\n    pressure=101325,\n    alpha_collision_efficiency=1,\n)\n# iterate over the time steps\nfor i, time in enumerate(time_array[1:], start=1):\n\n    # calculate the coagulation rate\n    coagulation_loss = coagulation.discrete_loss(\n        concentration=concentration_matrix[i - 1, :],\n        kernel=kernel,\n    )\n    coagulation_gain = coagulation.discrete_gain(\n        radius=radius_bins,\n        concentration=concentration_matrix[i - 1, :],\n        kernel=kernel,\n    )\n    coagulation_net = coagulation_gain - coagulation_loss\n\n    # calculate the dilution rate\n    dilution_coefficent = dilution.volume_dilution_coefficient(\n        volume=1,  # m^3\n        input_flow_rate=2 * 1e-6,  # m^3/s\n    )\n    dilution_loss = dilution.dilution_rate(\n        coefficient=dilution_coefficent,\n        concentration=concentration_matrix[i - 1, :],\n    )\n\n    # calculate the wall loss rate\n    chamber_wall_loss_rate = wall_loss.rectangle_wall_loss_rate(\n        wall_eddy_diffusivity=0.1,\n        particle_radius=radius_bins,\n        particle_density=1000,\n        particle_concentration=concentration_matrix[i - 1, :],\n        temperature=293.15,\n        pressure=101325,\n        chamber_dimensions=(1, 1, 1),  # m\n    )\n\n    # update the concentration matrix\n    concentration_matrix[i, :] = (\n        concentration_matrix[i - 1, :]\n        + (coagulation_net + dilution_loss + chamber_wall_loss_rate) * dt\n    )\n\n    # update the rate matrices\n    coagulation_net_matrix[i, :] = coagulation_net\n    dilution_loss_matrix[i, :] = dilution_loss\n    chamber_wall_loss_rate_matrix[i, :] = chamber_wall_loss_rate\n\nprint(\"Done\")\n</pre> # time steps time_array = np.linspace(start=0, stop=3600, num=1000) dt = time_array[1] - time_array[0]  # create a matrix to store the particle distribution at each time step concentration_matrix = np.zeros((len(time_array), len(radius_bins))) coagulation_net_matrix = np.zeros((len(time_array), len(radius_bins))) dilution_loss_matrix = np.zeros((len(time_array), len(radius_bins))) chamber_wall_loss_rate_matrix = np.zeros((len(time_array), len(radius_bins)))  # set the initial concentration concentration_matrix[0, :] = concentraiton_pmf  kernel = coagulation.brownian_coagulation_kernel_via_system_state(     radius_particle=radius_bins,     mass_particle=mass_particle,     temperature=293.15,     pressure=101325,     alpha_collision_efficiency=1, ) # iterate over the time steps for i, time in enumerate(time_array[1:], start=1):      # calculate the coagulation rate     coagulation_loss = coagulation.discrete_loss(         concentration=concentration_matrix[i - 1, :],         kernel=kernel,     )     coagulation_gain = coagulation.discrete_gain(         radius=radius_bins,         concentration=concentration_matrix[i - 1, :],         kernel=kernel,     )     coagulation_net = coagulation_gain - coagulation_loss      # calculate the dilution rate     dilution_coefficent = dilution.volume_dilution_coefficient(         volume=1,  # m^3         input_flow_rate=2 * 1e-6,  # m^3/s     )     dilution_loss = dilution.dilution_rate(         coefficient=dilution_coefficent,         concentration=concentration_matrix[i - 1, :],     )      # calculate the wall loss rate     chamber_wall_loss_rate = wall_loss.rectangle_wall_loss_rate(         wall_eddy_diffusivity=0.1,         particle_radius=radius_bins,         particle_density=1000,         particle_concentration=concentration_matrix[i - 1, :],         temperature=293.15,         pressure=101325,         chamber_dimensions=(1, 1, 1),  # m     )      # update the concentration matrix     concentration_matrix[i, :] = (         concentration_matrix[i - 1, :]         + (coagulation_net + dilution_loss + chamber_wall_loss_rate) * dt     )      # update the rate matrices     coagulation_net_matrix[i, :] = coagulation_net     dilution_loss_matrix[i, :] = dilution_loss     chamber_wall_loss_rate_matrix[i, :] = chamber_wall_loss_rate  print(\"Done\") <pre>Done\n</pre> In\u00a0[10]: Copied! <pre># Plotting the simulation results\n# Adjusting the figure size for better clarity\nfig, ax = plt.subplots(1, 1, figsize=[8, 6])\n\n# plot the initial particle distribution\nax.plot(\n    radius_bins,\n    concentration_matrix[0, :],\n    label=\"Initial distribution\",\n)\n# plot the final particle distribution\nax.plot(\n    radius_bins,\n    concentration_matrix[-1, :],\n    label=\"Final distribution\",\n)\nax.set_xscale(\"log\")\nax.set_xlabel(\"Particle radius (m)\")\nax.set_ylabel(r\"Particle Concentration (dN), $\\dfrac{1}{m^{3}}$\")\nplt.legend()\nplt.show()\n</pre> # Plotting the simulation results # Adjusting the figure size for better clarity fig, ax = plt.subplots(1, 1, figsize=[8, 6])  # plot the initial particle distribution ax.plot(     radius_bins,     concentration_matrix[0, :],     label=\"Initial distribution\", ) # plot the final particle distribution ax.plot(     radius_bins,     concentration_matrix[-1, :],     label=\"Final distribution\", ) ax.set_xscale(\"log\") ax.set_xlabel(\"Particle radius (m)\") ax.set_ylabel(r\"Particle Concentration (dN), $\\dfrac{1}{m^{3}}$\") plt.legend() plt.show() In\u00a0[11]: Copied! <pre># plot the Initial and Final rates\nfig, ax = plt.subplots()\nax.plot(\n    radius_bins,\n    coagulation_net_matrix[0, :],\n    label=\"Initial Coagulation Net\",\n)\nax.plot(\n    radius_bins,\n    dilution_loss_matrix[0, :],\n    label=\"Initial Dilution Loss\",\n)\nax.plot(\n    radius_bins,\n    chamber_wall_loss_rate_matrix[0, :],\n    label=\"Initial Chamber Wall Loss\",\n)\nax.plot(\n    radius_bins,\n    coagulation_net_matrix[-1, :],\n    label=\"Final Coagulation Net\",\n    linestyle=\"--\",\n)\nax.plot(\n    radius_bins,\n    dilution_loss_matrix[-1, :],\n    label=\"Final Dilution Loss\",\n    linestyle=\"--\",\n)\nax.plot(\n    radius_bins,\n    chamber_wall_loss_rate_matrix[-1, :],\n    label=\"Final Chamber Wall Loss\",\n    linestyle=\"--\",\n)\nax.set_xscale(\"log\")\nax.set_xlabel(\"Particle radius (m)\")\nax.set_ylabel(r\"Rate $\\dfrac{1}{m^{3} s^{1}}$\")\nax.grid()\nplt.legend()\nplt.show()\n</pre> # plot the Initial and Final rates fig, ax = plt.subplots() ax.plot(     radius_bins,     coagulation_net_matrix[0, :],     label=\"Initial Coagulation Net\", ) ax.plot(     radius_bins,     dilution_loss_matrix[0, :],     label=\"Initial Dilution Loss\", ) ax.plot(     radius_bins,     chamber_wall_loss_rate_matrix[0, :],     label=\"Initial Chamber Wall Loss\", ) ax.plot(     radius_bins,     coagulation_net_matrix[-1, :],     label=\"Final Coagulation Net\",     linestyle=\"--\", ) ax.plot(     radius_bins,     dilution_loss_matrix[-1, :],     label=\"Final Dilution Loss\",     linestyle=\"--\", ) ax.plot(     radius_bins,     chamber_wall_loss_rate_matrix[-1, :],     label=\"Final Chamber Wall Loss\",     linestyle=\"--\", ) ax.set_xscale(\"log\") ax.set_xlabel(\"Particle radius (m)\") ax.set_ylabel(r\"Rate $\\dfrac{1}{m^{3} s^{1}}$\") ax.grid() plt.legend() plt.show()"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Forward_Simulation/#chamber-forward-simulation","title":"Chamber Forward Simulation\u00b6","text":"<p>Comprehending particle dynamics within controlled environments is fundamental for the precise interpretation of experimental measurements. An aerosol chamber forward simulation is an approach employed to analyze and predict the behavior of particles under laboratory conditions. This method enables us to construct a virtual representation of the chamber dynamics, providing a platform to systematically examine the influence of different physical and chemical processes on aerosol populations. Specifically, we focus on three key processes: chamber aerosol dilution, particle coagulation, and wall loss (deposition). Each of these plays a pivotal role in shaping the size distribution of particles:</p> <ul> <li>Chamber Aerosol Dilution: Dilution refers to the reduction in particle concentration due to the introduction of clean air into the chamber. This process can lead to a decrease in the overall number of particles without altering the size distribution significantly. However, it can indirectly influence the dynamics of coagulation and deposition by changing the particle concentration.</li> <li>Particle Coagulation: Coagulation is the process where particles collide and stick together, forming larger particles. This leads to a shift in the size distribution towards larger sizes, reducing the number of smaller particles and increasing the average size of particles in the chamber. Coagulation is particularly significant for smaller particles due to their higher Brownian motion and likelihood of interaction.</li> <li>Wall Loss (Deposition): Wall loss occurs when particles deposit onto the walls of the chamber, removing them from the airborne population. This process preferentially affects larger particles due to their greater settling velocity and can lead to a decrease in the overall number of particles and a subtle shift in the size distribution towards smaller sizes.</li> </ul> <p>We'll be running a simulation of a chamber experiment, and turn on/off each of these processes to see how they affect the size distribution of particles. We'll also be able to see how the size distribution changes over time as the experiment progresses.</p> <p>The initial <code>particula</code> imports are next.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Forward_Simulation/#initial-distribution","title":"Initial Distribution\u00b6","text":"<p>In this section, we define the initial conditions and parameters for our chamber simulation. The <code>simple_dic_kwargs</code> dictionary contains all the necessary parameters to initialize our particle distribution within the chamber. Here's a breakdown of each parameter:</p> <ul> <li>mode: The median diameter of the particles.</li> <li>geometric_standard_deviation: The geometric standard deviation of the particle size distribution.</li> <li>number_in_mode: The number of particles in the mode.</li> </ul> <p>We define the radius bins, logarithmically, the we can get the particle concentration in a Probability Mass Function (PMF) representation. Or more commonly called <code>dN</code>.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Forward_Simulation/#rates","title":"Rates\u00b6","text":"<p>With the initial concentration setup we can now get the rates of change for the distribution of particles. These come from the <code>dynamics</code> module, which contains the functions to calculate the rates of change for each process. The <code>dynamics</code> module contains the following functions:</p> <ul> <li><code>dilution_rate</code>: Calculates the rate of change due to dilution.</li> <li><code>coagulation_rate</code>: Calculates the rate of change due to coagulation.</li> <li><code>wall_loss_rate</code>: Calculates the rate of change due to wall loss.</li> </ul>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Forward_Simulation/#for-loop-simulation","title":"For-loop Simulation\u00b6","text":"<p>With the an example of how to calculate the rates of change for each process, we can now simulate the chamber experiment. We'll iterate over a series of time steps and calculate the change in particle concentration due to each process. This is an iterative process where we update the particle distribution at each time step based on the rates of change calculated for dilution, coagulation, and wall loss. The rates are also updated at each time step to account for the changing particle concentration within the chamber.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Forward_Simulation/#visualization-of-particle-size-distribution-over-time","title":"Visualization of Particle Size Distribution Over Time\u00b6","text":"<p>In our chamber simulation, the output solution is a matrix representing the evolution of particle size distribution over time. Specifically, the solution is a 500x100 matrix where each row corresponds to a specific particle size (500 size bins in total), and each column represents the particle distribution at a given time point (100 time steps in total).</p> <p>The semi-logarithmic plot visualizes how the particle size distribution changes over the course of the simulation. We are focusing on three specific time points to illustrate these dynamics:</p> <ul> <li>Initial Distribution: This is the distribution at the beginning of the simulation (t=0). It sets the baseline for how particles are initially distributed across different sizes.</li> <li>Mid-Time Distribution: Represents the distribution at a midpoint in time (here, at the 50th time step out of 100). This snapshot provides insight into the evolution of the distribution as particles undergo processes like coagulation, dilution, and wall loss.</li> <li>Final Distribution: Shows the distribution at the end of the simulation (at the 100th time step). It indicates the final state of the particle sizes after all the simulated processes have taken place over the full time course.</li> </ul> <p>By comparing these three distributions, we can observe and analyze how the particle sizes have coalesced, dispersed, or shifted due to the underlying aerosol dynamics within the chamber.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Forward_Simulation/#takeaways","title":"Takeaways\u00b6","text":"<p>In this notebook, we conducted a series of simulations to study the behavior of aerosol particles within a controlled chamber environment. Our objective was to understand how different processes \u2014 namely coagulation, dilution, and wall loss \u2014 individually and collectively influence the size distribution of particles over time.</p> <p>Our simulations revealed several key findings:</p> <ul> <li>Coagulation Alone: When only coagulation was considered, the particle size distribution shifted towards larger particles as expected, since smaller particles tend to combine. However, this view was incomplete as it did not account for other loss mechanisms.</li> <li>Importance of Wall Loss: The inclusion of wall loss in the simulations proved to be significant. Wall loss, or deposition, especially affected the larger particles due to their higher probability of contact with the chamber walls. This process led to a noticeable reduction in the number concentration of particles, altering the peak and width of the size distribution.</li> <li>Combined Processes: By simulating a combination of processes, we observed a more complex and realistic representation of particle dynamics. The coagulation plus dilution scenario showed a lower overall concentration across all sizes, while adding wall loss further decreased the number concentration and altered the distribution shape, underscoring the importance of including wall loss in chamber simulations.</li> </ul> <p>The comparison between the different scenarios highlighted that coagulation alone could not fully explain the experimental observations. The absence of wall loss from the simulation would lead to discrepancies when comparing with empirical data, as wall loss is a critical process in chamber dynamics.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Forward_Simulation_fractal/","title":"Chamber Forward Simulation Non-Spherical Particles","text":"In\u00a0[89]: Copied! <pre>import numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom particula.dynamics import dilution, wall_loss, coagulation\nfrom particula.particles.properties import lognormal_pmf_distribution\n</pre> import numpy as np from matplotlib import pyplot as plt  from particula.dynamics import dilution, wall_loss, coagulation from particula.particles.properties import lognormal_pmf_distribution In\u00a0[90]: Copied! <pre># Define initial simulation parameters\nmode = np.array([100e-9, 500e-9])  # Median diameter of the particles in meters\ngeometric_standard_deviation = np.array(\n    [1.3, 1.5]\n)  # Geometric standard deviation of particle size distribution\nnumber_in_mode = (\n    np.array([5e4, 5e3]) * 1e6\n)  # Number of particles in each mode  1/m^3\n\n\n# define the radius bins for the simulation\nradius_bins = np.logspace(-8, -5, 250)\n\n\n# Create particle distribution using the defined parameters\n\nconcentraiton_pmf = lognormal_pmf_distribution(\n    x_values=radius_bins,\n    mode=mode,\n    geometric_standard_deviation=geometric_standard_deviation,\n    number_of_particles=number_in_mode,\n)\n\n\n# plot the initial particle distribution\nfig, ax = plt.subplots()\nax.plot(\n    radius_bins, concentraiton_pmf, label=\"Initial distribution\", marker=\".\"\n)\nax.set_xscale(\"log\")\nax.set_xlabel(\"Particle radius (m)\")\nax.set_ylabel(r\"Particle Concentration (dN), $\\dfrac{1}{m^{3}}$\")\nplt.show()\n</pre> # Define initial simulation parameters mode = np.array([100e-9, 500e-9])  # Median diameter of the particles in meters geometric_standard_deviation = np.array(     [1.3, 1.5] )  # Geometric standard deviation of particle size distribution number_in_mode = (     np.array([5e4, 5e3]) * 1e6 )  # Number of particles in each mode  1/m^3   # define the radius bins for the simulation radius_bins = np.logspace(-8, -5, 250)   # Create particle distribution using the defined parameters  concentraiton_pmf = lognormal_pmf_distribution(     x_values=radius_bins,     mode=mode,     geometric_standard_deviation=geometric_standard_deviation,     number_of_particles=number_in_mode, )   # plot the initial particle distribution fig, ax = plt.subplots() ax.plot(     radius_bins, concentraiton_pmf, label=\"Initial distribution\", marker=\".\" ) ax.set_xscale(\"log\") ax.set_xlabel(\"Particle radius (m)\") ax.set_ylabel(r\"Particle Concentration (dN), $\\dfrac{1}{m^{3}}$\") plt.show() In\u00a0[91]: Copied! <pre>from typing import Union\nfrom numpy.typing import NDArray\n</pre> from typing import Union from numpy.typing import NDArray In\u00a0[92]: Copied! <pre># radius of collision model\n\n\ndef radius_collision_3term(\n    monomer_radius: Union[float, NDArray[np.float64]],\n    radius_gyration: Union[float, NDArray[np.float64]],\n    fractal_dimension: Union[float, NDArray[np.float64]],\n    fractal_prefactor: Union[float, NDArray[np.float64]],\n) -&gt; Union[float, NDArray[np.float64]]:\n    \"\"\"Radius of collision model with 3-terms for fractal aggregates.\n\n    Args:\n        monomer_radius (float): Monomer radius in meters.\n        radius_gyration (float): Radius of gyration in meters.\n        fractal_dimension (float): Fractal dimension.\n        fractal_prefactor (float): Fractal prefactor.\n\n    Returns:\n        float: Radius of collision in meters.\n\n    References:\n        Qian, W., Kronenburg, A., Hui, X., Lin, Y., &amp; Karsch, M. (2022).\n        Effects of agglomerate characteristics on their collision kernels in\n        the free molecular regime. Journal of Aerosol Science, 159.\n        https://doi.org/10.1016/j.jaerosci.2021.105868\n    \"\"\"\n    normalized_collision_radius = (\n        0.777\n        * fractal_dimension**0.479\n        * fractal_prefactor**0.000970\n        * radius_gyration\n        / monomer_radius\n        + 0.267 * fractal_prefactor\n        - 0.0790\n    )\n    return normalized_collision_radius * monomer_radius\n\n\ndef rogak_flagan_1992(\n    radius_gyration: Union[NDArray[np.float64], float],\n    fractal_dimension: Union[NDArray[np.float64], float],\n) -&gt; Union[NDArray[np.float64], float]:\n    \"\"\"Collision radius with fractal dimension by Rogak and Flagan 1992.\n\n    Args:\n        radius_gyration: Radius of gyration of the particle [m].\n        fractal_dimension: Fractal dimension of the particle\n            [dimensionless, df].\n\n    Returns:\n        (float or NDArray[float]): Collision radius of the particle [m].\n\n    References:\n        Rogak, S. N., &amp; Flagan, R. C. (1992). Coagulation of aerosol\n        agglomerates in the transition regime. Journal of Colloid and\n        Interface Science, 151(1), 203-224.\n        https://doi.org/10.1016/0021-9797(92)90252-H\n    \"\"\"\n    return np.sqrt((fractal_dimension + 2) / 3) * radius_gyration\n\n\nradius_collision = radius_collision_3term(\n    monomer_radius=10e-9,\n    radius_gyration=radius_bins,\n    fractal_dimension=1.8,\n    fractal_prefactor=2.0,\n)\n\nradius_collision_rogak_flagan = rogak_flagan_1992(\n    radius_gyration=radius_bins, fractal_dimension=1.8\n)\n\nfig, ax = plt.subplots()\nax.plot(radius_bins, concentraiton_pmf, label=\"bins radius\")\nax.plot(\n    radius_collision, concentraiton_pmf, label=\"Collision radius\", marker=\".\"\n)\nax.plot(\n    radius_collision_rogak_flagan,\n    concentraiton_pmf,\n    label=\"Collision radius Rogak Flagan\",\n    marker=\"x\",\n)\nax.set_xscale(\"log\")\n# ax.set_yscale(\"log\")\nax.set_xlabel(\"Particle radius (m)\")\nax.set_ylabel(\"Collision radius (m)\")\nplt.show()\n</pre> # radius of collision model   def radius_collision_3term(     monomer_radius: Union[float, NDArray[np.float64]],     radius_gyration: Union[float, NDArray[np.float64]],     fractal_dimension: Union[float, NDArray[np.float64]],     fractal_prefactor: Union[float, NDArray[np.float64]], ) -&gt; Union[float, NDArray[np.float64]]:     \"\"\"Radius of collision model with 3-terms for fractal aggregates.      Args:         monomer_radius (float): Monomer radius in meters.         radius_gyration (float): Radius of gyration in meters.         fractal_dimension (float): Fractal dimension.         fractal_prefactor (float): Fractal prefactor.      Returns:         float: Radius of collision in meters.      References:         Qian, W., Kronenburg, A., Hui, X., Lin, Y., &amp; Karsch, M. (2022).         Effects of agglomerate characteristics on their collision kernels in         the free molecular regime. Journal of Aerosol Science, 159.         https://doi.org/10.1016/j.jaerosci.2021.105868     \"\"\"     normalized_collision_radius = (         0.777         * fractal_dimension**0.479         * fractal_prefactor**0.000970         * radius_gyration         / monomer_radius         + 0.267 * fractal_prefactor         - 0.0790     )     return normalized_collision_radius * monomer_radius   def rogak_flagan_1992(     radius_gyration: Union[NDArray[np.float64], float],     fractal_dimension: Union[NDArray[np.float64], float], ) -&gt; Union[NDArray[np.float64], float]:     \"\"\"Collision radius with fractal dimension by Rogak and Flagan 1992.      Args:         radius_gyration: Radius of gyration of the particle [m].         fractal_dimension: Fractal dimension of the particle             [dimensionless, df].      Returns:         (float or NDArray[float]): Collision radius of the particle [m].      References:         Rogak, S. N., &amp; Flagan, R. C. (1992). Coagulation of aerosol         agglomerates in the transition regime. Journal of Colloid and         Interface Science, 151(1), 203-224.         https://doi.org/10.1016/0021-9797(92)90252-H     \"\"\"     return np.sqrt((fractal_dimension + 2) / 3) * radius_gyration   radius_collision = radius_collision_3term(     monomer_radius=10e-9,     radius_gyration=radius_bins,     fractal_dimension=1.8,     fractal_prefactor=2.0, )  radius_collision_rogak_flagan = rogak_flagan_1992(     radius_gyration=radius_bins, fractal_dimension=1.8 )  fig, ax = plt.subplots() ax.plot(radius_bins, concentraiton_pmf, label=\"bins radius\") ax.plot(     radius_collision, concentraiton_pmf, label=\"Collision radius\", marker=\".\" ) ax.plot(     radius_collision_rogak_flagan,     concentraiton_pmf,     label=\"Collision radius Rogak Flagan\",     marker=\"x\", ) ax.set_xscale(\"log\") # ax.set_yscale(\"log\") ax.set_xlabel(\"Particle radius (m)\") ax.set_ylabel(\"Collision radius (m)\") plt.show() In\u00a0[93]: Copied! <pre>arr = np.array([0, None, np.nan])\narr = np.where(np.equal(arr, None), np.nan, arr)\nprint(arr)\n</pre> arr = np.array([0, None, np.nan]) arr = np.where(np.equal(arr, None), np.nan, arr) print(arr) <pre>[0 nan nan]\n</pre> In\u00a0[94]: Copied! <pre># coagulation rate\n\nmass_particle = (\n    4 / 3 * np.pi * radius_bins**3 * 1000\n)  # mass of the particles in kg\n\n\nkernel = coagulation.brownian_coagulation_kernel_via_system_state(\n    radius_particle=radius_collision_rogak_flagan,\n    mass_particle=mass_particle,\n    temperature=293.15,\n    pressure=101325,\n    alpha_collision_efficiency=1,\n)\ncoagulation_loss = coagulation.discrete_loss(\n    concentration=concentraiton_pmf,\n    kernel=kernel,\n)\ncoagulation_gain = coagulation.discrete_gain(\n    radius=radius_bins,\n    concentration=concentraiton_pmf,\n    kernel=kernel,\n)\ncoagulation_net = coagulation_gain - coagulation_loss\n\n# dilution rate\ndilution_coefficent = dilution.volume_dilution_coefficient(\n    volume=1,  # m^3\n    input_flow_rate=2 * 1e-6,  # m^3/s\n)\ndilution_loss = dilution.dilution_rate(\n    coefficient=dilution_coefficent,\n    concentration=concentraiton_pmf,\n)\n\n# wall loss rate\nchamber_wall_loss_rate = wall_loss.rectangle_wall_loss_rate(\n    wall_eddy_diffusivity=0.1,\n    particle_radius=radius_bins,\n    particle_density=1000,\n    particle_concentration=concentraiton_pmf,\n    temperature=293.15,\n    pressure=101325,\n    chamber_dimensions=(1, 1, 1),  # m\n)\n\n# plot rates\nfig, ax = plt.subplots()\nax.plot(\n    radius_bins,\n    coagulation_net,\n    label=\"Coagulation Net\",\n)\nax.plot(\n    radius_bins,\n    dilution_loss,\n    label=\"Dilution Loss\",\n)\nax.plot(\n    radius_bins,\n    chamber_wall_loss_rate,\n    label=\"Chamber Wall Loss\",\n)\nax.plot(\n    radius_bins,\n    coagulation_net + dilution_loss + chamber_wall_loss_rate,\n    label=\"Net Rate\",\n    linestyle=\"--\",\n)\nax.set_xscale(\"log\")\n# ax.set_yscale(\"log\")\nax.set_xlabel(\"Particle radius (m)\")\nax.set_ylabel(r\"Rate $\\dfrac{1}{m^{3} s^{1}}$\")\nax.grid()\nplt.legend()\nplt.show()\n</pre> # coagulation rate  mass_particle = (     4 / 3 * np.pi * radius_bins**3 * 1000 )  # mass of the particles in kg   kernel = coagulation.brownian_coagulation_kernel_via_system_state(     radius_particle=radius_collision_rogak_flagan,     mass_particle=mass_particle,     temperature=293.15,     pressure=101325,     alpha_collision_efficiency=1, ) coagulation_loss = coagulation.discrete_loss(     concentration=concentraiton_pmf,     kernel=kernel, ) coagulation_gain = coagulation.discrete_gain(     radius=radius_bins,     concentration=concentraiton_pmf,     kernel=kernel, ) coagulation_net = coagulation_gain - coagulation_loss  # dilution rate dilution_coefficent = dilution.volume_dilution_coefficient(     volume=1,  # m^3     input_flow_rate=2 * 1e-6,  # m^3/s ) dilution_loss = dilution.dilution_rate(     coefficient=dilution_coefficent,     concentration=concentraiton_pmf, )  # wall loss rate chamber_wall_loss_rate = wall_loss.rectangle_wall_loss_rate(     wall_eddy_diffusivity=0.1,     particle_radius=radius_bins,     particle_density=1000,     particle_concentration=concentraiton_pmf,     temperature=293.15,     pressure=101325,     chamber_dimensions=(1, 1, 1),  # m )  # plot rates fig, ax = plt.subplots() ax.plot(     radius_bins,     coagulation_net,     label=\"Coagulation Net\", ) ax.plot(     radius_bins,     dilution_loss,     label=\"Dilution Loss\", ) ax.plot(     radius_bins,     chamber_wall_loss_rate,     label=\"Chamber Wall Loss\", ) ax.plot(     radius_bins,     coagulation_net + dilution_loss + chamber_wall_loss_rate,     label=\"Net Rate\",     linestyle=\"--\", ) ax.set_xscale(\"log\") # ax.set_yscale(\"log\") ax.set_xlabel(\"Particle radius (m)\") ax.set_ylabel(r\"Rate $\\dfrac{1}{m^{3} s^{1}}$\") ax.grid() plt.legend() plt.show() In\u00a0[95]: Copied! <pre># time steps\ntime_array = np.linspace(start=0, stop=3600, num=1000)\ndt = time_array[1] - time_array[0]\n\n# create a matrix to store the particle distribution at each time step\nconcentration_matrix = np.zeros((len(time_array), len(radius_bins)))\ncoagulation_net_matrix = np.zeros((len(time_array), len(radius_bins)))\ndilution_loss_matrix = np.zeros((len(time_array), len(radius_bins)))\nchamber_wall_loss_rate_matrix = np.zeros((len(time_array), len(radius_bins)))\n\n# set the initial concentration\nconcentration_matrix[0, :] = concentraiton_pmf\n\nkernel = coagulation.brownian_coagulation_kernel_via_system_state(\n    radius_particle=radius_collision_rogak_flagan,\n    mass_particle=mass_particle,\n    temperature=293.15,\n    pressure=101325,\n    alpha_collision_efficiency=1,\n)\n# iterate over the time steps\nfor i, time in enumerate(time_array[1:], start=1):\n\n    # calculate the coagulation rate\n    coagulation_loss = coagulation.discrete_loss(\n        concentration=concentration_matrix[i - 1, :],\n        kernel=kernel,\n    )\n    coagulation_gain = coagulation.discrete_gain(\n        radius=radius_bins,\n        concentration=concentration_matrix[i - 1, :],\n        kernel=kernel,\n    )\n    coagulation_net = coagulation_gain - coagulation_loss\n\n    # calculate the dilution rate\n    dilution_coefficent = dilution.volume_dilution_coefficient(\n        volume=1,  # m^3\n        input_flow_rate=2 * 1e-6,  # m^3/s\n    )\n    dilution_loss = dilution.dilution_rate(\n        coefficient=dilution_coefficent,\n        concentration=concentration_matrix[i - 1, :],\n    )\n\n    # calculate the wall loss rate\n    chamber_wall_loss_rate = wall_loss.rectangle_wall_loss_rate(\n        wall_eddy_diffusivity=0.1,\n        particle_radius=radius_bins,\n        particle_density=1000,\n        particle_concentration=concentration_matrix[i - 1, :],\n        temperature=293.15,\n        pressure=101325,\n        chamber_dimensions=(1, 1, 1),  # m\n    )\n\n    # update the concentration matrix\n    concentration_matrix[i, :] = (\n        concentration_matrix[i - 1, :]\n        + (coagulation_net + dilution_loss + chamber_wall_loss_rate) * dt\n    )\n\n    # update the rate matrices\n    coagulation_net_matrix[i, :] = coagulation_net\n    dilution_loss_matrix[i, :] = dilution_loss\n    chamber_wall_loss_rate_matrix[i, :] = chamber_wall_loss_rate\n\nprint(\"Done\")\n</pre> # time steps time_array = np.linspace(start=0, stop=3600, num=1000) dt = time_array[1] - time_array[0]  # create a matrix to store the particle distribution at each time step concentration_matrix = np.zeros((len(time_array), len(radius_bins))) coagulation_net_matrix = np.zeros((len(time_array), len(radius_bins))) dilution_loss_matrix = np.zeros((len(time_array), len(radius_bins))) chamber_wall_loss_rate_matrix = np.zeros((len(time_array), len(radius_bins)))  # set the initial concentration concentration_matrix[0, :] = concentraiton_pmf  kernel = coagulation.brownian_coagulation_kernel_via_system_state(     radius_particle=radius_collision_rogak_flagan,     mass_particle=mass_particle,     temperature=293.15,     pressure=101325,     alpha_collision_efficiency=1, ) # iterate over the time steps for i, time in enumerate(time_array[1:], start=1):      # calculate the coagulation rate     coagulation_loss = coagulation.discrete_loss(         concentration=concentration_matrix[i - 1, :],         kernel=kernel,     )     coagulation_gain = coagulation.discrete_gain(         radius=radius_bins,         concentration=concentration_matrix[i - 1, :],         kernel=kernel,     )     coagulation_net = coagulation_gain - coagulation_loss      # calculate the dilution rate     dilution_coefficent = dilution.volume_dilution_coefficient(         volume=1,  # m^3         input_flow_rate=2 * 1e-6,  # m^3/s     )     dilution_loss = dilution.dilution_rate(         coefficient=dilution_coefficent,         concentration=concentration_matrix[i - 1, :],     )      # calculate the wall loss rate     chamber_wall_loss_rate = wall_loss.rectangle_wall_loss_rate(         wall_eddy_diffusivity=0.1,         particle_radius=radius_bins,         particle_density=1000,         particle_concentration=concentration_matrix[i - 1, :],         temperature=293.15,         pressure=101325,         chamber_dimensions=(1, 1, 1),  # m     )      # update the concentration matrix     concentration_matrix[i, :] = (         concentration_matrix[i - 1, :]         + (coagulation_net + dilution_loss + chamber_wall_loss_rate) * dt     )      # update the rate matrices     coagulation_net_matrix[i, :] = coagulation_net     dilution_loss_matrix[i, :] = dilution_loss     chamber_wall_loss_rate_matrix[i, :] = chamber_wall_loss_rate  print(\"Done\") <pre>Done\n</pre> In\u00a0[96]: Copied! <pre># Plotting the simulation results\n# Adjusting the figure size for better clarity\nfig, ax = plt.subplots(1, 1, figsize=[8, 6])\n\n# plot the initial particle distribution\nax.plot(\n    radius_bins,\n    concentration_matrix[0, :],\n    label=\"Initial distribution\",\n)\n# plot the final particle distribution\nax.plot(\n    radius_bins,\n    concentration_matrix[-1, :],\n    label=\"Final distribution\",\n)\nax.set_xscale(\"log\")\nax.set_xlabel(\"Particle radius (m)\")\nax.set_ylabel(r\"Particle Concentration (dN), $\\dfrac{1}{m^{3}}$\")\nplt.legend()\nplt.show()\n</pre> # Plotting the simulation results # Adjusting the figure size for better clarity fig, ax = plt.subplots(1, 1, figsize=[8, 6])  # plot the initial particle distribution ax.plot(     radius_bins,     concentration_matrix[0, :],     label=\"Initial distribution\", ) # plot the final particle distribution ax.plot(     radius_bins,     concentration_matrix[-1, :],     label=\"Final distribution\", ) ax.set_xscale(\"log\") ax.set_xlabel(\"Particle radius (m)\") ax.set_ylabel(r\"Particle Concentration (dN), $\\dfrac{1}{m^{3}}$\") plt.legend() plt.show() In\u00a0[97]: Copied! <pre># plot the Initial and Final rates\nfig, ax = plt.subplots()\nax.plot(\n    radius_bins,\n    coagulation_net_matrix[0, :],\n    label=\"Initial Coagulation Net\",\n)\nax.plot(\n    radius_bins,\n    dilution_loss_matrix[0, :],\n    label=\"Initial Dilution Loss\",\n)\nax.plot(\n    radius_bins,\n    chamber_wall_loss_rate_matrix[0, :],\n    label=\"Initial Chamber Wall Loss\",\n)\nax.plot(\n    radius_bins,\n    coagulation_net_matrix[-1, :],\n    label=\"Final Coagulation Net\",\n    linestyle=\"--\",\n)\nax.plot(\n    radius_bins,\n    dilution_loss_matrix[-1, :],\n    label=\"Final Dilution Loss\",\n    linestyle=\"--\",\n)\nax.plot(\n    radius_bins,\n    chamber_wall_loss_rate_matrix[-1, :],\n    label=\"Final Chamber Wall Loss\",\n    linestyle=\"--\",\n)\nax.set_xscale(\"log\")\nax.set_xlabel(\"Particle radius (m)\")\nax.set_ylabel(r\"Rate $\\dfrac{1}{m^{3} s^{1}}$\")\nax.grid()\nplt.legend()\nplt.show()\n</pre> # plot the Initial and Final rates fig, ax = plt.subplots() ax.plot(     radius_bins,     coagulation_net_matrix[0, :],     label=\"Initial Coagulation Net\", ) ax.plot(     radius_bins,     dilution_loss_matrix[0, :],     label=\"Initial Dilution Loss\", ) ax.plot(     radius_bins,     chamber_wall_loss_rate_matrix[0, :],     label=\"Initial Chamber Wall Loss\", ) ax.plot(     radius_bins,     coagulation_net_matrix[-1, :],     label=\"Final Coagulation Net\",     linestyle=\"--\", ) ax.plot(     radius_bins,     dilution_loss_matrix[-1, :],     label=\"Final Dilution Loss\",     linestyle=\"--\", ) ax.plot(     radius_bins,     chamber_wall_loss_rate_matrix[-1, :],     label=\"Final Chamber Wall Loss\",     linestyle=\"--\", ) ax.set_xscale(\"log\") ax.set_xlabel(\"Particle radius (m)\") ax.set_ylabel(r\"Rate $\\dfrac{1}{m^{3} s^{1}}$\") ax.grid() plt.legend() plt.show()"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Forward_Simulation_fractal/#chamber-forward-simulation-non-spherical-particles","title":"Chamber Forward Simulation Non-Spherical Particles\u00b6","text":"<p>Extending from the previous forward simulation example, we'll now consider the effect of non-spherical particles on the size distribution in a chamber experiment. Non-spherical particles are common in the atmosphere and can have a significant impact on the dynamics of particle coagulation and deposition. In this example, we'll use the <code>particula</code> package to simulate the evolution of the size distribution of non-spherical particles in a chamber experiment. We'll consider the following processes:</p> <ul> <li>Chamber Aerosol Dilution: Dilution refers to the reduction in particle concentration due to the introduction of clean air into the chamber. This process can lead to a decrease in the overall number of particles without altering the size distribution significantly. However, it can indirectly influence the dynamics of coagulation and deposition by changing the particle concentration.</li> <li>Particle Coagulation: Coagulation is the process where particles collide and stick together, forming larger particles. This leads to a shift in the size distribution towards larger sizes, reducing the number of smaller particles and increasing the average size of particles in the chamber. Coagulation is particularly significant for smaller particles due to their higher Brownian motion and likelihood of interaction.<ul> <li>Including fractal dimension for non-spherical particles.</li> </ul> </li> <li>Wall Loss (Deposition): Wall loss occurs when particles deposit onto the walls of the chamber, removing them from the airborne population. This process preferentially affects larger particles due to their greater settling velocity and can lead to a decrease in the overall number of particles and a subtle shift in the size distribution towards smaller sizes.</li> </ul> <p>We'll be running a simulation of a chamber experiment, and turn on/off each of these processes to see how they affect the size distribution of particles. We'll also be able to see how the size distribution changes over time as the experiment progresses.</p> <p>The initial <code>particula</code> imports are next.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Forward_Simulation_fractal/#initial-distribution","title":"Initial Distribution\u00b6","text":"<p>In this section, we define the initial conditions and parameters for our chamber simulation. The <code>simple_dic_kwargs</code> dictionary contains all the necessary parameters to initialize our particle distribution within the chamber. Here's a breakdown of each parameter:</p> <ul> <li>mode: The median diameter of the particles.</li> <li>geometric_standard_deviation: The geometric standard deviation of the particle size distribution.</li> <li>number_in_mode: The number of particles in the mode.</li> </ul> <p>We define the radius bins, logarithmically, the we can get the particle concentration in a Probability Mass Function (PMF) representation. Or more commonly called <code>dN</code>.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Forward_Simulation_fractal/#rates","title":"Rates\u00b6","text":"<p>With the initial concentration setup we can now get the rates of change for the distribution of particles. These come from the <code>dynamics</code> module, which contains the functions to calculate the rates of change for each process. The <code>dynamics</code> module contains the following functions:</p> <ul> <li><code>dilution_rate</code>: Calculates the rate of change due to dilution.</li> <li><code>coagulation_rate</code>: Calculates the rate of change due to coagulation.</li> <li><code>wall_loss_rate</code>: Calculates the rate of change due to wall loss.</li> </ul>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Forward_Simulation_fractal/#for-loop-simulation","title":"For-loop Simulation\u00b6","text":"<p>With the an example of how to calculate the rates of change for each process, we can now simulate the chamber experiment. We'll iterate over a series of time steps and calculate the change in particle concentration due to each process. This is an iterative process where we update the particle distribution at each time step based on the rates of change calculated for dilution, coagulation, and wall loss. The rates are also updated at each time step to account for the changing particle concentration within the chamber.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Forward_Simulation_fractal/#visualization-of-particle-size-distribution-over-time","title":"Visualization of Particle Size Distribution Over Time\u00b6","text":"<p>In our chamber simulation, the output solution is a matrix representing the evolution of particle size distribution over time. Specifically, the solution is a 500x100 matrix where each row corresponds to a specific particle size (500 size bins in total), and each column represents the particle distribution at a given time point (100 time steps in total).</p> <p>The semi-logarithmic plot visualizes how the particle size distribution changes over the course of the simulation. We are focusing on three specific time points to illustrate these dynamics:</p> <ul> <li>Initial Distribution: This is the distribution at the beginning of the simulation (t=0). It sets the baseline for how particles are initially distributed across different sizes.</li> <li>Mid-Time Distribution: Represents the distribution at a midpoint in time (here, at the 50th time step out of 100). This snapshot provides insight into the evolution of the distribution as particles undergo processes like coagulation, dilution, and wall loss.</li> <li>Final Distribution: Shows the distribution at the end of the simulation (at the 100th time step). It indicates the final state of the particle sizes after all the simulated processes have taken place over the full time course.</li> </ul> <p>By comparing these three distributions, we can observe and analyze how the particle sizes have coalesced, dispersed, or shifted due to the underlying aerosol dynamics within the chamber.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Forward_Simulation_fractal/#takeaways","title":"Takeaways\u00b6","text":"<p>In this notebook, we conducted a series of simulations to study the behavior of aerosol particles within a controlled chamber environment. Our objective was to understand how different processes \u2014 namely coagulation, dilution, and wall loss \u2014 individually and collectively influence the size distribution of particles over time.</p> <p>Our simulations revealed several key findings:</p> <ul> <li>Coagulation Alone: When only coagulation was considered, the particle size distribution shifted towards larger particles as expected, since smaller particles tend to combine. However, this view was incomplete as it did not account for other loss mechanisms.</li> <li>Importance of Wall Loss: The inclusion of wall loss in the simulations proved to be significant. Wall loss, or deposition, especially affected the larger particles due to their higher probability of contact with the chamber walls. This process led to a noticeable reduction in the number concentration of particles, altering the peak and width of the size distribution.</li> <li>Combined Processes: By simulating a combination of processes, we observed a more complex and realistic representation of particle dynamics. The coagulation plus dilution scenario showed a lower overall concentration across all sizes, while adding wall loss further decreased the number concentration and altered the distribution shape, underscoring the importance of including wall loss in chamber simulations.</li> </ul> <p>The comparison between the different scenarios highlighted that coagulation alone could not fully explain the experimental observations. The absence of wall loss from the simulation would lead to discrepancies when comparing with empirical data, as wall loss is a critical process in chamber dynamics.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Observations/","title":"Chamber Observations","text":"In\u00a0[2]: Copied! <pre># all the imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom particula_beta.data import loader_interface, settings_generator\nfrom particula_beta.data.tests.example_data.get_example_data import (\n    get_data_folder,\n)\n\nfrom particula_beta.data import stream_stats\nfrom particula.util import convert, time_manage\n\n# set the parent directory of the data folders\npath = get_data_folder()\nprint(\"Path to data folder:\")\nprint(path.rsplit(\"particula\")[-1])\n</pre> # all the imports import numpy as np import matplotlib.pyplot as plt from particula_beta.data import loader_interface, settings_generator from particula_beta.data.tests.example_data.get_example_data import (     get_data_folder, )  from particula_beta.data import stream_stats from particula.util import convert, time_manage  # set the parent directory of the data folders path = get_data_folder() print(\"Path to data folder:\") print(path.rsplit(\"particula\")[-1]) <pre>Path to data folder:\n_beta\\data\\tests\\example_data\n</pre> In\u00a0[3]: Copied! <pre># load the 1d data\nsmps_1d_stream_settings = settings_generator.load_settings_for_stream(\n    path=path,\n    subfolder=\"chamber_data\",\n    settings_suffix=\"_smps_1d\",\n)\nstream_smps_1d = loader_interface.load_files_interface(\n    path=path, settings=smps_1d_stream_settings\n)\n\n# load the 2d data\nsmps_2d_stream_settings = settings_generator.load_settings_for_stream(\n    path=path,\n    subfolder=\"chamber_data\",\n    settings_suffix=\"_smps_2d\",\n)\nstream_smps_2d = loader_interface.load_files_interface(\n    path=path, settings=smps_2d_stream_settings\n)\n\nprint(stream_smps_1d.header)\n</pre> # load the 1d data smps_1d_stream_settings = settings_generator.load_settings_for_stream(     path=path,     subfolder=\"chamber_data\",     settings_suffix=\"_smps_1d\", ) stream_smps_1d = loader_interface.load_files_interface(     path=path, settings=smps_1d_stream_settings )  # load the 2d data smps_2d_stream_settings = settings_generator.load_settings_for_stream(     path=path,     subfolder=\"chamber_data\",     settings_suffix=\"_smps_2d\", ) stream_smps_2d = loader_interface.load_files_interface(     path=path, settings=smps_2d_stream_settings )  print(stream_smps_1d.header) <pre>  Loading file: 2023-09-25_160155_SMPS.csv\n  Loading file: 2023-09-25_160155_SMPS.csv\n['Lower_Size_(nm)', 'Upper_Size_(nm)', 'Sample_Temp_(C)', 'Sample_Pressure_(kPa)', 'Relative_Humidity_(%)', 'Median_(nm)', 'Mean_(nm)', 'Geo_Mean_(nm)', 'Mode_(nm)', 'Geo_Std_Dev.', 'Total_Conc_(#/cc)']\n</pre> In\u00a0[4]: Copied! <pre># plot the 1d data\nfig, ax = plt.subplots()\nax.plot(\n    stream_smps_1d.datetime64,\n    stream_smps_1d[\"Total_Conc_(#/cc)\"],\n    label=\"Concentration\",\n)\nplt.xticks(rotation=45)\nax.set_xlabel(\"Time (UTC)\")\nax.set_ylabel(\"Particle concentration (#/cc)\")\nplt.show()\nfig.tight_layout()\n</pre> # plot the 1d data fig, ax = plt.subplots() ax.plot(     stream_smps_1d.datetime64,     stream_smps_1d[\"Total_Conc_(#/cc)\"],     label=\"Concentration\", ) plt.xticks(rotation=45) ax.set_xlabel(\"Time (UTC)\") ax.set_ylabel(\"Particle concentration (#/cc)\") plt.show() fig.tight_layout() In\u00a0[5]: Copied! <pre># 1 convert to dn/dDp\nstream_smps_2d.data = convert.convert_sizer_dn(\n    diameter=np.array(stream_smps_2d.header, dtype=float),\n    dn_dlogdp=stream_smps_2d.data,\n)\n\n# Dilution correction\ndilution_correction = 2\n\n# scale the concentrations\nstream_smps_2d.data *= dilution_correction\nstream_smps_1d[\"Total_Conc_(#/cc)\"] *= dilution_correction\n</pre> # 1 convert to dn/dDp stream_smps_2d.data = convert.convert_sizer_dn(     diameter=np.array(stream_smps_2d.header, dtype=float),     dn_dlogdp=stream_smps_2d.data, )  # Dilution correction dilution_correction = 2  # scale the concentrations stream_smps_2d.data *= dilution_correction stream_smps_1d[\"Total_Conc_(#/cc)\"] *= dilution_correction <p>Plot the distributions</p> In\u00a0[6]: Copied! <pre># plot the 2d data\nfig, ax = plt.subplots()\nax.plot(\n    stream_smps_2d.header_float,\n    stream_smps_2d.data[10, :],\n    label=\"Concentration earlier\",\n)\nax.plot(\n    stream_smps_2d.header_float,\n    stream_smps_2d.data[20, :],\n    label=\"Concentration later\",\n)\nax.set_xscale(\"log\")\nplt.xticks(rotation=45)\nax.set_xlabel(\"Bin diameter (nm)\")\nax.set_ylabel(\"Particle concentration (#/cc per bin)\")\nplt.legend()\nplt.show()\nfig.tight_layout()\n</pre> # plot the 2d data fig, ax = plt.subplots() ax.plot(     stream_smps_2d.header_float,     stream_smps_2d.data[10, :],     label=\"Concentration earlier\", ) ax.plot(     stream_smps_2d.header_float,     stream_smps_2d.data[20, :],     label=\"Concentration later\", ) ax.set_xscale(\"log\") plt.xticks(rotation=45) ax.set_xlabel(\"Bin diameter (nm)\") ax.set_ylabel(\"Particle concentration (#/cc per bin)\") plt.legend() plt.show() fig.tight_layout() <p>Remove the Bad Scans</p> <p>To we'll send the select start and end times to <code>particula_beta.data.stream_stats.remove_time_window</code> to remove the data in that time window.</p> In\u00a0[7]: Copied! <pre># select the time window\nbad_window_start_epoch = time_manage.time_str_to_epoch(\n    time=\"09-25-2023 19:00:00\",\n    time_format=\"%m-%d-%Y %H:%M:%S\",\n    timezone_identifier=\"UTC\",\n)\nbad_window_end_epoch = time_manage.time_str_to_epoch(\n    time=\"09-25-2023 19:45:00\",\n    time_format=\"%m-%d-%Y %H:%M:%S\",\n    timezone_identifier=\"UTC\",\n)\n\nprint(f\"Length of stream before crop: {len(stream_smps_1d)}\")\n# remove the bad data\nstream_smps_1d = stream_stats.remove_time_window(\n    stream=stream_smps_1d,\n    epoch_start=bad_window_start_epoch,\n    epoch_end=bad_window_end_epoch,\n)\nprint(f\"Length of stream after crop: {len(stream_smps_1d)}\")\n# plot the 1d data\nfig, ax = plt.subplots()\nax.plot(\n    stream_smps_1d.datetime64,\n    stream_smps_1d[\"Total_Conc_(#/cc)\"],\n    label=\"Concentration\",\n    marker=\".\",\n)\nplt.xticks(rotation=45)\nax.set_xlabel(\"Time (UTC)\")\nax.set_ylabel(\"Particle concentration (#/cc)\")\nplt.show()\n</pre> # select the time window bad_window_start_epoch = time_manage.time_str_to_epoch(     time=\"09-25-2023 19:00:00\",     time_format=\"%m-%d-%Y %H:%M:%S\",     timezone_identifier=\"UTC\", ) bad_window_end_epoch = time_manage.time_str_to_epoch(     time=\"09-25-2023 19:45:00\",     time_format=\"%m-%d-%Y %H:%M:%S\",     timezone_identifier=\"UTC\", )  print(f\"Length of stream before crop: {len(stream_smps_1d)}\") # remove the bad data stream_smps_1d = stream_stats.remove_time_window(     stream=stream_smps_1d,     epoch_start=bad_window_start_epoch,     epoch_end=bad_window_end_epoch, ) print(f\"Length of stream after crop: {len(stream_smps_1d)}\") # plot the 1d data fig, ax = plt.subplots() ax.plot(     stream_smps_1d.datetime64,     stream_smps_1d[\"Total_Conc_(#/cc)\"],     label=\"Concentration\",     marker=\".\", ) plt.xticks(rotation=45) ax.set_xlabel(\"Time (UTC)\") ax.set_ylabel(\"Particle concentration (#/cc)\") plt.show() <pre>Length of stream before crop: 336\nLength of stream after crop: 321\n</pre> In\u00a0[8]: Copied! <pre>### Repeat the removal for the 2d data\n\nstream_smps_2d = stream_stats.remove_time_window(\n    stream=stream_smps_2d,\n    epoch_start=bad_window_start_epoch,\n    epoch_end=bad_window_end_epoch,\n)\n</pre> ### Repeat the removal for the 2d data  stream_smps_2d = stream_stats.remove_time_window(     stream=stream_smps_2d,     epoch_start=bad_window_start_epoch,     epoch_end=bad_window_end_epoch, ) <p>Crop the Start and End</p> <p>To crop the start and end we can use the same functions as in the previous example. This time the start/end will just be the first and last time in the data.</p> In\u00a0[9]: Copied! <pre># crop start\nexperiment_start_epoch = time_manage.time_str_to_epoch(\n    time=\"09-25-2023 15:25:00\",\n    time_format=\"%m-%d-%Y %H:%M:%S\",\n    timezone_identifier=\"UTC\",\n)\n# crop the end\nexperiment_end_epoch = time_manage.time_str_to_epoch(\n    time=\"09-26-2023 07:00:00\",\n    time_format=\"%m-%d-%Y %H:%M:%S\",\n    timezone_identifier=\"UTC\",\n)\n\n# apply the time window\nstream_smps_1d = stream_stats.select_time_window(\n    stream=stream_smps_1d,\n    epoch_start=experiment_start_epoch,\n    epoch_end=experiment_end_epoch,\n)\nstream_smps_2d = stream_stats.select_time_window(\n    stream=stream_smps_2d,\n    epoch_start=experiment_start_epoch,\n    epoch_end=experiment_end_epoch,\n)\n</pre> # crop start experiment_start_epoch = time_manage.time_str_to_epoch(     time=\"09-25-2023 15:25:00\",     time_format=\"%m-%d-%Y %H:%M:%S\",     timezone_identifier=\"UTC\", ) # crop the end experiment_end_epoch = time_manage.time_str_to_epoch(     time=\"09-26-2023 07:00:00\",     time_format=\"%m-%d-%Y %H:%M:%S\",     timezone_identifier=\"UTC\", )  # apply the time window stream_smps_1d = stream_stats.select_time_window(     stream=stream_smps_1d,     epoch_start=experiment_start_epoch,     epoch_end=experiment_end_epoch, ) stream_smps_2d = stream_stats.select_time_window(     stream=stream_smps_2d,     epoch_start=experiment_start_epoch,     epoch_end=experiment_end_epoch, ) In\u00a0[10]: Copied! <pre># plot the 1d data\nexperiment_time = time_manage.relative_time(\n    epoch_array=stream_smps_1d.time,\n    units=\"hours\",\n)\n\nfig, ax = plt.subplots()\nax.plot(\n    experiment_time,\n    stream_smps_1d[\"Total_Conc_(#/cc)\"],\n    label=\"Concentration\",\n    marker=\".\",\n)\nplt.xticks(rotation=45)\nax.set_xlabel(\"Experiment time (hours)\")\nax.set_ylabel(\"Particle concentration (#/cc)\")\nplt.show()\n</pre> # plot the 1d data experiment_time = time_manage.relative_time(     epoch_array=stream_smps_1d.time,     units=\"hours\", )  fig, ax = plt.subplots() ax.plot(     experiment_time,     stream_smps_1d[\"Total_Conc_(#/cc)\"],     label=\"Concentration\",     marker=\".\", ) plt.xticks(rotation=45) ax.set_xlabel(\"Experiment time (hours)\") ax.set_ylabel(\"Particle concentration (#/cc)\") plt.show() In\u00a0[11]: Copied! <pre># Plot the 2d data\n\nfig, ax = plt.subplots(1, 1)\nplt.contourf(\n    experiment_time,\n    stream_smps_2d.header_float,\n    stream_smps_2d.data.T,\n    cmap=plt.cm.PuBu_r,\n    levels=50,\n)\nplt.yscale(\"log\")\nax.set_xlabel(\"Experiment time (hours)\")\nax.set_ylabel(\"Diameter (nm)\")\nplt.colorbar(label=\"Concentration dN [#/cm3]\", ax=ax)\nplt.show()\n</pre> # Plot the 2d data  fig, ax = plt.subplots(1, 1) plt.contourf(     experiment_time,     stream_smps_2d.header_float,     stream_smps_2d.data.T,     cmap=plt.cm.PuBu_r,     levels=50, ) plt.yscale(\"log\") ax.set_xlabel(\"Experiment time (hours)\") ax.set_ylabel(\"Diameter (nm)\") plt.colorbar(label=\"Concentration dN [#/cm3]\", ax=ax) plt.show()"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Observations/#chamber-observations","title":"Chamber Observations\u00b6","text":"<p>In this example we'll start to look at the aerosols in a smog chamber with a focus on how the size distributions evolves with time. The example data is from a NaCl aerosol experiment in a smog chamber (900 L).</p> <p>The Experiment:</p> <p>After injection of NaCl particles, the chamber was only slowly purged with clean air, so the aerosols would be pushed out to the instruments. The clean air flow was 1.25 LPM. No chemistry was happening in the chamber, so the aerosol size distribution should be stable from a condensation point of view. The aerosol distribution should change due to coagulation, wall losses and dilution.</p> <p>Let's start by loading the data and plotting the size distribution.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Observations/#load-the-data","title":"Load the data\u00b6","text":"<p>We'll uses the settings from the Data loading examples, to load the Scanning Mobility Particle Sizer (SMPS) data.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Observations/#visualize","title":"Visualize\u00b6","text":"<p>Let's pause to plot the data. We'll plot the size distribution at the start and end of the experiment.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Observations/#clean-up-the-data","title":"Clean up the data\u00b6","text":"<p>Now their are a couple things we need to account for from the raw data.</p> <ol> <li>The SMPS data is in dN/dlogDp, but we want dN/dDp. So we need to convert the data.</li> <li>The SMPS sample line was diluted 1:2 with clean air, so we need to account for that.</li> <li>Then we need to remove those couple scans where data jumps crazy high (due to a cpc malfunction), around 1900.</li> <li>We need to select from after the injection starts and before the chamber is purged (that small bump at the end of the data).</li> </ol>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Observations/#plot-the-cleaned-data","title":"Plot the Cleaned Data\u00b6","text":"<p>Here, we'll also conver to relative time, so hours since the start of the experiment.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Observations/#summary","title":"Summary\u00b6","text":"<p>We've now loaded the data, cleaned it up, and plotted the size distribution. We can see that the size distribution is changing with time. We'll look at accounting for chamber processes, like coagulation, dilution, and wall losses, in the next part.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Rates_Fitting/","title":"Chamber Rates Fitting","text":"In\u00a0[8]: Copied! <pre>import numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom particula_beta.data.process import chamber_rate_fitting, lognormal_2mode\nfrom particula_beta.data import (\n    stream_stats,\n    loader,\n    loader_interface,\n    settings_generator,\n)\n\n# all the imports\nfrom particula_beta.data.tests.example_data.get_example_data import (\n    get_data_folder,\n)\nfrom particula.util.convert import distribution_convert_pdf_pms\nfrom particula.util import convert, time_manage\nfrom particula.util.input_handling import convert_units\n\n# set the parent directory of the data folders\npath = get_data_folder()\nprint(\"Path to data folder:\")\nprint(path.rsplit(\"particula\")[-1])\n</pre> import numpy as np  import matplotlib.pyplot as plt  from particula_beta.data.process import chamber_rate_fitting, lognormal_2mode from particula_beta.data import (     stream_stats,     loader,     loader_interface,     settings_generator, )  # all the imports from particula_beta.data.tests.example_data.get_example_data import (     get_data_folder, ) from particula.util.convert import distribution_convert_pdf_pms from particula.util import convert, time_manage from particula.util.input_handling import convert_units  # set the parent directory of the data folders path = get_data_folder() print(\"Path to data folder:\") print(path.rsplit(\"particula\")[-1]) <pre>Path to data folder:\n_beta\\data\\tests\\example_data\n</pre> In\u00a0[9]: Copied! <pre># load the 2d data\nsmps_2d_stream_settings = settings_generator.load_settings_for_stream(\n    path=path,\n    subfolder=\"chamber_data\",\n    settings_suffix=\"_smps_2d\",\n)\nstream_sizer_2d = loader_interface.load_files_interface(\n    path=path, settings=smps_2d_stream_settings\n)\n# 1 convert to dn/dDp\nstream_sizer_2d.data = convert.convert_sizer_dn(\n    diameter=np.array(stream_sizer_2d.header, dtype=float),\n    dn_dlogdp=stream_sizer_2d.data,\n)\n# Dilution correction\ndilution_correction = 2\n# scale the concentrations\nstream_sizer_2d.data *= dilution_correction\n\n# select the time window\nbad_window_start_epoch = time_manage.time_str_to_epoch(\n    time=\"09-25-2023 19:00:00\",\n    time_format=\"%m-%d-%Y %H:%M:%S\",\n    timezone_identifier=\"UTC\",\n)\nbad_window_end_epoch = time_manage.time_str_to_epoch(\n    time=\"09-25-2023 19:45:00\",\n    time_format=\"%m-%d-%Y %H:%M:%S\",\n    timezone_identifier=\"UTC\",\n)\n# remove the bad data\nstream_sizer_2d = stream_stats.remove_time_window(\n    stream=stream_sizer_2d,\n    epoch_start=bad_window_start_epoch,\n    epoch_end=bad_window_end_epoch,\n)\n# remove the first few bins\nstream_sizer_2d.data = stream_sizer_2d.data[:, 20:]\nstream_sizer_2d.header = stream_sizer_2d.header[20:]\n\n# crop start\nexperiment_start_epoch = time_manage.time_str_to_epoch(\n    time=\"09-25-2023 15:25:00\",\n    time_format=\"%m-%d-%Y %H:%M:%S\",\n    timezone_identifier=\"UTC\",\n)\n# crop the end\nexperiment_end_epoch = time_manage.time_str_to_epoch(\n    time=\"09-25-2023 17:00:00\",\n    time_format=\"%m-%d-%Y %H:%M:%S\",\n    timezone_identifier=\"UTC\",\n)  # time=\"09-26-2023 07:00:00\",\n# select the time window\nstream_sizer_2d = stream_stats.select_time_window(\n    stream=stream_sizer_2d,\n    epoch_start=experiment_start_epoch,\n    epoch_end=experiment_end_epoch,\n)\n\n# # save the cleaned data\n# loader.save_stream(\n#     stream=stream_sizer_2d,\n#     path=path,\n#     suffix_name=\"_sizer_2d_cleaned\",\n#     folder=\"chamber_analysis\",\n# )\n\nexperiment_time = time_manage.relative_time(\n    epoch_array=stream_sizer_2d.time,\n    units=\"hours\",\n)\n# Plot the 2d data\nfig, ax = plt.subplots(1, 1)\nplt.contourf(\n    experiment_time,\n    stream_sizer_2d.header_float,\n    stream_sizer_2d.data.T,\n    cmap=plt.cm.PuBu_r,\n    levels=50,\n)\nplt.yscale(\"log\")\nax.set_xlabel(\"Experiment time (hours)\")\nax.set_ylabel(\"Diameter (nm)\")\nax.set_title(\"Concentration vs time\")\nplt.colorbar(label=r\"Concentration $\\dfrac{1}{cm^3}$\", ax=ax)\nplt.show()\n</pre> # load the 2d data smps_2d_stream_settings = settings_generator.load_settings_for_stream(     path=path,     subfolder=\"chamber_data\",     settings_suffix=\"_smps_2d\", ) stream_sizer_2d = loader_interface.load_files_interface(     path=path, settings=smps_2d_stream_settings ) # 1 convert to dn/dDp stream_sizer_2d.data = convert.convert_sizer_dn(     diameter=np.array(stream_sizer_2d.header, dtype=float),     dn_dlogdp=stream_sizer_2d.data, ) # Dilution correction dilution_correction = 2 # scale the concentrations stream_sizer_2d.data *= dilution_correction  # select the time window bad_window_start_epoch = time_manage.time_str_to_epoch(     time=\"09-25-2023 19:00:00\",     time_format=\"%m-%d-%Y %H:%M:%S\",     timezone_identifier=\"UTC\", ) bad_window_end_epoch = time_manage.time_str_to_epoch(     time=\"09-25-2023 19:45:00\",     time_format=\"%m-%d-%Y %H:%M:%S\",     timezone_identifier=\"UTC\", ) # remove the bad data stream_sizer_2d = stream_stats.remove_time_window(     stream=stream_sizer_2d,     epoch_start=bad_window_start_epoch,     epoch_end=bad_window_end_epoch, ) # remove the first few bins stream_sizer_2d.data = stream_sizer_2d.data[:, 20:] stream_sizer_2d.header = stream_sizer_2d.header[20:]  # crop start experiment_start_epoch = time_manage.time_str_to_epoch(     time=\"09-25-2023 15:25:00\",     time_format=\"%m-%d-%Y %H:%M:%S\",     timezone_identifier=\"UTC\", ) # crop the end experiment_end_epoch = time_manage.time_str_to_epoch(     time=\"09-25-2023 17:00:00\",     time_format=\"%m-%d-%Y %H:%M:%S\",     timezone_identifier=\"UTC\", )  # time=\"09-26-2023 07:00:00\", # select the time window stream_sizer_2d = stream_stats.select_time_window(     stream=stream_sizer_2d,     epoch_start=experiment_start_epoch,     epoch_end=experiment_end_epoch, )  # # save the cleaned data # loader.save_stream( #     stream=stream_sizer_2d, #     path=path, #     suffix_name=\"_sizer_2d_cleaned\", #     folder=\"chamber_analysis\", # )  experiment_time = time_manage.relative_time(     epoch_array=stream_sizer_2d.time,     units=\"hours\", ) # Plot the 2d data fig, ax = plt.subplots(1, 1) plt.contourf(     experiment_time,     stream_sizer_2d.header_float,     stream_sizer_2d.data.T,     cmap=plt.cm.PuBu_r,     levels=50, ) plt.yscale(\"log\") ax.set_xlabel(\"Experiment time (hours)\") ax.set_ylabel(\"Diameter (nm)\") ax.set_title(\"Concentration vs time\") plt.colorbar(label=r\"Concentration $\\dfrac{1}{cm^3}$\", ax=ax) plt.show() <pre>  Loading file: 2023-09-25_160155_SMPS.csv\n</pre> In\u00a0[10]: Copied! <pre>radius_m = stream_sizer_2d.header_float / 2 * convert_units(old=\"nm\", new=\"m\")\nconcentration_m3_pmf = stream_sizer_2d.data * convert_units(\n    old=\"1/cm^3\", new=\"1/m^3\"\n)\n\nconcentration_m3_pdf = distribution_convert_pdf_pms(\n    x_array=radius_m,\n    distribution=concentration_m3_pmf,\n    to_pdf=True,\n)\n\n# fit the lognormal pdf distribution with 2 modes\n\nstream_lognormal_prameters = lognormal_2mode.guess_and_optimize_looped(\n    experiment_time=stream_sizer_2d.time,\n    radius_m=radius_m,\n    concentration_m3_pdf=concentration_m3_pdf,\n)\n\n\n# plot the 2 mode lognormal fit\nfig, ax = plt.subplots(1, 1)\nplt.plot(experiment_time, stream_lognormal_prameters[\"R2\"])\nax.set_ylim(0, 1)\nax.set_xlabel(\"Time (hours)\")\nax.set_ylabel(\"R squared\")\nax.set_title(\"R squared of Lognormal 2 mode fit\")\nplt.show()\n</pre> radius_m = stream_sizer_2d.header_float / 2 * convert_units(old=\"nm\", new=\"m\") concentration_m3_pmf = stream_sizer_2d.data * convert_units(     old=\"1/cm^3\", new=\"1/m^3\" )  concentration_m3_pdf = distribution_convert_pdf_pms(     x_array=radius_m,     distribution=concentration_m3_pmf,     to_pdf=True, )  # fit the lognormal pdf distribution with 2 modes  stream_lognormal_prameters = lognormal_2mode.guess_and_optimize_looped(     experiment_time=stream_sizer_2d.time,     radius_m=radius_m,     concentration_m3_pdf=concentration_m3_pdf, )   # plot the 2 mode lognormal fit fig, ax = plt.subplots(1, 1) plt.plot(experiment_time, stream_lognormal_prameters[\"R2\"]) ax.set_ylim(0, 1) ax.set_xlabel(\"Time (hours)\") ax.set_ylabel(\"R squared\") ax.set_title(\"R squared of Lognormal 2 mode fit\") plt.show() <pre>Lognormal 2-mode:   0%|          | 0/32 [00:00&lt;?, ?it/s]</pre> <pre>Lognormal 2-mode: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 32/32 [04:16&lt;00:00,  8.02s/it]\n</pre> <p>Get the new distribution</p> <p>With the new parameters we can now calculate the new distribution. This is also where we can resample. We can change:</p> <ul> <li><code>radius_min</code>: Minimum radius of the distribution</li> <li><code>radius_max</code>: Maximum radius of the distribution</li> <li><code>num_radius_bins</code>: Number of bins in the radius</li> </ul> In\u00a0[11]: Copied! <pre># Create the fitted pmf\n(fitted_pmf_stream, fitted_pmf_concentration) = (\n    lognormal_2mode.create_lognormal_2mode_from_fit(\n        parameters_stream=stream_lognormal_prameters,\n        radius_min=1e-9,\n        radius_max=1e-6,\n        num_radius_bins=250,\n    )\n)\n\n# % Plot the fitted pmf\nfig, ax = plt.subplots(1, 1)\nplt.contourf(\n    experiment_time,\n    fitted_pmf_stream.header_float,\n    fitted_pmf_stream.data.T,\n    cmap=plt.cm.PuBu_r,\n    levels=20,\n)\nplt.yscale(\"log\")\nax.set_xlabel(\"Experiment time (hours)\")\nax.set_ylabel(\"Radius (m)\")\nax.set_title(\"Fitted Concentration PMF\")\nplt.colorbar(label=r\"Concentration $\\dfrac{1}{m^3}$\", ax=ax)\nplt.show()\n</pre> # Create the fitted pmf (fitted_pmf_stream, fitted_pmf_concentration) = (     lognormal_2mode.create_lognormal_2mode_from_fit(         parameters_stream=stream_lognormal_prameters,         radius_min=1e-9,         radius_max=1e-6,         num_radius_bins=250,     ) )  # % Plot the fitted pmf fig, ax = plt.subplots(1, 1) plt.contourf(     experiment_time,     fitted_pmf_stream.header_float,     fitted_pmf_stream.data.T,     cmap=plt.cm.PuBu_r,     levels=20, ) plt.yscale(\"log\") ax.set_xlabel(\"Experiment time (hours)\") ax.set_ylabel(\"Radius (m)\") ax.set_title(\"Fitted Concentration PMF\") plt.colorbar(label=r\"Concentration $\\dfrac{1}{m^3}$\", ax=ax) plt.show() In\u00a0[12]: Copied! <pre>pmf_derivative_stream = stream_stats.time_derivative_of_stream(\n    stream=fitted_pmf_stream,\n    liner_slope_window_size=10,\n)\n\n# Plot the 2d data\nfig, ax = plt.subplots(1, 1)\nplt.contourf(\n    experiment_time,\n    pmf_derivative_stream.header_float,\n    pmf_derivative_stream.data.T,\n    cmap=plt.cm.PuBu_r,\n    levels=20,\n    vmin=-3e5,\n)\nplt.yscale(\"log\")\nax.set_xlabel(\"Experiment time (hours)\")\nax.set_ylabel(\"Radius (m)\")\nax.set_title(\"dN/dt\")\nplt.colorbar(label=r\"Measured Rate $\\dfrac{1}{m^3 s}$\", ax=ax)\nplt.show()\n</pre> pmf_derivative_stream = stream_stats.time_derivative_of_stream(     stream=fitted_pmf_stream,     liner_slope_window_size=10, )  # Plot the 2d data fig, ax = plt.subplots(1, 1) plt.contourf(     experiment_time,     pmf_derivative_stream.header_float,     pmf_derivative_stream.data.T,     cmap=plt.cm.PuBu_r,     levels=20,     vmin=-3e5, ) plt.yscale(\"log\") ax.set_xlabel(\"Experiment time (hours)\") ax.set_ylabel(\"Radius (m)\") ax.set_title(\"dN/dt\") plt.colorbar(label=r\"Measured Rate $\\dfrac{1}{m^3 s}$\", ax=ax) plt.show() In\u00a0[13]: Copied! <pre># %% optimize eddy diffusivity and alpha collision efficiency\n\n# set the chamber properties\nchamber_parameters = chamber_rate_fitting.ChamberParameters()\nchamber_parameters.temperature = 293.15\nchamber_parameters.pressure = 78000\nchamber_parameters.particle_density = 1600\nchamber_parameters.volume = 0.9\nchamber_parameters.input_flow_rate_m3_sec = 1.2 * convert_units(\n    \"L/min\", \"m^3/s\"\n)\nchamber_parameters.chamber_dimensions = (0.739, 0.739, 1.663)\n\nfit_guess, fit_bounds = chamber_rate_fitting.create_guess_and_bounds(\n    guess_eddy_diffusivity=0.1,\n    guess_alpha_collision_efficiency=0.5,\n    bounds_eddy_diffusivity=(1e-2, 50),\n    bounds_alpha_collision_efficiency=(0.1, 1),\n)\n\n# % fit chamber eddy diffusivity and alpha collision efficiency\n(\n    result_stream,\n    coagulation_loss_stream,\n    coagulation_gain_stream,\n    coagulation_net_stream,\n    dilution_loss_stream,\n    wall_loss_rate_stream,\n    total_rate_stream,\n) = chamber_rate_fitting.optimize_and_calculate_rates_looped(\n    pmf_stream=fitted_pmf_stream,\n    pmf_derivative_stream=pmf_derivative_stream,\n    chamber_parameters=chamber_parameters,\n    fit_guess=fit_guess,\n    fit_bounds=fit_bounds,\n)\n</pre> # %% optimize eddy diffusivity and alpha collision efficiency  # set the chamber properties chamber_parameters = chamber_rate_fitting.ChamberParameters() chamber_parameters.temperature = 293.15 chamber_parameters.pressure = 78000 chamber_parameters.particle_density = 1600 chamber_parameters.volume = 0.9 chamber_parameters.input_flow_rate_m3_sec = 1.2 * convert_units(     \"L/min\", \"m^3/s\" ) chamber_parameters.chamber_dimensions = (0.739, 0.739, 1.663)  fit_guess, fit_bounds = chamber_rate_fitting.create_guess_and_bounds(     guess_eddy_diffusivity=0.1,     guess_alpha_collision_efficiency=0.5,     bounds_eddy_diffusivity=(1e-2, 50),     bounds_alpha_collision_efficiency=(0.1, 1), )  # % fit chamber eddy diffusivity and alpha collision efficiency (     result_stream,     coagulation_loss_stream,     coagulation_gain_stream,     coagulation_net_stream,     dilution_loss_stream,     wall_loss_rate_stream,     total_rate_stream, ) = chamber_rate_fitting.optimize_and_calculate_rates_looped(     pmf_stream=fitted_pmf_stream,     pmf_derivative_stream=pmf_derivative_stream,     chamber_parameters=chamber_parameters,     fit_guess=fit_guess,     fit_bounds=fit_bounds, ) <pre>Chamber rates: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 32/32 [02:50&lt;00:00,  5.33s/it]\n</pre> <p>Visualize the results</p> In\u00a0[14]: Copied! <pre># %% plot the optimized rates\n\nfig, ax = plt.subplots(1, 1)\nax.plot(experiment_time, result_stream[\"r2_value\"])\nax.set_ylim(0, 1)\nax.set_xlabel(\"Time (hours)\")\nax.set_ylabel(\"R squared\")\nax.set_title(\"R squared from Rates fit\")\nplt.show()\n\n# plot the optimized values\nfig, ax = plt.subplots(1, 1)\nax.plot(\n    experiment_time,\n    result_stream[\"wall_eddy_diffusivity_[1/s]\"],\n    label=\"Wall eddy diffusivity\",\n)\nax.set_yscale(\"log\")\nax.set_xlabel(\"Time (hours)\")\nax.set_ylabel(\"Wall eddy diffusivity (1/s)\")\nax.set_title(\"Wall eddy diffusivity\")\nplt.legend()\nplt.show()\n\nfig, ax = plt.subplots(1, 1)\nax.plot(\n    experiment_time,\n    result_stream[\"alpha_collision_efficiency_[-]\"],\n    label=\"Alpha collision efficiency\",\n)\nax.set_xlabel(\"Time (hours)\")\nax.set_ylabel(\"Alpha collision efficiency (-)\")\nax.set_title(\"Alpha collision efficiency\")\nplt.legend()\nplt.show()\n\n# plot the rate fractions\nfig, ax = plt.subplots(1, 1)\nax.plot(\n    experiment_time,\n    result_stream[\"coagulation_net_fraction\"],\n    label=\"Coagulation net\",\n)\nax.plot(\n    experiment_time,\n    result_stream[\"dilution_loss_fraction\"],\n    label=\"Dilution loss\",\n)\nax.plot(\n    experiment_time,\n    result_stream[\"wall_loss_rate_fraction\"],\n    label=\"Wall loss\",\n)\nax.set_ylim(0, 1)\nax.set_xlabel(\"Time (hours)\")\nax.set_ylabel(\"Rate fraction\")\nax.set_title(\"Rate fractions\")\nplt.legend()\nplt.show()\n\n# plot the optimized rates\ntime_index = 20\nradius_m_values = fitted_pmf_stream.header_float\nfig, ax = plt.subplots(1, 1)\nplt.plot(\n    radius_m_values,\n    coagulation_net_stream.data[time_index, :],\n    label=\"Coagulation net\",\n)\nplt.plot(\n    radius_m_values, dilution_loss_stream.data[time_index, :], label=\"Dilution\"\n)\nplt.plot(\n    radius_m_values,\n    wall_loss_rate_stream.data[time_index, :],\n    label=\"Wall loss\",\n)\nplt.plot(\n    radius_m_values, total_rate_stream.data[time_index, :], label=\"Total rate\"\n)\nplt.plot(\n    radius_m_values,\n    pmf_derivative_stream.data[time_index, :],\n    label=\"Measured rate\",\n    linestyle=\"--\",\n)\nplt.fill_between(\n    radius_m_values,\n    pmf_derivative_stream.data[time_index, :] * 0.75,\n    pmf_derivative_stream.data[time_index, :] * 1.25,\n    alpha=0.5,\n)\nplt.xscale(\"log\")\nplt.xlabel(\"Diameter (m)\")\nplt.ylabel(r\"Rate $\\dfrac{1}{m^3 s}$\")\nplt.title(\"Rate comparison\")\nplt.legend()\nplt.show()\n</pre> # %% plot the optimized rates  fig, ax = plt.subplots(1, 1) ax.plot(experiment_time, result_stream[\"r2_value\"]) ax.set_ylim(0, 1) ax.set_xlabel(\"Time (hours)\") ax.set_ylabel(\"R squared\") ax.set_title(\"R squared from Rates fit\") plt.show()  # plot the optimized values fig, ax = plt.subplots(1, 1) ax.plot(     experiment_time,     result_stream[\"wall_eddy_diffusivity_[1/s]\"],     label=\"Wall eddy diffusivity\", ) ax.set_yscale(\"log\") ax.set_xlabel(\"Time (hours)\") ax.set_ylabel(\"Wall eddy diffusivity (1/s)\") ax.set_title(\"Wall eddy diffusivity\") plt.legend() plt.show()  fig, ax = plt.subplots(1, 1) ax.plot(     experiment_time,     result_stream[\"alpha_collision_efficiency_[-]\"],     label=\"Alpha collision efficiency\", ) ax.set_xlabel(\"Time (hours)\") ax.set_ylabel(\"Alpha collision efficiency (-)\") ax.set_title(\"Alpha collision efficiency\") plt.legend() plt.show()  # plot the rate fractions fig, ax = plt.subplots(1, 1) ax.plot(     experiment_time,     result_stream[\"coagulation_net_fraction\"],     label=\"Coagulation net\", ) ax.plot(     experiment_time,     result_stream[\"dilution_loss_fraction\"],     label=\"Dilution loss\", ) ax.plot(     experiment_time,     result_stream[\"wall_loss_rate_fraction\"],     label=\"Wall loss\", ) ax.set_ylim(0, 1) ax.set_xlabel(\"Time (hours)\") ax.set_ylabel(\"Rate fraction\") ax.set_title(\"Rate fractions\") plt.legend() plt.show()  # plot the optimized rates time_index = 20 radius_m_values = fitted_pmf_stream.header_float fig, ax = plt.subplots(1, 1) plt.plot(     radius_m_values,     coagulation_net_stream.data[time_index, :],     label=\"Coagulation net\", ) plt.plot(     radius_m_values, dilution_loss_stream.data[time_index, :], label=\"Dilution\" ) plt.plot(     radius_m_values,     wall_loss_rate_stream.data[time_index, :],     label=\"Wall loss\", ) plt.plot(     radius_m_values, total_rate_stream.data[time_index, :], label=\"Total rate\" ) plt.plot(     radius_m_values,     pmf_derivative_stream.data[time_index, :],     label=\"Measured rate\",     linestyle=\"--\", ) plt.fill_between(     radius_m_values,     pmf_derivative_stream.data[time_index, :] * 0.75,     pmf_derivative_stream.data[time_index, :] * 1.25,     alpha=0.5, ) plt.xscale(\"log\") plt.xlabel(\"Diameter (m)\") plt.ylabel(r\"Rate $\\dfrac{1}{m^3 s}$\") plt.title(\"Rate comparison\") plt.legend() plt.show()"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Rates_Fitting/#chamber-rates-fitting","title":"Chamber Rates Fitting\u00b6","text":"<p>In this notebook we now will use the chamber observations and process simulations to fit the chamber rates. We will use the same approach as in the previous notebook, but now we will fit the rates for each chamber. We will be fitting the free parameters of <code>wall_eddy_diffusivity</code> and <code>alpha_collision_efficiency</code>.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Rates_Fitting/#load-and-prepare-data","title":"Load and Prepare Data\u00b6","text":"<p>First we will load the data, clean and prepare it for the fitting of the size distributions.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Rates_Fitting/#lognormal-distribution-fitting","title":"Lognormal Distribution Fitting\u00b6","text":"<p>We use a 2-mode lognormal distribution to fit the size distributions. This means we first convert the measured observations to a pdf and then fit the lognormal distribution to it. We will use a machine learning model to guess the initial parameters of the lognormal distribution. Then a scipy optimization routine will be used to optimize the lognormal parameters.</p> <p>The fit parameters are the following:</p> <ul> <li><code>Mode_1</code>: Mode 1 of the lognormal distribution</li> <li><code>Mode_2</code>: Mode 2 of the lognormal distribution</li> <li><code>GSD_1</code>: Geometric standard deviation of mode 1</li> <li><code>GSD_2</code>: Geometric standard deviation of mode 2</li> <li><code>N_1</code>: Number concentration of mode 1</li> <li><code>N_2</code>: Number concentration of mode 2</li> </ul> <p>This is likely the computational part of the notebook, so save the results in a file to avoid running it again. This can be done by uncommenting the <code>save_stream</code> call.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Rates_Fitting/#time-derivative","title":"Time Derivative\u00b6","text":"<p>To get the measured rates we need to calculate the time derivative of the size distribution. We will use the linear slope of a moving window to calculate the time derivative. The window size is defined by <code>linear_slope_window_size</code>.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Rates_Fitting/#fitting-the-chamber-rates","title":"Fitting the Chamber Rates\u00b6","text":"<p>Now we will fit the chamber rates. We will be fitting the free parameters of <code>wall_eddy_diffusivity</code> and <code>alpha_collision_efficiency</code>. First we define the chamber and its environment. Then we call the optimization routine to fit the missing rates.</p>"},{"location":"How-To-Guides/Chamber_Wall_Loss/Notebooks/Chamber_Rates_Fitting/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook we have fitted the chamber rates using the observations and process simulations. We have used a 2-mode lognormal distribution to fit the size distributions and calculated the time derivative of the size distribution to get the rates. We have then fitted the chamber rates using the optimization routine.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/","title":"Index: Data Streams and Lakes BETA","text":"<p>Within Particula, instrument data loading and processing is handled by the <code>particula_beta.data</code> module. This module contains a number of classes and functions that can be used to load and process data from a variety of sources. The <code>particula_beta.data</code> module is designed to be used in conjunction with the <code>particula.model</code> module, which can improve the interpretation of the data.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/#notebooks","title":"Notebooks","text":"<ul> <li>Data Loading Part 1</li> <li>Data Loading Part 2</li> <li>Data Loading Part 3</li> <li>Data Loading Part 4</li> <li>Stream Statistics</li> <li>Stream Statistics Sizer Data</li> </ul>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/","title":"Loading Part 1: Into &amp; 1D Data","text":"In\u00a0[1]: Copied! <pre>import os  # For handling file paths and directories\nimport matplotlib.pyplot as plt  # For plotting data\n\n# Importing specific functions from the particula package for data loading\n# and handling\nfrom particula_beta.data import loader, loader_interface\nfrom particula_beta.data.tests.example_data.get_example_data import (\n    get_data_folder,\n)\nfrom particula_beta.data.loader_setting_builders import (\n    DataChecksBuilder,\n    Loader1DSettingsBuilder,\n)\n</pre> import os  # For handling file paths and directories import matplotlib.pyplot as plt  # For plotting data  # Importing specific functions from the particula package for data loading # and handling from particula_beta.data import loader, loader_interface from particula_beta.data.tests.example_data.get_example_data import (     get_data_folder, ) from particula_beta.data.loader_setting_builders import (     DataChecksBuilder,     Loader1DSettingsBuilder, ) In\u00a0[2]: Copied! <pre># Setting up the path for data files\nimport os  # Re-importing os for clarity\nfrom particula_beta.data.tests.example_data.get_example_data import (\n    get_data_folder,\n)\n\ncurrent_path = os.getcwd()  # Getting the current working directory\nprint(\"Current path for this script:\")\nprint(current_path.rsplit(\"particula\")[-1])  # Displaying the current path\n\npath = get_data_folder()  # Getting the example data folder path\nprint(\"Path to data folder:\")\nprint(path.rsplit(\"particula\")[-1])  # Displaying the data folder path\n</pre> # Setting up the path for data files import os  # Re-importing os for clarity from particula_beta.data.tests.example_data.get_example_data import (     get_data_folder, )  current_path = os.getcwd()  # Getting the current working directory print(\"Current path for this script:\") print(current_path.rsplit(\"particula\")[-1])  # Displaying the current path  path = get_data_folder()  # Getting the example data folder path print(\"Path to data folder:\") print(path.rsplit(\"particula\")[-1])  # Displaying the data folder path <pre>Current path for this script:\n-beta\\docs\\How-To-Guides\\Data_Streams_and_Lakes\\notebooks\nPath to data folder:\n_beta\\data\\tests\\example_data\n</pre> In\u00a0[3]: Copied! <pre>data_file = os.path.join(\n    path, \"CPC_3010_data\", \"CPC_3010_data_20220709_Jul.csv\"\n)\n\n# load the data\nraw_data = loader.data_raw_loader(data_file)\n\n# print the first 2 rows\nprint(raw_data[:2])\n</pre> data_file = os.path.join(     path, \"CPC_3010_data\", \"CPC_3010_data_20220709_Jul.csv\" )  # load the data raw_data = loader.data_raw_loader(data_file)  # print the first 2 rows print(raw_data[:2]) <pre>['1657342801,32660,17.1,', '1,0.017']\n</pre> In\u00a0[4]: Copied! <pre># Printing the length of raw_data before cleaning\nprint(f\"raw_data length: {len(raw_data)}\")\n\n# This function needs Data checks dict for:\n# - characters: Ensures each line has between 10 and 100 characters\n# - char_counts: Checks that each line contains 4 commas (this is customizable)\n# - skip_rows: Number of rows to skip at the start (none in this case)\n# - skip_end: Number of rows to skip at the end (none in this case)\n# This can be built using the DataChecksBuilder class\ndata_checks = (\n    DataChecksBuilder()\n    .set_characters([10, 100])\n    .set_char_counts({\",\": 4})\n    .set_skip_rows(0)\n    .set_skip_end(0)\n    .build()\n)\nprint(f\"data checks: {data_checks}\")\n\n# Cleaning the data using loader.data_format_checks\ndata = loader.data_format_checks(data=raw_data, data_checks=data_checks)\n\n# Printing the length of data after cleaning\nprint(f\"data length: {len(data)}\")\n\n# Calculating and printing the number of lines removed during the cleaning\n# process\nprint(f\"There was {len(raw_data) - len(data)} lines removed from the data\")\n\n# Note: The data cleaning is performed by the general_data_formatter function for timeseries data.\n# If dealing with 2D data, a separate function is used for cleaning.\n</pre> # Printing the length of raw_data before cleaning print(f\"raw_data length: {len(raw_data)}\")  # This function needs Data checks dict for: # - characters: Ensures each line has between 10 and 100 characters # - char_counts: Checks that each line contains 4 commas (this is customizable) # - skip_rows: Number of rows to skip at the start (none in this case) # - skip_end: Number of rows to skip at the end (none in this case) # This can be built using the DataChecksBuilder class data_checks = (     DataChecksBuilder()     .set_characters([10, 100])     .set_char_counts({\",\": 4})     .set_skip_rows(0)     .set_skip_end(0)     .build() ) print(f\"data checks: {data_checks}\")  # Cleaning the data using loader.data_format_checks data = loader.data_format_checks(data=raw_data, data_checks=data_checks)  # Printing the length of data after cleaning print(f\"data length: {len(data)}\")  # Calculating and printing the number of lines removed during the cleaning # process print(f\"There was {len(raw_data) - len(data)} lines removed from the data\")  # Note: The data cleaning is performed by the general_data_formatter function for timeseries data. # If dealing with 2D data, a separate function is used for cleaning. <pre>raw_data length: 33280\ndata checks: {'characters': [10, 100], 'char_counts': {',': 4}, 'replace_chars': {}, 'skip_rows': 0, 'skip_end': 0}\ndata length: 33254\nThere was 26 lines removed from the data\n</pre> In\u00a0[5]: Copied! <pre># Sample the data to get the epoch times and the data\nepoch_time, data_array = loader.sample_data(\n    data=data,\n    time_column=[0],  # Indicate the column(s) that contain time data.\n    # For instance, if time data spans multiple columns,\n    # list them here, like [0, 2] for columns 0 and 2.\n    time_format=\"epoch\",  # Define the format of the time data.\n    # Use \"epoch\" for epoch time, or specify another format\n    # compatible with datetime.strptime(), such as\n    # \"%m/%d/%Y %I:%M:%S %p\".\n    # Specify columns that contain the actual data for analysis.\n    data_columns=[1, 2],\n    # Indicate the delimiter used in the data (e.g., comma for CSV files).\n    delimiter=\",\",\n)\n\n# Printing the shape and first few entries of the epoch time array\nprint(f\"epoch_time shape: {epoch_time.shape}\")\nprint(\"First 5 epoch times:\", epoch_time[:5])\n\n# Printing the shape and first few entries of the data array\nprint(f\"data_array shape: {data_array.shape}\")\nprint(\"First 5 data entries:\", data_array[:5])\n</pre> # Sample the data to get the epoch times and the data epoch_time, data_array = loader.sample_data(     data=data,     time_column=[0],  # Indicate the column(s) that contain time data.     # For instance, if time data spans multiple columns,     # list them here, like [0, 2] for columns 0 and 2.     time_format=\"epoch\",  # Define the format of the time data.     # Use \"epoch\" for epoch time, or specify another format     # compatible with datetime.strptime(), such as     # \"%m/%d/%Y %I:%M:%S %p\".     # Specify columns that contain the actual data for analysis.     data_columns=[1, 2],     # Indicate the delimiter used in the data (e.g., comma for CSV files).     delimiter=\",\", )  # Printing the shape and first few entries of the epoch time array print(f\"epoch_time shape: {epoch_time.shape}\") print(\"First 5 epoch times:\", epoch_time[:5])  # Printing the shape and first few entries of the data array print(f\"data_array shape: {data_array.shape}\") print(\"First 5 data entries:\", data_array[:5]) <pre>epoch_time shape: (33254,)\nFirst 5 epoch times: [1.65734280e+09 1.65734281e+09 1.65734281e+09 1.65734281e+09\n 1.65734282e+09]\ndata_array shape: (33254, 2)\nFirst 5 data entries: [[3.3510e+04 1.7000e+01]\n [3.3465e+04 1.7100e+01]\n [3.2171e+04 1.7000e+01]\n [3.2889e+04 1.6800e+01]\n [3.2706e+04 1.7000e+01]]\n</pre> In\u00a0[6]: Copied! <pre># Creating a figure and axis for the plot\nfig, ax = plt.subplots()\n\n# Plotting the data\n# `epoch_time` on the x-axis and the first column of `data_array` on the y-axis\nax.plot(\n    epoch_time,\n    data_array[:, 0],  # Selecting the first column of data_array\n    label=\"data column 1\",  # Label for this data series\n    linestyle=\"none\",  # No line connecting the data points\n    marker=\".\",\n)  # Style of the data points; here, it's a dot\n\n# Setting the x-axis label to \"Time (epoch)\"\nax.set_xlabel(\"Time (epoch)\")\n\n# Setting the y-axis label to \"Data\"\nax.set_ylabel(\"Data\")\n\n# Adding a legend to the plot, which helps identify the data series\nax.legend()\n\n# Displaying the plot\nplt.show()\n\n# Adjusting the layout to make sure everything fits well within the figure\n# This is particularly useful for ensuring labels and titles are not cut off\nfig.tight_layout()\n</pre> # Creating a figure and axis for the plot fig, ax = plt.subplots()  # Plotting the data # `epoch_time` on the x-axis and the first column of `data_array` on the y-axis ax.plot(     epoch_time,     data_array[:, 0],  # Selecting the first column of data_array     label=\"data column 1\",  # Label for this data series     linestyle=\"none\",  # No line connecting the data points     marker=\".\", )  # Style of the data points; here, it's a dot  # Setting the x-axis label to \"Time (epoch)\" ax.set_xlabel(\"Time (epoch)\")  # Setting the y-axis label to \"Data\" ax.set_ylabel(\"Data\")  # Adding a legend to the plot, which helps identify the data series ax.legend()  # Displaying the plot plt.show()  # Adjusting the layout to make sure everything fits well within the figure # This is particularly useful for ensuring labels and titles are not cut off fig.tight_layout() In\u00a0[7]: Copied! <pre># Using the LoaderSetting1DBuilder class to generate the settings dictionary\n\nsettings = (\n    Loader1DSettingsBuilder()\n    .set_relative_data_folder(\"CPC_3010_data\")\n    .set_filename_regex(\"*.csv\")\n    .set_header_row(0)\n    .set_data_checks(data_checks)  # from above\n    .set_data_column([1, 2])\n    .set_data_header([\"data 1\", \"data 2\"])\n    .set_time_column([0])\n    .set_time_format(\"epoch\")\n    .set_delimiter(\",\")\n    .set_timezone_identifier(\"UTC\")\n    .build()\n)\n\n# Printing the generated settings dictionary to verify its contents\nprint(\"Settings dictionary:\")\nfor key, value in settings.items():\n    print(f\"{key}: {value}\")\n</pre> # Using the LoaderSetting1DBuilder class to generate the settings dictionary  settings = (     Loader1DSettingsBuilder()     .set_relative_data_folder(\"CPC_3010_data\")     .set_filename_regex(\"*.csv\")     .set_header_row(0)     .set_data_checks(data_checks)  # from above     .set_data_column([1, 2])     .set_data_header([\"data 1\", \"data 2\"])     .set_time_column([0])     .set_time_format(\"epoch\")     .set_delimiter(\",\")     .set_timezone_identifier(\"UTC\")     .build() )  # Printing the generated settings dictionary to verify its contents print(\"Settings dictionary:\") for key, value in settings.items():     print(f\"{key}: {value}\") <pre>Settings dictionary:\nrelative_data_folder: CPC_3010_data\nfilename_regex: *.csv\nMIN_SIZE_BYTES: 10000\ndata_loading_function: general_1d_load\nheader_row: 0\ndata_checks: {'characters': [10, 100], 'char_counts': {',': 4}, 'replace_chars': {}, 'skip_rows': 0, 'skip_end': 0}\ndata_column: [1, 2]\ndata_header: ['data 1', 'data 2']\ntime_column: [0]\ntime_format: epoch\ndelimiter: ,\ntime_shift_seconds: 0\ntimezone_identifier: UTC\n</pre> In\u00a0[8]: Copied! <pre># Importing the loader interface from the particula package\nfrom particula_beta.data import loader_interface\nfrom particula_beta.data.tests.example_data.get_example_data import (\n    get_data_folder,\n)\n\n# Getting the working path where the data files are located\nworking_path = get_data_folder()\n\n# Defining the settings for loading CPC 3010 data\n# These settings were previously generated or can be created manually\ncpc_settings = {\n    # Folder name containing the data files\n    \"relative_data_folder\": \"CPC_3010_data\",\n    # Pattern to match filenames (e.g., all CSV files)\n    \"filename_regex\": \"*.csv\",\n    \"header_row\": 0,  # Row number of the header in the data file\n    \"MIN_SIZE_BYTES\": 10,  # Minimum file size in bytes for a file to be considered\n    # Function to be used for loading the data\n    \"data_loading_function\": \"general_1d_load\",\n    \"header_row\": 0,  # Row number of the header in the data file\n    \"data_checks\": {\n        # Range of character count per line (min, max)\n        \"characters\": [10, 100],\n        # Number of times a character (comma) should appear in each line\n        \"char_counts\": {\",\": 4},\n        \"skip_rows\": 0,  # Number of rows to skip at the beginning of the file\n        \"skip_end\": 0,  # Number of rows to skip at the end of the file\n    },\n    \"data_column\": [1, 2],  # Columns in the file that contain the data\n    \"data_header\": [\"data 1\", \"data 2\"],  # Headers for the data columns\n    \"time_column\": [0],  # Column in the file that contains the time data\n    \"time_format\": \"epoch\",  # Format of the time data (epoch, ISO 8601, etc.)\n    \"delimiter\": \",\",  # Delimiter used in the data file (e.g., comma for CSV)\n    \"time_shift_seconds\": 0,  # Shift in time data if needed, in seconds\n    \"timezone_identifier\": \"UTC\",  # Timezone identifier for the time data\n}\n\n# Now call the loader interface to load the data using the specified settings\ndata_stream = loader_interface.load_files_interface(\n    path=working_path,  # Path to the data folder\n    settings=cpc_settings,  # Settings defined above\n)\n\n# The data_stream object now contains the loaded and formatted data ready\n# for analysis\n</pre> # Importing the loader interface from the particula package from particula_beta.data import loader_interface from particula_beta.data.tests.example_data.get_example_data import (     get_data_folder, )  # Getting the working path where the data files are located working_path = get_data_folder()  # Defining the settings for loading CPC 3010 data # These settings were previously generated or can be created manually cpc_settings = {     # Folder name containing the data files     \"relative_data_folder\": \"CPC_3010_data\",     # Pattern to match filenames (e.g., all CSV files)     \"filename_regex\": \"*.csv\",     \"header_row\": 0,  # Row number of the header in the data file     \"MIN_SIZE_BYTES\": 10,  # Minimum file size in bytes for a file to be considered     # Function to be used for loading the data     \"data_loading_function\": \"general_1d_load\",     \"header_row\": 0,  # Row number of the header in the data file     \"data_checks\": {         # Range of character count per line (min, max)         \"characters\": [10, 100],         # Number of times a character (comma) should appear in each line         \"char_counts\": {\",\": 4},         \"skip_rows\": 0,  # Number of rows to skip at the beginning of the file         \"skip_end\": 0,  # Number of rows to skip at the end of the file     },     \"data_column\": [1, 2],  # Columns in the file that contain the data     \"data_header\": [\"data 1\", \"data 2\"],  # Headers for the data columns     \"time_column\": [0],  # Column in the file that contains the time data     \"time_format\": \"epoch\",  # Format of the time data (epoch, ISO 8601, etc.)     \"delimiter\": \",\",  # Delimiter used in the data file (e.g., comma for CSV)     \"time_shift_seconds\": 0,  # Shift in time data if needed, in seconds     \"timezone_identifier\": \"UTC\",  # Timezone identifier for the time data }  # Now call the loader interface to load the data using the specified settings data_stream = loader_interface.load_files_interface(     path=working_path,  # Path to the data folder     settings=cpc_settings,  # Settings defined above )  # The data_stream object now contains the loaded and formatted data ready # for analysis <pre>  Loading file: CPC_3010_data_20220709_Jul.csv\n  Loading file: CPC_3010_data_20220710_Jul.csv\n</pre> In\u00a0[9]: Copied! <pre># Example 1: Print the Headers of the Data Stream\nprint(\"Headers of the Data Stream:\")\nprint(data_stream.header)\n\n# Example 2: Display the First Few Rows of Data\nprint(\"\\nFirst 5 Rows of Data:\")\nprint(data_stream.data[:5, :])\n\n# Example 3: Print the Time Stamps Associated with the First Few Data Entries\nprint(\"\\nTime Stamps for the First 5 Data Entries:\")\nprint(data_stream.time[:5])\n\n# Example 4: Retrieve and Print Data from a Specific Column using Header Name\ncolumn_name = \"data 1\"  # Replace with an actual column name from your header\nprint(f\"\\n5 Entries from data Column '{column_name}':\")\nprint(data_stream[column_name][:5])\n\n# Example 5: Print the Length of the Time Stream (Number of Data Entries)\nprint(\"\\nNumber of Data Entries in the Time Stream:\")\nprint(len(data_stream))\n\n# Example 6: Convert Time to datetime64 and Print the First Few Entries\nprint(\"\\nFirst 5 Time Entries in datetime64 Format:\")\nprint(data_stream.datetime64[:5])\n\n# Example 7: Print the Names of the Source Files\nprint(\"\\nNames of the Source Files for the Data Stream:\")\nprint(data_stream.files)\n</pre> # Example 1: Print the Headers of the Data Stream print(\"Headers of the Data Stream:\") print(data_stream.header)  # Example 2: Display the First Few Rows of Data print(\"\\nFirst 5 Rows of Data:\") print(data_stream.data[:5, :])  # Example 3: Print the Time Stamps Associated with the First Few Data Entries print(\"\\nTime Stamps for the First 5 Data Entries:\") print(data_stream.time[:5])  # Example 4: Retrieve and Print Data from a Specific Column using Header Name column_name = \"data 1\"  # Replace with an actual column name from your header print(f\"\\n5 Entries from data Column '{column_name}':\") print(data_stream[column_name][:5])  # Example 5: Print the Length of the Time Stream (Number of Data Entries) print(\"\\nNumber of Data Entries in the Time Stream:\") print(len(data_stream))  # Example 6: Convert Time to datetime64 and Print the First Few Entries print(\"\\nFirst 5 Time Entries in datetime64 Format:\") print(data_stream.datetime64[:5])  # Example 7: Print the Names of the Source Files print(\"\\nNames of the Source Files for the Data Stream:\") print(data_stream.files) <pre>Headers of the Data Stream:\n['data 1', 'data 2']\n\nFirst 5 Rows of Data:\n[[3.3510e+04 1.7000e+01]\n [3.3465e+04 1.7100e+01]\n [3.2171e+04 1.7000e+01]\n [3.2889e+04 1.6800e+01]\n [3.2706e+04 1.7000e+01]]\n\nTime Stamps for the First 5 Data Entries:\n[1.65734280e+09 1.65734281e+09 1.65734281e+09 1.65734281e+09\n 1.65734282e+09]\n\n5 Entries from data Column 'data 1':\n[33510. 33465. 32171. 32889. 32706.]\n\nNumber of Data Entries in the Time Stream:\n68551\n\nFirst 5 Time Entries in datetime64 Format:\n['2022-07-09T05:00:04' '2022-07-09T05:00:07' '2022-07-09T05:00:10'\n '2022-07-09T05:00:13' '2022-07-09T05:00:16']\n\nNames of the Source Files for the Data Stream:\n[['CPC_3010_data_20220709_Jul.csv', 1044534], ['CPC_3010_data_20220710_Jul.csv', 1113488]]\n</pre> In\u00a0[10]: Copied! <pre># Importing the matplotlib library for plotting\nimport matplotlib.pyplot as plt\n\n# Creating a new figure and axis for the plot\nfig, ax = plt.subplots()\n\n# Plotting data from the data_stream object\n# data_stream.datetime64 is used for the x-axis (time data)\n# data_stream.data[:, 0] selects the first column of data for the y-axis\nax.plot(\n    data_stream.datetime64,\n    data_stream.data[:, 0],  # Selecting the first column of data\n    label=\"data column 1\",  # Label for the plotted data series\n    linestyle=\"none\",  # No line connecting the data points\n    marker=\".\",\n)  # Style of the data points; here, it's a dot\n\n# Adjusting the x-axis labels for better readability\n# Rotating the labels by -35 degrees\nplt.tick_params(rotation=-35, axis=\"x\")\n\n# Setting the labels for the x-axis and y-axis\nax.set_xlabel(\"Time (UTC)\")  # Label for the x-axis\nax.set_ylabel(\"Data\")  # Label for the y-axis\n\n# Adding a legend to the plot\n# This helps in identifying the data series\nax.legend()\n\n# Displaying the plot\nplt.show()\n\n# Adjusting the layout of the plot\n# Ensures that all elements of the plot are nicely fitted within the figure\nfig.tight_layout()\n</pre> # Importing the matplotlib library for plotting import matplotlib.pyplot as plt  # Creating a new figure and axis for the plot fig, ax = plt.subplots()  # Plotting data from the data_stream object # data_stream.datetime64 is used for the x-axis (time data) # data_stream.data[:, 0] selects the first column of data for the y-axis ax.plot(     data_stream.datetime64,     data_stream.data[:, 0],  # Selecting the first column of data     label=\"data column 1\",  # Label for the plotted data series     linestyle=\"none\",  # No line connecting the data points     marker=\".\", )  # Style of the data points; here, it's a dot  # Adjusting the x-axis labels for better readability # Rotating the labels by -35 degrees plt.tick_params(rotation=-35, axis=\"x\")  # Setting the labels for the x-axis and y-axis ax.set_xlabel(\"Time (UTC)\")  # Label for the x-axis ax.set_ylabel(\"Data\")  # Label for the y-axis  # Adding a legend to the plot # This helps in identifying the data series ax.legend()  # Displaying the plot plt.show()  # Adjusting the layout of the plot # Ensures that all elements of the plot are nicely fitted within the figure fig.tight_layout() In\u00a0[11]: Copied! <pre># call help on data_stream to see the available methods\nhelp(data_stream)\n</pre> # call help on data_stream to see the available methods help(data_stream) <pre>Help on Stream in module particula_beta.data.stream object:\n\nclass Stream(builtins.object)\n |  Stream(header: List[str] = &lt;factory&gt;, data: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]] = &lt;factory&gt;, time: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]] = &lt;factory&gt;, files: List[str] = &lt;factory&gt;) -&gt; None\n |  \n |  Consistent format for storing data.\n |  \n |  Represents a consistent format for storing and managing data streams\n |  within a list. Similar to pandas but with tighter control over the\n |  data allowed and expected format.\n |  \n |  Attributes:\n |      header: Headers of the data stream, each a string.\n |      data: 2D numpy array where rows are timepoints and columns\n |          correspond to headers.\n |      time: 1D numpy array representing the time points of the data stream.\n |      files: List of filenames that contain the data stream.\n |  \n |  Methods:\n |      validate_inputs: Validates the types of class inputs.\n |      __getitem__(index): Returns the data at the specified index.\n |      __setitem__(index, value): Sets or updates data at the specified index.\n |      __len__(): Returns the length of the time stream.\n |      datetime64: Converts time stream to numpy datetime64 array for plots.\n |      header_dict: Provides a dictionary mapping of header indices to names.\n |      header_float: Converts header names to a numpy array of floats.\n |  \n |  Methods defined here:\n |  \n |  __eq__(self, other)\n |      Return self==value.\n |  \n |  __getitem__(self, index: Union[int, str]) -&gt; numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]]\n |      Gets data at a specified index or header name.\n |      \n |      Allows indexing of the data stream using an integer index or a string\n |      corresponding to the header. If a string is used, the header index is\n |      retrieved and used to return the data array. Only one str\n |      argument is allowed. A list of int is allowed.\n |      \n |      Args:\n |          index: The index or name of the data column to\n |              retrieve.\n |      \n |      Returns:\n |          np.ndarray: The data array at the specified index.\n |  \n |  __init__(self, header: List[str] = &lt;factory&gt;, data: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]] = &lt;factory&gt;, time: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]] = &lt;factory&gt;, files: List[str] = &lt;factory&gt;) -&gt; None\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __len__(self) -&gt; int\n |      Returns the number of time points in the data stream.\n |      \n |      Returns:\n |          int: Length of the time stream.\n |  \n |  __pop__(self, index: Union[int, str]) -&gt; None\n |      Removes data at a specified index or header name.\n |      \n |      Allows indexing of the data stream using an integer index or a string\n |      corresponding to the header. If a string is used, the header index is\n |      retrieved and used to return the data array. Only one str\n |      argument is allowed. A list of int is allowed.\n |      \n |      Args:\n |          index: The index or name of the data column to\n |              retrieve.\n |  \n |  __post_init__(self)\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  __setitem__(self, index: Union[int, str], value: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]])\n |      Sets or adds data at a specified index.\n |      \n |      If index is a string and not in headers, it is added. This is used\n |      to add new data columns to the stream.\n |      \n |      Args:\n |          index: The index or name of the data column to set.\n |          value: The data to set at the specified index.\n |      \n |      Notes:\n |          Support setting multiple rows by accepting a list of values.\n |  \n |  validate_inputs(self)\n |      Validates that header is a list.\n |      \n |      Raises:\n |          TypeError: If `header` is not a list.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  datetime64\n |      Converts the epoch time array to a datetime64 for plotting.\n |      \n |      This method converts the time array to a datetime64 array, which\n |      can be used for plotting time series data. This generally assumes\n |      that the time array is in seconds since the epoch.\n |      \n |      Returns:\n |          np.ndarray: Datetime64 array representing the time stream.\n |  \n |  header_dict\n |      Provides a dictionary mapping from index to header names.\n |      \n |      Returns:\n |          dict: Dictionary with indices as keys and header names as values.\n |  \n |  header_float\n |      Attempts to convert header names to a float array, where possible.\n |      \n |      Returns:\n |          np.ndarray: Array of header names converted to floats.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables\n |  \n |  __weakref__\n |      list of weak references to the object\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __annotations__ = {'data': numpy.ndarray[typing.Any, numpy.dtype[numpy...\n |  \n |  __dataclass_fields__ = {'data': Field(name='data',type=numpy.ndarray[t...\n |  \n |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n |  \n |  __hash__ = None\n |  \n |  __match_args__ = ('header', 'data', 'time', 'files')\n\n</pre>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#loading-part-1-into-1d-data","title":"Loading Part 1: Into &amp; 1D Data\u00b6","text":"<p>This notebook is designed to guide you through the process of loading, cleaning, formatting, and processing data, a common task in aerosol research. You'll learn how to automate these repetitive tasks using Python scripts, specifically leveraging the functionalities of the <code>particula_beta.data</code> package. This will streamline your data handling process, making your research more efficient.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#setting-the-working-path","title":"Setting the Working Path\u00b6","text":"<p>The first step in data processing is to set the working path where your data files are located. In this example, we'll use example data from the data/examples directory. However, you can easily adapt this to point to any directory on your computer.</p> <p>For instance, if you have a data folder in your home directory, you might set the path like this: <code>path = \"U:\\\\data\\\\processing\\\\Campaign2023_of_awesome\\\\data\"</code></p> <p>This flexibility allows you to work with data stored in different locations on your computer.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#importing-packages","title":"Importing Packages\u00b6","text":"<p>Before we get started, we need to import the packages we'll be using.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#loading-the-data","title":"Loading the Data\u00b6","text":"<p>With the working directory set, we're ready to load the data. For this task, we'll use the <code>loader</code> module from the <code>particula</code> package. The <code>loader.data_raw_loader()</code> function allows us to easily load data files by providing the file path. This simplifies the initial step of any data analysis process, especially for those new to Python.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#clean-up-data","title":"Clean Up Data\u00b6","text":"<p>To facilitate data cleaning, we utilize <code>loader.data_format_checks</code>. This function performs a series of checks on the data, ensuring it meets specified criteria. The checks include:</p> <ul> <li><code>characters</code>: Validates the minimum and maximum number of characters in each data line, ensuring data integrity and uniformity.</li> <li><code>char_counts</code>: Counts the occurrences of specific characters in each line. This is defined in a dictionary, with characters as keys and their expected counts as values.</li> <li><code>skip_rows</code>: Specifies the number of rows to skip at the beginning of the file, useful for bypassing headers or non-data lines.</li> <li><code>skip_end</code>: Determines the number of rows to omit at the end of the file, often used to avoid reading summary or footer information.</li> </ul> <p>After performing these checks, the function returns a list of data that has passed all the criteria. This cleaned data is then ready for further analysis or processing.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#data-and-time","title":"Data and Time\u00b6","text":"<p>Once the data is cleaned, the next crucial step is extracting and standardizing the time and data columns. This process is particularly important because time data can come in many formats, and standardizing it to a single format, like epoch time, facilitates analysis and comparison. Epoch time, also known as Unix time, is a system for describing points in time as the number of seconds elapsed since January 1, 1970. It's a widely used standard in computing and data processing.</p> <p>To handle the conversion and extraction of time and data, we use the <code>loader.sample_data()</code> function. This function is designed to:</p> <ol> <li>Identify and Extract Time Data: It locates the time information within the dataset, which can be in various formats such as ISO 8601, DD/MM/YYYY, MM/DD/YYYY, etc.</li> <li>Convert to Epoch Time: It standardizes the extracted time data to epoch time. This conversion is crucial because epoch time provides a consistent reference for time data, making it easier to perform time-based calculations and comparisons.</li> <li>Separate Data Columns: Along with time data, it also segregates other data columns for further analysis.</li> </ol> <p>By using <code>loader.sample_data()</code> to convert various time formats to epoch time and separate the data columns, we effectively prepare our dataset for robust and error-free analysis. This approach is essential in research and data science, where dealing with multiple time formats is a common challenge.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#pause-to-plot-verifying-data-integrity","title":"Pause to Plot: Verifying Data Integrity\u00b6","text":"<p>After successfully importing the data and converting time information, it's crucial to pause and visually inspect our dataset. Plotting the data serves as an essential checkpoint to verify that the import process has been executed correctly. This step is vital for several reasons:</p> <ol> <li><p>Data Integrity Check: Visualizing the data helps in quickly identifying any anomalies or irregularities that might indicate issues in the data import or cleaning processes.</p> </li> <li><p>Understanding Data Structure: A plot can provide immediate insights into the nature and structure of the dataset, such as trends, patterns, and outliers.</p> </li> <li><p>Ensuring Accuracy: Before proceeding to more complex analyses or modeling, confirming the accuracy of the data through visualization is a fundamental best practice.</p> </li> </ol> <p>In this section, we will create a simple plot to examine our time series data, ensuring that the time conversion and data import have been performed correctly.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#stream-object-streamlining-your-analysis","title":"Stream Object: Streamlining Your Analysis\u00b6","text":"<p>Once you have the cleaned and formatted data, you might find yourself repeatedly using the same code to analyze different datasets. To avoid the tedium of copying and pasting the same code, we can utilize the <code>Stream</code> object provided by the <code>loader</code> module. The <code>Stream</code> object allows for more automated and efficient analysis, significantly simplifying repetitive tasks.</p> <p>The <code>Stream</code> object requires a settings dictionary that encapsulates all necessary parameters for data loading. Below, we'll explore how to create this settings dictionary and use it to initialize a <code>Stream</code> object.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#settings-dictionary-centralizing-data-loading-parameters","title":"Settings Dictionary: Centralizing Data Loading Parameters\u00b6","text":"<p>The settings dictionary is a crucial component in the data loading process. It consolidates all the necessary settings for loading your data into a single, easily manageable structure. This includes parameters for data checks, specification of time and data columns, as well as the time format.</p> <p>There are two primary methods to generate this settings dictionary:</p> <ol> <li><p>Using <code>LoaderSetting1DBuilder</code>: This is a builder class that simplifies the creation of the settings dictionary. It provides a structured approach to defining the settings, ensuring that all required parameters are included. This method is particularly useful for beginners or those looking for a more guided approach.</p> </li> <li><p>Manual Creation: Alternatively, you can manually construct the settings dictionary. This might involve writing the dictionary from scratch or modifying one generated by the <code>settings_generator</code> module. Manual creation offers more flexibility and is particularly useful if your data loading requirements are unique or complex.</p> </li> </ol> <p>In the next steps, we will demonstrate how to use the settings dictionary for loading data into a <code>Stream</code> object.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#load-the-data-with-the-interface","title":"Load the Data with the Interface\u00b6","text":"<p>With our settings dictionary in hand, we can now streamline the data loading process using a specialized interface. This interface acts as a bridge between our predefined settings and the actual data loading steps. It essentially automates the sequence of actions we performed manually earlier, like data cleaning and formatting, based on the parameters specified in the settings dictionary.</p> <p>By utilizing this interface, we can efficiently load our data with a few lines of code, ensuring consistency and accuracy. This approach is particularly beneficial when dealing with multiple datasets or when needing to replicate the same data processing steps in different projects.</p> <p>The interface function we'll use is designed to accept the settings dictionary and the location of the data. It then internally calls the necessary functions to execute the data loading process. This includes:</p> <ul> <li>Reading the data files based on the specified filename patterns and folder locations.</li> <li>Performing data checks and cleaning as defined in the settings.</li> <li>Extracting and formatting time and data columns according to our requirements.</li> </ul> <p>This method significantly simplifies the data loading process, reducing the potential for errors and increasing efficiency.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#exploring-the-stream-class-in-particula","title":"Exploring the Stream Class in Particula\u00b6","text":"<p>The <code>Stream</code> class in the Particula package is a sophisticated tool for data management, akin to a well-organized filing cabinet for your data. Let's dive into its features and functionalities to understand how it can streamline your data analysis tasks.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#key-features-of-the-stream-class","title":"Key Features of the Stream Class\u00b6","text":"<ul> <li><p>Header: Just as labels on filing cabinet drawers help you identify contents, the <code>header</code> in <code>Stream</code> serves a similar purpose. It's a list of strings that represent the column names of your data, guiding you to understand what each column in your dataset signifies.</p> </li> <li><p>Data: Imagine each drawer in a filing cabinet filled with files; <code>data</code> in <code>Stream</code> is akin to this. It's a numpy array containing your dataset, neatly organized where each row represents a point in time and each column corresponds to one of the headers.</p> </li> <li><p>Time: Like time tags on files that show when they were recorded, the <code>time</code> attribute in <code>Stream</code> keeps a chronological record of each data entry. It\u2019s a numpy array that represents the time dimension of your data, correlating each row in the <code>data</code> array with a specific moment.</p> </li> <li><p>Files: Similar to having a list that tells you about all the folders inside a filing cabinet, <code>files</code> in <code>Stream</code> reveals the names of all the data files that comprise your data stream. This list of strings provides a clear trace back to the original data sources.</p> </li> </ul>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#functionalities-and-tools","title":"Functionalities and Tools\u00b6","text":"<ul> <li><p>validate_inputs: This function acts like a checklist ensuring all your files are correctly formatted before being placed in the cabinet. In <code>Stream</code>, it checks the validity of header, data, and time inputs, ensuring everything is in order for smooth data handling.</p> </li> <li><p>datetime64: Think of it as a tool that standardizes the dates and times on your documents into a uniform format (datetime64), making them easier to understand and use, particularly for plotting and time-based operations.</p> </li> <li><p>return_header_dict: It\u2019s like having a quick reference guide in your filing cabinet, telling you exactly where to find data corresponding to each label (header). In <code>Stream</code>, it offers a dictionary mapping header elements to their indices in the data array, simplifying data access.</p> </li> </ul>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#practical-application","title":"Practical Application\u00b6","text":"<p>For beginners and experienced users alike, the Stream class in Particula package is your organized, efficient data manager. It takes the complexity out of data handling, allowing you to focus more on insightful analysis and less on the intricacies of data organization.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#plotting-data-using-the-stream-object","title":"Plotting Data Using the Stream Object\u00b6","text":"<p>Visualizing your data is a crucial step in understanding its patterns and behavior. With the <code>data_stream</code> object from the Particula package, plotting time-series data becomes straightforward and efficient. Here's how you can use <code>data_stream</code> for plotting:</p> <ol> <li><p>Preparing the Plot: Start by creating a figure and an axis using <code>matplotlib</code>'s <code>plt.subplots()</code>. This sets up the canvas on which you'll draw your plot.</p> </li> <li><p>Plotting Time-Series Data:</p> <ul> <li>The <code>data_stream.datetime64</code> provides time data in a format that is ideal for plotting on the x-axis.</li> <li>Since <code>data_stream.data</code> is a 2D array (where rows correspond to time and columns to different data types), you need to specify which column you want to plot. For example, <code>data_stream.data[:, 0]</code> plots the first column of data.</li> </ul> </li> <li><p>Customizing the Plot:</p> <ul> <li>Add labels to your plot for clarity. The <code>label</code> parameter in the <code>ax.plot</code> function can be used to name the data series being plotted.</li> <li>Customize the appearance of your plot. In this example, we use <code>linestyle=\"none\"</code> and <code>marker=\".\"</code> to plot individual data points without connecting lines.</li> </ul> </li> <li><p>Adjusting Axes and Display:</p> <ul> <li>The <code>plt.tick_params</code> function allows you to rotate the x-axis labels, making them easier to read, especially for densely plotted data.</li> <li>Set the x-axis and y-axis labels using <code>ax.set_xlabel</code> and <code>ax.set_ylabel</code> to provide context to your plot.</li> </ul> </li> <li><p>Final Touches:</p> <ul> <li>Include a legend by calling <code>ax.legend()</code>. This is particularly useful when plotting multiple data series.</li> <li>Use <code>plt.show()</code> to display the plot.</li> <li>The <code>fig.tight_layout()</code> ensures that the layout of the plot is adjusted so that all elements (like labels and titles) are clearly visible.</li> </ul> </li> </ol>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#summary","title":"Summary\u00b6","text":"<p>In this section, we explored a comprehensive approach to handling data using the functionalities provided by the Particula package. Key takeaways include:</p> <ol> <li><p>Data Loading and Cleaning: We began by loading data from files, emphasizing the importance of automating the cleaning process. This step involved removing errors, handling missing values, and ensuring the data is in a consistent format, which is crucial for accurate analysis.</p> </li> <li><p>Using the Settings Dictionary: A significant part of the process was the creation of a settings dictionary. This dictionary serves as a blueprint for the data loading process, specifying parameters like data checks, column information, and time formatting. This approach is particularly effective when working with large datasets or needing to replicate data processing steps across various projects.</p> </li> <li><p>Streamlining with the Stream Object: We introduced the <code>Stream</code> object, a powerful tool for organizing and processing data streams. The <code>Stream</code> object allows for efficient data manipulation, storage, and retrieval, making it an invaluable resource for handling complex or voluminous datasets.</p> </li> <li><p>Simplifying Repetitive Tasks: By automating data loading and cleaning through the <code>Stream</code> object and settings dictionary, we significantly reduced the redundancy of repetitive tasks. This method proves beneficial in projects where data from multiple files needs to be loaded and combined into a single, manageable unit.</p> </li> <li><p>Visualization and Analysis: Finally, we demonstrated how to plot the data using <code>matplotlib</code>, a crucial step for visually inspecting the data and ensuring its integrity post-import. This visual check is vital for confirming that the import process has been executed correctly and the data is ready for further analysis.</p> </li> </ol> <p>In conclusion, this section provided a solid foundation for efficiently managing and processing data in Python, setting the stage for more advanced analysis and applications in future sections.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#bonus-utilizing-pythons-help-function","title":"Bonus: Utilizing Python's Help Function\u00b6","text":"<p>One of Python's most useful built-in functions for learners and developers alike is the <code>help</code> function. It provides detailed information about objects, functions, modules, and more, directly within your Python environment. This can be particularly helpful when exploring new libraries or understanding the functionalities of different components in your code.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#how-to-use-the-help-function","title":"How to Use the Help Function\u00b6","text":"<p>The <code>help</code> function can be invoked directly in your Python code or interactive session. Simply pass the object or function you're curious about as an argument to <code>help()</code>, and it will display documentation including descriptions, available methods, attributes, and other relevant details.</p> <p>For example, to learn more about the <code>data_stream</code> object we've been working with, you can use:</p> <pre>help(data_stream)\n</pre> <p>This command will display information about the Stream class, including its methods, attributes, and how to use them.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part1/#applying-help-to-explore-functionality","title":"Applying Help to Explore Functionality\u00b6","text":"<p>You can use help with any object or function in Python. For instance, if you want to understand more about a function from the Particula package or even a built-in Python function, simply pass it to help(). Here's how you can use it:</p> <p>To explore a module: help(particula) To learn about a specific function: help(particula.some_function) To understand an object you created: help(my_object) Enhancing Learning and Troubleshooting Using the help function is an excellent habit to develop. It enhances your learning process by providing immediate access to documentation. It's also a valuable tool for troubleshooting and understanding the functionalities of different components in your code.</p> <p>Remember, the help function is always there to assist you, making it a bit easier to navigate the extensive world of Python programming.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/","title":"Formatting the Data","text":"In\u00a0[1]: Copied! <pre>import os  # For handling file and directory paths\nimport numpy as np  # For numerical operations\nimport matplotlib.pyplot as plt  # For plotting and visualizing data\n\n# Particula package components\nfrom particula_beta.data import loader, loader_interface\nfrom particula_beta.data.loader_setting_builders import (\n    SizerDataReaderBuilder,\n    LoaderSizerSettingsBuilder,\n    DataChecksBuilder,\n)\n\n# For accessing example data\nfrom particula_beta.data.tests.example_data.get_example_data import (\n    get_data_folder,\n)\n</pre> import os  # For handling file and directory paths import numpy as np  # For numerical operations import matplotlib.pyplot as plt  # For plotting and visualizing data  # Particula package components from particula_beta.data import loader, loader_interface from particula_beta.data.loader_setting_builders import (     SizerDataReaderBuilder,     LoaderSizerSettingsBuilder,     DataChecksBuilder, )  # For accessing example data from particula_beta.data.tests.example_data.get_example_data import (     get_data_folder, ) <p>Now, we'll determine the current working directory and set the path for the data folder. This step is essential for ensuring that our scripts know where to look for the data files.</p> In\u00a0[2]: Copied! <pre># Retrieving and printing the current working directory of this script\ncurrent_path = os.getcwd()\nprint(\"Current path for this script:\")\nprint(current_path.rsplit(\"particula\")[-1])\n\n# Setting and printing the path to the data folder\npath = get_data_folder()\nprint(\"Path to data folder:\")\nprint(path.rsplit(\"particula\")[-1])\n</pre> # Retrieving and printing the current working directory of this script current_path = os.getcwd() print(\"Current path for this script:\") print(current_path.rsplit(\"particula\")[-1])  # Setting and printing the path to the data folder path = get_data_folder() print(\"Path to data folder:\") print(path.rsplit(\"particula\")[-1]) <pre>Current path for this script:\n-beta\\docs\\How-To-Guides\\Data_Streams_and_Lakes\\notebooks\nPath to data folder:\n_beta\\data\\tests\\example_data\n</pre> In\u00a0[3]: Copied! <pre># Constructing the full file path for the data file\n# We're joining the path set earlier with the specific file we want to load\ndata_file = os.path.join(\n    path,  # The base path we set earlier\n    \"SMPS_data\",  # The subdirectory where the data file is located\n    \"2022-07-07_095151_SMPS.csv\",\n)  # The name of the data file\n\n# Optional: Print the file path to confirm it's correct\n# print(\"Data file path:\", data_file)\n\n# Using the loader module to load the data from the file\n# The data_raw_loader function takes the file path as its argument and\n# reads the data\nraw_data = loader.data_raw_loader(data_file)\n\n# Printing a snippet of the loaded data for a quick preview\n# This helps us to confirm that the data is loaded and to get an idea of\n# its structure\nprint(\"Preview of loaded data:\")\nfor row in raw_data[22:30]:  # Displaying rows 22 to 30 as a sample\n    print(row)\n</pre> # Constructing the full file path for the data file # We're joining the path set earlier with the specific file we want to load data_file = os.path.join(     path,  # The base path we set earlier     \"SMPS_data\",  # The subdirectory where the data file is located     \"2022-07-07_095151_SMPS.csv\", )  # The name of the data file  # Optional: Print the file path to confirm it's correct # print(\"Data file path:\", data_file)  # Using the loader module to load the data from the file # The data_raw_loader function takes the file path as its argument and # reads the data raw_data = loader.data_raw_loader(data_file)  # Printing a snippet of the loaded data for a quick preview # This helps us to confirm that the data is loaded and to get an idea of # its structure print(\"Preview of loaded data:\") for row in raw_data[22:30]:  # Displaying rows 22 to 30 as a sample     print(row) <pre>Preview of loaded data:\nUnits,dW/dlogDp\nWeight,Number\nSample #,Date,Start Time,Sample Temp (C),Sample Pressure (kPa),Relative Humidity (%),Mean Free Path (m),Gas Viscosity (Pa*s),Diameter Midpoint (nm),20.72,21.10,21.48,21.87,22.27,22.67,23.08,23.50,23.93,24.36,24.80,25.25,25.71,26.18,26.66,27.14,27.63,28.13,28.64,29.16,29.69,30.23,30.78,31.34,31.91,32.49,33.08,33.68,34.29,34.91,35.55,36.19,36.85,37.52,38.20,38.89,39.60,40.32,41.05,41.79,42.55,43.32,44.11,44.91,45.73,46.56,47.40,48.26,49.14,50.03,50.94,51.86,52.80,53.76,54.74,55.73,56.74,57.77,58.82,59.89,60.98,62.08,63.21,64.36,65.52,66.71,67.93,69.16,70.41,71.69,72.99,74.32,75.67,77.04,78.44,79.86,81.31,82.79,84.29,85.82,87.38,88.96,90.58,92.22,93.90,95.60,97.34,99.10,100.90,102.74,104.60,106.50,108.43,110.40,112.40,114.44,116.52,118.64,120.79,122.98,125.21,127.49,129.80,132.16,134.56,137.00,139.49,142.02,144.60,147.22,149.89,152.61,155.38,158.20,161.08,164.00,166.98,170.01,173.09,176.24,179.43,182.69,186.01,189.38,192.82,196.32,199.89,203.51,207.21,210.97,214.80,218.70,222.67,226.71,230.82,235.01,239.28,243.62,248.05,252.55,257.13,261.80,266.55,271.39,276.32,281.33,286.44,291.64,296.93,302.32,307.81,313.40,319.08,324.88,330.77,336.78,342.89,349.12,355.45,361.90,368.47,375.16,381.97,388.91,395.96,403.15,410.47,417.92,425.51,433.23,441.09,449.10,457.25,465.55,474.00,482.61,491.37,500.29,509.37,518.61,528.03,537.61,547.37,557.31,567.42,577.72,588.21,598.89,609.76,620.82,632.09,643.57,655.25,667.14,679.25,691.58,704.14,716.92,729.93,743.18,756.67,770.40,784.39,Scan Time (s),Retrace Time (s),Scan Resolution (Hz),Scans Per Sample,HV Polarity,Sheath Flow (L/min),Aerosol Flow (L/min),Bypass Flow (L/min),Low Voltage (V),High Voltage (V),Lower Size (nm),Upper Size (nm),Density (g/cm\u00b3),td + 0.5 (s),tf (s),D50 (nm),Median (nm),Mean (nm),Geo. Mean (nm),Mode (nm),Geo. Std. Dev.,Total Conc. (#/cm\u00b3),Neutralizer Status,Dilution Factor,Test Name,Test Description,Dataset Name,Dataset Description,Instrument Errors\n1,07/07/2022,08:49:17,23.7,101.2,61.9,6.75690e-8,1.83579e-5,,6103.186,2832.655,4733.553,4765.944,5960.964,4475.806,4412.044,5853.069,4832.167,3781.343,3675.830,3271.549,3084.392,3668.269,4116.143,3310.157,3978.368,4151.566,2515.995,3755.837,2776.663,5032.745,3775.426,2818.553,2641.302,2636.806,3079.759,2606.094,2317.234,3192.346,2226.703,2484.878,3394.395,1762.834,3172.359,2919.533,2452.013,3403.780,2360.277,2543.386,2563.290,2649.769,1375.374,1364.046,1446.529,2068.167,1336.070,1542.077,1707.249,1482.481,2272.182,1754.409,2472.438,1191.563,2221.825,1635.293,2548.571,1991.926,2546.956,1790.114,2115.075,1138.769,1934.746,2163.955,1613.179,2132.750,1654.348,1698.154,2403.529,1222.983,1829.254,1197.162,1638.797,1248.565,2417.521,1130.421,1429.423,1694.923,1658.378,1443.393,1731.346,1277.799,1089.149,1072.630,1205.387,1693.146,1109.648,915.428,491.529,881.028,1218.297,755.658,714.301,686.247,790.943,398.805,1043.226,1298.495,1548.704,1070.899,846.596,938.241,232.947,926.941,837.452,794.492,254.455,392.637,353.144,872.576,693.986,1544.164,657.340,546.445,311.890,365.934,616.794,610.810,938.786,815.964,593.441,939.634,188.115,1077.429,1213.142,737.913,1876.626,735.779,996.521,1098.601,1166.494,962.551,1392.535,947.504,655.459,993.819,682.087,852.503,601.057,733.860,529.122,960.578,687.512,839.973,652.820,289.921,623.835,453.604,588.057,856.253,283.994,282.839,365.801,200.382,365.756,146.548,306.730,373.162,114.272,0.000,182.692,260.788,164.857,19.851,89.612,0.000,181.974,0.000,53.276,20.016,0.000,0.000,95.433,96.222,0.000,0.000,197.307,0.000,100.336,0.000,102.072,0.000,0.000,209.529,0.000,213.227,107.561,0.000,218.965,220.930,111.453,0.000,113.460,0.000,115.513,0.000,0.000,0.000,0.000,28.221,93.413,122.992,0.000,75,4,50,1,Negative,2.000,0.300,0.00,10.07,9863.01,20.5,791.5,1.0,1.81,10.79,1000.0,41.562,74.959,52.078,20.721,2.179,2.16900e+3,ON,1,TRACER-CAT,,2022 07 07 09_51,,Detector aerosol flow rate error;Incomplete Scan\n2,07/07/2022,08:50:48,23.6,101.2,61.7,6.75401e-8,1.83531e-5,,5621.118,5867.747,6233.403,3453.156,4484.307,5468.148,4725.052,4689.983,3661.759,4356.725,4292.911,7728.414,5112.679,4746.084,3957.005,3472.977,3496.697,4674.202,4188.868,2868.559,3375.113,4306.112,5191.077,4732.512,4566.029,3514.167,5172.877,3825.270,5323.756,2327.737,3846.602,2347.097,3182.011,1876.273,2952.863,2831.255,2497.869,4158.061,3828.510,3199.720,2309.195,2462.550,3060.240,1086.744,1476.289,2069.774,1727.787,2710.631,2067.327,2619.082,2345.026,2362.235,1429.749,2557.408,2660.327,1209.933,1590.320,1696.569,2236.773,1499.046,1922.632,1650.213,3147.351,2201.919,1622.954,2198.739,1800.998,1429.621,1426.761,1923.931,1262.939,1745.284,1458.571,1523.548,1920.108,1382.558,2211.525,2571.277,1979.297,1562.697,1741.573,1307.680,967.481,838.919,1502.136,1301.401,1011.619,829.770,973.269,1100.004,1152.808,749.250,1187.900,806.256,111.008,297.062,809.059,1361.412,779.536,535.087,881.522,1307.518,800.804,1053.953,182.381,1042.830,673.021,646.171,825.612,963.187,748.743,540.954,769.157,788.222,825.566,236.537,865.009,289.185,803.098,398.510,446.847,439.645,1118.961,1003.003,924.180,745.149,430.134,415.522,805.970,790.348,998.975,1043.136,604.082,1004.545,1082.455,1312.781,1447.390,872.420,398.380,695.719,857.412,645.872,691.129,623.007,471.728,641.049,1023.693,394.611,475.599,446.076,657.686,313.003,136.395,248.550,579.894,336.126,485.938,298.810,0.000,227.571,104.550,157.583,289.697,0.229,0.000,0.000,217.592,67.816,24.067,0.000,0.000,0.000,0.000,0.000,97.009,0.000,0.000,0.000,0.000,0.000,0.000,205.900,0.000,104.761,0.000,0.000,0.000,108.509,0.000,110.461,0.000,0.000,0.000,114.471,115.506,116.540,0.000,118.660,0.000,0.000,0.000,0.000,75.377,75,4,50,1,Negative,2.000,0.300,0.00,10.07,9863.01,20.5,791.5,1.0,1.81,10.79,1000.0,39.458,69.080,49.198,25.255,2.101,2.39408e+3,ON,1,TRACER-CAT,,2022 07 07 09_51,,Detector aerosol flow rate error;Incomplete Scan\n3,07/07/2022,08:52:19,23.7,101.2,61.5,6.75690e-8,1.83579e-5,,5165.139,4969.987,4312.386,6939.394,4680.764,3224.473,4999.149,3653.002,4241.532,3928.137,2718.607,3363.947,4863.410,5338.452,4659.515,3430.329,3997.386,4644.421,4943.511,3883.970,3212.310,4445.981,2349.435,3605.419,4366.557,4969.924,4880.573,3186.281,3089.412,2724.537,3195.740,4277.947,4864.436,4263.532,2100.807,1967.634,3283.337,3268.660,3001.917,2781.549,1879.354,1376.083,2051.524,2165.874,2012.210,2923.129,1575.515,1544.252,1610.635,1572.609,1299.370,1549.832,1145.100,2897.864,1839.992,2351.579,2102.027,1543.106,953.811,2073.610,2317.378,2087.617,1586.363,1897.860,2456.722,1647.781,1013.534,1734.023,1633.021,1841.697,2193.442,2714.856,1396.336,2264.046,1671.363,1538.012,1257.148,1423.316,1217.281,1745.437,1787.473,1284.774,1534.815,1274.852,1438.025,1199.602,964.066,862.098,685.995,679.146,879.775,806.703,979.672,894.103,1379.499,1112.031,744.999,580.777,1241.262,960.784,750.484,908.236,957.901,652.265,1200.515,429.487,347.453,552.393,617.871,652.163,709.227,788.963,1499.238,627.895,1315.208,976.800,555.360,440.680,1182.819,863.800,362.530,942.047,460.380,1222.507,678.820,1006.555,319.371,91.941,761.841,205.384,449.120,751.217,572.530,350.734,295.089,413.379,612.088,474.457,678.504,490.408,751.536,400.656,585.567,676.707,364.052,124.385,631.790,788.487,566.062,390.904,141.751,256.369,366.589,528.781,512.078,257.120,393.412,350.601,361.659,65.138,348.203,326.629,329.714,175.810,111.365,74.091,103.212,0.000,0.000,47.532,0.000,166.826,0.000,96.217,388.070,97.832,98.649,99.490,200.678,202.399,0.000,102.953,0.000,0.000,105.683,106.611,33.630,183.108,2.602,218.305,222.901,0.000,226.925,0.000,0.000,116.553,0.000,118.661,119.732,120.801,0.000,122.992,124.085,75,4,50,1,Negative,2.000,0.300,0.00,10.07,9863.01,20.5,791.5,1.0,1.81,10.79,1000.0,39.324,72.102,50.019,21.870,2.136,2.27861e+3,ON,1,TRACER-CAT,,2022 07 07 09_51,,Detector aerosol flow rate error;Incomplete Scan\n4,07/07/2022,08:53:50,23.8,101.2,61.4,6.75979e-8,1.83627e-5,,5814.745,5937.421,5542.118,7127.484,5341.069,4793.690,4938.844,5721.541,4877.746,5900.250,5104.984,4914.366,4891.892,6655.579,4431.173,3389.961,4947.809,3115.245,4138.126,5421.474,4589.063,4007.156,2524.137,5009.064,4780.963,4959.096,3648.285,4148.676,4270.099,2229.465,3043.487,5618.376,3689.188,4700.549,2535.915,1754.223,2560.335,2853.385,2454.711,2515.907,3015.370,1502.864,2344.161,2761.448,2047.076,1542.531,2151.757,2365.884,2330.816,2585.566,1431.955,2391.335,2097.717,1891.014,2211.815,2071.479,2188.302,2475.058,1906.364,1781.793,2356.998,1527.723,2609.446,1644.771,1917.624,1843.984,2418.197,1385.516,1263.621,2155.939,2083.223,1765.167,957.777,2077.747,1667.811,1122.065,1579.113,1709.471,1604.406,686.151,390.075,1194.313,1657.144,1462.232,1870.846,1012.132,847.165,1248.528,1039.604,779.076,1375.101,1058.272,1013.378,1211.420,1641.490,979.146,835.539,763.524,951.720,1270.393,1308.492,1056.486,1715.924,657.112,1475.767,235.866,827.129,1266.089,1080.958,1246.249,1147.116,840.719,1560.246,1201.554,1743.366,1233.526,1166.422,1068.551,1047.492,787.018,759.836,491.419,714.111,460.361,681.068,767.815,654.715,501.038,357.016,575.937,613.281,851.029,583.739,475.691,431.584,616.144,744.932,409.334,984.682,371.750,613.130,757.474,637.077,441.004,609.132,380.961,595.419,565.033,566.955,332.402,450.524,139.761,430.419,443.058,558.628,158.467,271.708,346.807,57.637,148.050,226.825,353.827,77.661,0.000,0.000,74.100,0.000,250.296,117.433,93.156,187.816,0.000,95.443,0.000,0.000,293.505,0.000,99.496,100.342,0.000,102.078,102.959,0.000,0.000,0.000,106.622,322.709,0.000,328.474,0.000,67.473,44.378,0.000,0.000,115.519,0.000,0.000,118.668,0.000,0.000,0.000,0.000,0.000,75,4,50,1,Negative,2.000,0.300,0.00,10.07,9863.01,20.5,791.5,1.0,1.81,10.79,1000.0,37.995,68.796,48.896,21.870,2.107,2.51144e+3,ON,1,TRACER-CAT,,2022 07 07 09_51,,Detector aerosol flow rate error;Incomplete Scan\n5,07/07/2022,08:55:21,24.0,101.1,61.4,6.77227e-8,1.83722e-5,,8034.425,6317.981,6972.600,4577.324,6488.519,4985.397,5484.518,7295.312,3449.590,4261.716,4259.456,6124.670,4418.824,5418.742,3311.293,3548.897,4940.747,6738.536,3377.823,3309.433,5322.339,4148.187,3387.285,3967.636,5064.382,4573.259,3896.245,4006.531,3769.030,4129.946,4678.454,3121.839,3888.625,2443.782,1947.617,2321.130,1845.465,2833.269,2745.881,3262.145,4055.876,2319.187,3397.282,2596.623,2935.256,1508.733,1555.232,3184.200,2683.631,2158.530,2303.663,2739.336,2714.276,2536.377,2051.076,2063.667,2074.972,2852.267,2366.702,2135.668,1500.801,2228.817,2220.527,1501.131,2354.567,2072.434,2547.917,2111.890,1474.809,1561.614,1334.889,1100.318,1077.335,1470.618,1377.825,1684.933,1093.441,1596.409,1456.255,1543.298,1116.499,984.258,1294.805,1586.816,723.664,1709.369,1060.965,1415.310,1611.158,1791.258,1098.238,1513.790,1335.019,1178.572,1538.772,477.803,1130.380,1596.999,652.664,1098.951,1384.104,772.285,788.185,1432.363,773.331,729.470,819.882,979.684,925.309,753.771,706.255,659.741,1026.707,818.647,1205.428,940.460,906.655,758.763,811.344,1123.245,520.356,1009.392,651.265,735.336,209.657,549.624,537.181,841.849,483.705,713.011,497.248,743.196,556.459,953.140,847.692,614.097,423.810,816.193,627.059,453.998,976.898,592.170,548.197,535.480,667.837,312.390,476.781,369.028,451.687,432.520,1001.512,312.053,498.408,198.771,399.968,363.778,403.848,381.782,223.839,227.667,212.819,101.097,164.909,359.326,285.450,0.000,44.177,0.000,158.441,220.559,81.404,49.687,95.468,0.000,194.095,391.452,98.679,0.000,0.000,0.000,0.000,102.990,103.881,104.800,0.000,106.644,0.000,108.554,0.000,110.496,0.000,112.492,113.494,0.000,115.548,0.000,0.000,0.000,239.531,241.683,0.000,0.000,248.252,75,4,50,1,Negative,2.000,0.300,0.00,10.07,9863.01,20.5,791.5,1.0,1.81,10.79,1000.0,39.214,69.960,48.959,20.721,2.123,2.56068e+3,ON,1,TRACER-CAT,,2022 07 07 09_51,,Detector aerosol flow rate error;Incomplete Scan\n</pre> In\u00a0[4]: Copied! <pre># Formatting the data for time series analysis\n# The sizer_data_formatter function from the loader module is used for\n# this purpose\n\ndata_checks = (\n    DataChecksBuilder()\n    .set_characters([250])\n    .set_skip_rows(25)\n    .set_char_counts({\"/\": 2, \":\": 2})\n    .build()\n)\ndata_sizer_reader = (\n    SizerDataReaderBuilder()\n    .set_sizer_start_keyword(\"20.72\")\n    .set_sizer_end_keyword(\"784.39\")\n    .set_sizer_concentration_convert_from(\"dw/dlogdp\")\n    .build()\n)\n\nepoch_time, data, header = loader.sizer_data_formatter(\n    data=raw_data,  # The raw data that was loaded earlier\n    data_checks=data_checks,\n    data_sizer_reader=data_sizer_reader,\n    time_column=[1, 2],  # Columns that contain the time data\n    time_format=\"%m/%d/%Y %H:%M:%S\",  # Format of the time data\n    delimiter=\",\",  # Delimiter used in the data file\n    header_row=24,\n)  # Row number that contains the header\n\n# Printing a preview of the formatted data to confirm successful formatting\nprint(\"Epoch time (First 5 Entries):\")\nprint(epoch_time[:5])  # Displaying the first 5 time entries\nprint(\"Data shape:\")\nprint(data.shape)  # Showing the shape of the data array\nprint(\"Header (First 10 Entries):\")\nprint(header[:10])  # Displaying the first 10 headers\n</pre> # Formatting the data for time series analysis # The sizer_data_formatter function from the loader module is used for # this purpose  data_checks = (     DataChecksBuilder()     .set_characters([250])     .set_skip_rows(25)     .set_char_counts({\"/\": 2, \":\": 2})     .build() ) data_sizer_reader = (     SizerDataReaderBuilder()     .set_sizer_start_keyword(\"20.72\")     .set_sizer_end_keyword(\"784.39\")     .set_sizer_concentration_convert_from(\"dw/dlogdp\")     .build() )  epoch_time, data, header = loader.sizer_data_formatter(     data=raw_data,  # The raw data that was loaded earlier     data_checks=data_checks,     data_sizer_reader=data_sizer_reader,     time_column=[1, 2],  # Columns that contain the time data     time_format=\"%m/%d/%Y %H:%M:%S\",  # Format of the time data     delimiter=\",\",  # Delimiter used in the data file     header_row=24, )  # Row number that contains the header  # Printing a preview of the formatted data to confirm successful formatting print(\"Epoch time (First 5 Entries):\") print(epoch_time[:5])  # Displaying the first 5 time entries print(\"Data shape:\") print(data.shape)  # Showing the shape of the data array print(\"Header (First 10 Entries):\") print(header[:10])  # Displaying the first 10 headers <pre>Epoch time (First 5 Entries):\n[1.65718376e+09 1.65718385e+09 1.65718394e+09 1.65718403e+09\n 1.65718412e+09]\nData shape:\n(2854, 203)\nHeader (First 10 Entries):\n['20.72', '21.10', '21.48', '21.87', '22.27', '22.67', '23.08', '23.50', '23.93', '24.36']\n</pre> In\u00a0[5]: Copied! <pre># Creating a plot using matplotlib\nfig, ax = plt.subplots()  # Creating a figure and axis for the plot\n\n# Plotting data from a specific size bin against time\n# 'epoch_time' is used on the x-axis (time data)\n# 'data[:, 50]' selects the data from the 50th bin (as an example) for the y-axis\nax.plot(\n    epoch_time,\n    data[:, 50],  # Selecting the 50th bin of data to plot\n    label=f\"Bin {header[50]} nm\",  # Adding a label with the bin size\n)\n\n# Setting labels for the x-axis and y-axis\nax.set_xlabel(\"Time (epoch)\")  # Label for the x-axis\nax.set_ylabel(\"Bin Concentration (#/cm\u00b3)\")  # Label for the y-axis\n\n# Adding a legend to the plot for clarity\nax.legend()\n\n# Displaying the plot\nplt.show()\n\n# Adjusting the layout to ensure all plot elements are visible and\n# well-arranged\nfig.tight_layout()\n</pre> # Creating a plot using matplotlib fig, ax = plt.subplots()  # Creating a figure and axis for the plot  # Plotting data from a specific size bin against time # 'epoch_time' is used on the x-axis (time data) # 'data[:, 50]' selects the data from the 50th bin (as an example) for the y-axis ax.plot(     epoch_time,     data[:, 50],  # Selecting the 50th bin of data to plot     label=f\"Bin {header[50]} nm\",  # Adding a label with the bin size )  # Setting labels for the x-axis and y-axis ax.set_xlabel(\"Time (epoch)\")  # Label for the x-axis ax.set_ylabel(\"Bin Concentration (#/cm\u00b3)\")  # Label for the y-axis  # Adding a legend to the plot for clarity ax.legend()  # Displaying the plot plt.show()  # Adjusting the layout to ensure all plot elements are visible and # well-arranged fig.tight_layout() In\u00a0[6]: Copied! <pre># Importing the necessary function for converting epoch time to datetime64\nfrom particula.util.time_manage import datetime64_from_epoch_array\n\n# Converting the epoch time to datetime64 format\ntime_in_datetime64 = datetime64_from_epoch_array(epoch_time)\n\n# Creating a plot\nfig, ax = plt.subplots()\n\n# Plotting the data with time in datetime64 format on the x-axis\nax.plot(\n    time_in_datetime64,\n    data[:, 50],  # Selecting the 50th bin of data\n    label=f\"Bin {header[50]} nm\",  # Label for the data series\n)\n\n# Rotating the x-axis labels to 45 degrees for better readability\nplt.xticks(rotation=45)\n\n# Setting the x-axis and y-axis labels\nax.set_xlabel(\"Time (UTC)\")  # Updated label to indicate the time format\nax.set_ylabel(\"Bin Concentration (#/cm\u00b3)\")\n\n# Adding a legend to the plot\nax.legend()\n\n# Displaying the plot\nplt.show()\n\n# Adjusting the layout for a neat presentation\nfig.tight_layout()\n</pre> # Importing the necessary function for converting epoch time to datetime64 from particula.util.time_manage import datetime64_from_epoch_array  # Converting the epoch time to datetime64 format time_in_datetime64 = datetime64_from_epoch_array(epoch_time)  # Creating a plot fig, ax = plt.subplots()  # Plotting the data with time in datetime64 format on the x-axis ax.plot(     time_in_datetime64,     data[:, 50],  # Selecting the 50th bin of data     label=f\"Bin {header[50]} nm\",  # Label for the data series )  # Rotating the x-axis labels to 45 degrees for better readability plt.xticks(rotation=45)  # Setting the x-axis and y-axis labels ax.set_xlabel(\"Time (UTC)\")  # Updated label to indicate the time format ax.set_ylabel(\"Bin Concentration (#/cm\u00b3)\")  # Adding a legend to the plot ax.legend()  # Displaying the plot plt.show()  # Adjusting the layout for a neat presentation fig.tight_layout() In\u00a0[7]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Setting limits on the concentration data to improve plot readability\nconcentration = data\nconcentration = np.where(\n    concentration &lt; 1e-5, 1e-5, concentration\n)  # Setting a lower limit\nconcentration = np.where(\n    concentration &gt; 10**5, 10**5, concentration\n)  # Setting an upper limit\n# Uncomment the next line to plot concentration in logarithmic scale\n# concentration = np.log10(concentration)\n\n# Creating a figure and axis for the contour plot\nfig, ax = plt.subplots(1, 1)\n\n# Creating the contour plot\nplt.contourf(\n    epoch_time,  # X-axis: Time data in epoch format\n    # Y-axis: Particle sizes converted to float\n    np.array(header).astype(float),\n    concentration.T,  # Transposed concentration data for correct orientation\n    cmap=plt.cm.PuBu_r,  # Color map for the plot\n    levels=50,  # Number of levels in the contour plot\n)\n\n# Setting the y-axis to logarithmic scale for better visualization of size\n# distribution\nplt.yscale(\"log\")\n\n# Setting labels for the x-axis and y-axis\nax.set_xlabel(\"Epoch Time\")  # Label for the x-axis\nax.set_ylabel(\"Diameter (nm)\")  # Label for the y-axis\n\n# Adding a color bar to the plot, indicating concentration levels\nplt.colorbar(label=\"Concentration dN/dlogDp [#/cm\u00b3]\", ax=ax)\n\n# Displaying the plot\nplt.show()\n\n# Adjusting the layout for a better presentation of the plot elements\nfig.tight_layout()\n</pre> import numpy as np import matplotlib.pyplot as plt  # Setting limits on the concentration data to improve plot readability concentration = data concentration = np.where(     concentration &lt; 1e-5, 1e-5, concentration )  # Setting a lower limit concentration = np.where(     concentration &gt; 10**5, 10**5, concentration )  # Setting an upper limit # Uncomment the next line to plot concentration in logarithmic scale # concentration = np.log10(concentration)  # Creating a figure and axis for the contour plot fig, ax = plt.subplots(1, 1)  # Creating the contour plot plt.contourf(     epoch_time,  # X-axis: Time data in epoch format     # Y-axis: Particle sizes converted to float     np.array(header).astype(float),     concentration.T,  # Transposed concentration data for correct orientation     cmap=plt.cm.PuBu_r,  # Color map for the plot     levels=50,  # Number of levels in the contour plot )  # Setting the y-axis to logarithmic scale for better visualization of size # distribution plt.yscale(\"log\")  # Setting labels for the x-axis and y-axis ax.set_xlabel(\"Epoch Time\")  # Label for the x-axis ax.set_ylabel(\"Diameter (nm)\")  # Label for the y-axis  # Adding a color bar to the plot, indicating concentration levels plt.colorbar(label=\"Concentration dN/dlogDp [#/cm\u00b3]\", ax=ax)  # Displaying the plot plt.show()  # Adjusting the layout for a better presentation of the plot elements fig.tight_layout() In\u00a0[8]: Copied! <pre># Importing the necessary module for settings generation\nfrom particula_beta.data import settings_generator\n\n# Generating settings for loading 1d and 2d data from files\n# This is useful for instruments that output both types of data in the\n# same file\n\nsettings_1d, settings_2d = (\n    LoaderSizerSettingsBuilder()\n    .set_relative_data_folder(\"SMPS_data\")\n    .set_filename_regex(\"*.csv\")\n    .set_header_row(24)\n    .set_data_checks(data_checks)\n    .set_data_column(\n        [\n            \"Lower Size (nm)\",\n            \"Upper Size (nm)\",\n            \"Sample Temp (C)\",\n            \"Sample Pressure (kPa)\",\n            \"Relative Humidity (%)\",\n            \"Median (nm)\",\n            \"Mean (nm)\",\n            \"Geo. Mean (nm)\",\n            \"Mode (nm)\",\n            \"Geo. Std. Dev.\",\n            \"Total Conc. (#/cm\u00b3)\",\n        ]\n    )\n    .set_data_header(\n        [\n            \"Lower_Size_(nm)\",\n            \"Upper_Size_(nm)\",\n            \"Sample_Temp_(C)\",\n            \"Sample_Pressure_(kPa)\",\n            \"Relative_Humidity_(%)\",\n            \"Median_(nm)\",\n            \"Mean_(nm)\",\n            \"Geo_Mean_(nm)\",\n            \"Mode_(nm)\",\n            \"Geo_Std_Dev.\",\n            \"Total_Conc_(#/cc)\",\n        ]\n    )\n    .set_data_sizer_reader(data_sizer_reader)\n    .set_time_column([1, 2])\n    .set_time_format(\"%m/%d/%Y %H:%M:%S\")\n    .set_delimiter(\",\")\n    .set_timezone_identifier(\"UTC\")\n    .build()\n)\n\n# Printing the generated settings dictionaries for both 1d and 2d data\nprint(\"Settings for 1d data:\")\nfor key, value in settings_1d.items():\n    print(f\"{key}: {value}\")\n\nprint(\"\\nSettings for 2d data:\")\nfor key, value in settings_2d.items():\n    print(f\"{key}: {value}\")\n</pre> # Importing the necessary module for settings generation from particula_beta.data import settings_generator  # Generating settings for loading 1d and 2d data from files # This is useful for instruments that output both types of data in the # same file  settings_1d, settings_2d = (     LoaderSizerSettingsBuilder()     .set_relative_data_folder(\"SMPS_data\")     .set_filename_regex(\"*.csv\")     .set_header_row(24)     .set_data_checks(data_checks)     .set_data_column(         [             \"Lower Size (nm)\",             \"Upper Size (nm)\",             \"Sample Temp (C)\",             \"Sample Pressure (kPa)\",             \"Relative Humidity (%)\",             \"Median (nm)\",             \"Mean (nm)\",             \"Geo. Mean (nm)\",             \"Mode (nm)\",             \"Geo. Std. Dev.\",             \"Total Conc. (#/cm\u00b3)\",         ]     )     .set_data_header(         [             \"Lower_Size_(nm)\",             \"Upper_Size_(nm)\",             \"Sample_Temp_(C)\",             \"Sample_Pressure_(kPa)\",             \"Relative_Humidity_(%)\",             \"Median_(nm)\",             \"Mean_(nm)\",             \"Geo_Mean_(nm)\",             \"Mode_(nm)\",             \"Geo_Std_Dev.\",             \"Total_Conc_(#/cc)\",         ]     )     .set_data_sizer_reader(data_sizer_reader)     .set_time_column([1, 2])     .set_time_format(\"%m/%d/%Y %H:%M:%S\")     .set_delimiter(\",\")     .set_timezone_identifier(\"UTC\")     .build() )  # Printing the generated settings dictionaries for both 1d and 2d data print(\"Settings for 1d data:\") for key, value in settings_1d.items():     print(f\"{key}: {value}\")  print(\"\\nSettings for 2d data:\") for key, value in settings_2d.items():     print(f\"{key}: {value}\") <pre>Settings for 1d data:\nrelative_data_folder: SMPS_data\nfilename_regex: *.csv\nMIN_SIZE_BYTES: 10000\ndata_loading_function: general_1d_load\nheader_row: 24\ndata_checks: {'characters': [250], 'char_counts': {'/': 2, ':': 2}, 'replace_chars': {}, 'skip_rows': 25, 'skip_end': 0}\ndata_column: ['Lower Size (nm)', 'Upper Size (nm)', 'Sample Temp (C)', 'Sample Pressure (kPa)', 'Relative Humidity (%)', 'Median (nm)', 'Mean (nm)', 'Geo. Mean (nm)', 'Mode (nm)', 'Geo. Std. Dev.', 'Total Conc. (#/cm\u00b3)']\ndata_header: ['Lower_Size_(nm)', 'Upper_Size_(nm)', 'Sample_Temp_(C)', 'Sample_Pressure_(kPa)', 'Relative_Humidity_(%)', 'Median_(nm)', 'Mean_(nm)', 'Geo_Mean_(nm)', 'Mode_(nm)', 'Geo_Std_Dev.', 'Total_Conc_(#/cc)']\ntime_column: [1, 2]\ntime_format: %m/%d/%Y %H:%M:%S\ndelimiter: ,\ntime_shift_seconds: 0\ntimezone_identifier: UTC\n\nSettings for 2d data:\nrelative_data_folder: SMPS_data\nfilename_regex: *.csv\nMIN_SIZE_BYTES: 10000\ndata_loading_function: general_2d_load\nheader_row: 24\ndata_checks: {'characters': [250], 'char_counts': {'/': 2, ':': 2}, 'replace_chars': {}, 'skip_rows': 25, 'skip_end': 0}\ndata_sizer_reader: {'convert_scale_from': 'dw/dlogdp', 'Dp_start_keyword': '20.72', 'Dp_end_keyword': '784.39'}\ndata_header: ['Lower_Size_(nm)', 'Upper_Size_(nm)', 'Sample_Temp_(C)', 'Sample_Pressure_(kPa)', 'Relative_Humidity_(%)', 'Median_(nm)', 'Mean_(nm)', 'Geo_Mean_(nm)', 'Mode_(nm)', 'Geo_Std_Dev.', 'Total_Conc_(#/cc)']\ntime_column: [1, 2]\ntime_format: %m/%d/%Y %H:%M:%S\ndelimiter: ,\ntime_shift_seconds: 0\ntimezone_identifier: UTC\n</pre> In\u00a0[9]: Copied! <pre># Setting the working path to the directory where the data files are located\nworking_path = get_data_folder()\n\n# Using the settings dictionaries created earlier for 1d and 2d data\n\n# Loading 1-dimensional data using the loader interface\n# The interface takes the path and settings for 1d data and loads the data\n# accordingly\ndata_stream_1d = loader_interface.load_files_interface(\n    path=working_path,  # The path where data files are stored\n    settings=settings_1d,  # Settings dictionary for 1d data\n)\n\n# Loading 2-dimensional data using the loader interface\n# Similar to the 1d data, but using the settings for 2d data\ndata_stream_2d = loader_interface.load_files_interface(\n    path=working_path,  # The path where data files are stored\n    settings=settings_2d,  # Settings dictionary for 2d data\n)\n\n# The data_stream_1d and data_stream_2d objects now contain the loaded data\n# ready for further analysis and visualization\n</pre> # Setting the working path to the directory where the data files are located working_path = get_data_folder()  # Using the settings dictionaries created earlier for 1d and 2d data  # Loading 1-dimensional data using the loader interface # The interface takes the path and settings for 1d data and loads the data # accordingly data_stream_1d = loader_interface.load_files_interface(     path=working_path,  # The path where data files are stored     settings=settings_1d,  # Settings dictionary for 1d data )  # Loading 2-dimensional data using the loader interface # Similar to the 1d data, but using the settings for 2d data data_stream_2d = loader_interface.load_files_interface(     path=working_path,  # The path where data files are stored     settings=settings_2d,  # Settings dictionary for 2d data )  # The data_stream_1d and data_stream_2d objects now contain the loaded data # ready for further analysis and visualization <pre>  Loading file: 2022-07-07_095151_SMPS.csv\n  Loading file: 2022-07-10_094659_SMPS.csv\n  Loading file: 2022-07-07_095151_SMPS.csv\n  Loading file: 2022-07-10_094659_SMPS.csv\n</pre> In\u00a0[10]: Copied! <pre># Print a blank line for better readability\nprint(\"\")\n\n# Print the summary of the 1-dimensional data stream\nprint(\"Data stream 1d summary:\")\nprint(data_stream_1d)  # This will display a summary of the 1d data\n\n# Print another blank line for separation\nprint(\"\")\n\n# Print the summary of the 2-dimensional data stream\nprint(\"Data stream 2d summary:\")\nprint(data_stream_2d)  # This will display a summary of the 2d data\n</pre> # Print a blank line for better readability print(\"\")  # Print the summary of the 1-dimensional data stream print(\"Data stream 1d summary:\") print(data_stream_1d)  # This will display a summary of the 1d data  # Print another blank line for separation print(\"\")  # Print the summary of the 2-dimensional data stream print(\"Data stream 2d summary:\") print(data_stream_2d)  # This will display a summary of the 2d data <pre>\nData stream 1d summary:\nStream(header=['Lower_Size_(nm)', 'Upper_Size_(nm)', 'Sample_Temp_(C)', 'Sample_Pressure_(kPa)', 'Relative_Humidity_(%)', 'Median_(nm)', 'Mean_(nm)', 'Geo_Mean_(nm)', 'Mode_(nm)', 'Geo_Std_Dev.', 'Total_Conc_(#/cc)'], data=array([[2.05000e+01, 7.91500e+02, 2.37000e+01, ..., 2.07210e+01,\n        2.17900e+00, 2.16900e+03],\n       [2.05000e+01, 7.91500e+02, 2.36000e+01, ..., 2.52550e+01,\n        2.10100e+00, 2.39408e+03],\n       [2.05000e+01, 7.91500e+02, 2.37000e+01, ..., 2.18700e+01,\n        2.13600e+00, 2.27861e+03],\n       ...,\n       [2.05000e+01, 7.91500e+02, 2.35000e+01, ..., 2.07210e+01,\n        2.31800e+00, 2.08056e+03],\n       [2.05000e+01, 7.91500e+02, 2.33000e+01, ..., 2.10970e+01,\n        2.31800e+00, 2.10616e+03],\n       [2.05000e+01, 7.91500e+02, 2.35000e+01, ..., 2.07210e+01,\n        2.24800e+00, 2.45781e+03]]), time=array([1.65718376e+09, 1.65718385e+09, 1.65718394e+09, ...,\n       1.65753440e+09, 1.65753450e+09, 1.65753459e+09]), files=[['2022-07-07_095151_SMPS.csv', 5620804], ['2022-07-10_094659_SMPS.csv', 2004838]])\n\nData stream 2d summary:\nStream(header=['20.72', '21.10', '21.48', '21.87', '22.27', '22.67', '23.08', '23.50', '23.93', '24.36', '24.80', '25.25', '25.71', '26.18', '26.66', '27.14', '27.63', '28.13', '28.64', '29.16', '29.69', '30.23', '30.78', '31.34', '31.91', '32.49', '33.08', '33.68', '34.29', '34.91', '35.55', '36.19', '36.85', '37.52', '38.20', '38.89', '39.60', '40.32', '41.05', '41.79', '42.55', '43.32', '44.11', '44.91', '45.73', '46.56', '47.40', '48.26', '49.14', '50.03', '50.94', '51.86', '52.80', '53.76', '54.74', '55.73', '56.74', '57.77', '58.82', '59.89', '60.98', '62.08', '63.21', '64.36', '65.52', '66.71', '67.93', '69.16', '70.41', '71.69', '72.99', '74.32', '75.67', '77.04', '78.44', '79.86', '81.31', '82.79', '84.29', '85.82', '87.38', '88.96', '90.58', '92.22', '93.90', '95.60', '97.34', '99.10', '100.90', '102.74', '104.60', '106.50', '108.43', '110.40', '112.40', '114.44', '116.52', '118.64', '120.79', '122.98', '125.21', '127.49', '129.80', '132.16', '134.56', '137.00', '139.49', '142.02', '144.60', '147.22', '149.89', '152.61', '155.38', '158.20', '161.08', '164.00', '166.98', '170.01', '173.09', '176.24', '179.43', '182.69', '186.01', '189.38', '192.82', '196.32', '199.89', '203.51', '207.21', '210.97', '214.80', '218.70', '222.67', '226.71', '230.82', '235.01', '239.28', '243.62', '248.05', '252.55', '257.13', '261.80', '266.55', '271.39', '276.32', '281.33', '286.44', '291.64', '296.93', '302.32', '307.81', '313.40', '319.08', '324.88', '330.77', '336.78', '342.89', '349.12', '355.45', '361.90', '368.47', '375.16', '381.97', '388.91', '395.96', '403.15', '410.47', '417.92', '425.51', '433.23', '441.09', '449.10', '457.25', '465.55', '474.00', '482.61', '491.37', '500.29', '509.37', '518.61', '528.03', '537.61', '547.37', '557.31', '567.42', '577.72', '588.21', '598.89', '609.76', '620.82', '632.09', '643.57', '655.25', '667.14', '679.25', '691.58', '704.14', '716.92', '729.93', '743.18', '756.67', '770.40', '784.39'], data=array([[ 6103.186,  2832.655,  4733.553, ...,    93.413,   122.992,\n            0.   ],\n       [ 5621.118,  5867.747,  6233.403, ...,     0.   ,     0.   ,\n           75.377],\n       [ 5165.139,  4969.987,  4312.386, ...,     0.   ,   122.992,\n          124.085],\n       ...,\n       [ 9962.036,  7986.823,  8682.258, ...,     0.   ,     0.   ,\n          124.153],\n       [ 8765.782, 11175.603,  8148.945, ...,     0.   ,     0.   ,\n          372.433],\n       [14380.528, 11524.35 , 13632.727, ...,     0.   ,     0.   ,\n            0.   ]]), time=array([1.65718376e+09, 1.65718385e+09, 1.65718394e+09, ...,\n       1.65753440e+09, 1.65753450e+09, 1.65753459e+09]), files=[['2022-07-07_095151_SMPS.csv', 5620804], ['2022-07-10_094659_SMPS.csv', 2004838]])\n</pre> In\u00a0[11]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Adjusting the concentration data for better visualization in the plot\nconcentration = data_stream_2d.data\nconcentration = np.where(\n    concentration &lt; 1e-5, 1e-5, concentration\n)  # Setting a lower limit\nconcentration = np.where(\n    concentration &gt; 10**5, 10**5, concentration\n)  # Setting an upper limit\n# Uncomment the following line to plot the concentration on a logarithmic scale\n# concentration = np.log10(concentration)\n\n# Creating a figure and axis for the contour plot\nfig, ax = plt.subplots(1, 1)\n\n# Creating the contour plot\n# X-axis: Time data in datetime64 format from data_stream_2d\n# Y-axis: Particle sizes (diameter in nm) converted from the header strings to floats\n# Z-axis: Concentration data\nplt.contourf(\n    data_stream_2d.datetime64,  # Time data\n    data_stream_2d.header_float,  # Particle sizes\n    concentration.T,  # Concentration data, transposed for correct orientation\n    cmap=plt.cm.PuBu_r,  # Color map for the plot\n    levels=50,  # Number of contour levels\n)\n\n# Setting the y-axis to logarithmic scale for better visualization of size\n# distribution\nplt.yscale(\"log\")\n\n# Rotating the x-axis labels for better readability\nplt.tick_params(rotation=35)\n\n# Setting labels for the x-axis and y-axis\nax.set_xlabel(\"Time (UTC)\")\nax.set_ylabel(\"Diameter (nm)\")\n\n# Adding a color bar to indicate concentration levels\nplt.colorbar(label=\"Concentration dN/dlog(Dp) [#/cm\u00b3]\", ax=ax)\n\n# Displaying the plot\nplt.show()\n\n# Adjusting the layout to ensure all elements of the plot are clearly visible\nfig.tight_layout()\n</pre> import numpy as np import matplotlib.pyplot as plt  # Adjusting the concentration data for better visualization in the plot concentration = data_stream_2d.data concentration = np.where(     concentration &lt; 1e-5, 1e-5, concentration )  # Setting a lower limit concentration = np.where(     concentration &gt; 10**5, 10**5, concentration )  # Setting an upper limit # Uncomment the following line to plot the concentration on a logarithmic scale # concentration = np.log10(concentration)  # Creating a figure and axis for the contour plot fig, ax = plt.subplots(1, 1)  # Creating the contour plot # X-axis: Time data in datetime64 format from data_stream_2d # Y-axis: Particle sizes (diameter in nm) converted from the header strings to floats # Z-axis: Concentration data plt.contourf(     data_stream_2d.datetime64,  # Time data     data_stream_2d.header_float,  # Particle sizes     concentration.T,  # Concentration data, transposed for correct orientation     cmap=plt.cm.PuBu_r,  # Color map for the plot     levels=50,  # Number of contour levels )  # Setting the y-axis to logarithmic scale for better visualization of size # distribution plt.yscale(\"log\")  # Rotating the x-axis labels for better readability plt.tick_params(rotation=35)  # Setting labels for the x-axis and y-axis ax.set_xlabel(\"Time (UTC)\") ax.set_ylabel(\"Diameter (nm)\")  # Adding a color bar to indicate concentration levels plt.colorbar(label=\"Concentration dN/dlog(Dp) [#/cm\u00b3]\", ax=ax)  # Displaying the plot plt.show()  # Adjusting the layout to ensure all elements of the plot are clearly visible fig.tight_layout()"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#loading-part-2-sizer-data","title":"Loading Part 2: Sizer Data\u00b6","text":"<p>Welcome to Part 2 of our data loading series! This section builds upon the concepts and techniques introduced in Part 1. If you haven't gone through the first part, we highly recommend you do so to get a firm foundation. Here, we'll dive into handling 2-dimensional data, such as size distributions, which are common in fields like environmental science and engineering.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#setting-up-your-working-path","title":"Setting Up Your Working Path\u00b6","text":"<p>Before we begin, let's set up the working path, which is the location on your computer where your data files are stored. In this example, we'll use data provided in the current directory of this notebook. However, you can easily change this to any directory where your data files are located. For instance, if you have a folder named \"data\" in your home directory for a project called \"Campaign2023_of_awesome\", you would set the path like this:</p> <p><code>path = \"U:\\\\data\\\\processing\\\\Campaign2023_of_awesome\\\\data\"</code></p> <p>Let's start by importing the necessary Python libraries and modules. We'll explain each one as we use them throughout this notebook.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#load-the-data","title":"Load the Data\u00b6","text":"<p>Now that we've set our working directory, the next step is to load the data. We'll be using the <code>loader</code> module from the Particula package for this task. The function <code>loader.data_raw_loader()</code> is specifically designed to read data from a file, which we'll specify using its file path. This approach is straightforward and efficient for loading data into Python for analysis.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#formatting-the-data","title":"Formatting the Data\u00b6","text":"<p>When dealing with 2-dimensional data, such as size distributions, the formatting process can be a bit more complex compared to 1-dimensional data. For our data, we need to extract size bins and use them as headers for our dataset. This involves specifying the start and end points within the data that define our size bins. In our specific example, the start point is indicated by the keyword \"Date Time\" and the end point by \"Total Conc\". Understanding where your data starts and ends is crucial for accurate formatting and analysis.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#pause-to-plot","title":"Pause to Plot\u00b6","text":"<p>Visualizing your data is a crucial step in the data analysis process. It allows you to see patterns, trends, and potential anomalies that might not be evident from the raw data alone. Now that we have formatted the data and extracted the time information, let's create a plot. This will help us get a visual sense of the data's characteristics, such as the concentration of particles in different size bins over time.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#dates-in-plots","title":"Dates in Plots\u00b6","text":"<p>When working with time-series data, it's often helpful to have dates on the x-axis of your plots for better readability and understanding. However, to display dates effectively in plots using <code>matplotlib</code>, we need to convert our time data into a format that <code>matplotlib</code> can recognize and work with.</p> <p>One common format for this purpose is <code>np.datetime64</code>. This format represents dates and times in a way that is compatible with numpy arrays, making it ideal for plotting time-related data. In our case, we can convert our epoch time (time since a fixed point in the past, typically January 1, 1970) to <code>np.datetime64</code> using the <code>convert.datetime64_from_epoch_array</code> function from the Particula package.</p> <p>Additionally, to make the plot more readable, especially when there are many data points, it's a good practice to rotate the x-axis labels. This prevents overlapping and makes each date and time label clear. We can achieve this rotation using <code>plt.xticks(rotation=45)</code>.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#contour-plot-of-data","title":"Contour Plot of Data\u00b6","text":"<p>Contour plots are a powerful tool for visualizing how data changes over time and space. In the context of size distribution data, a contour plot can effectively show the variation in particle concentration across different sizes over time. It's like looking at a topographic map where different colors or shades represent varying concentrations of particles at different sizes and times.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#preparing-the-data-for-contour-plotting","title":"Preparing the Data for Contour Plotting\u00b6","text":"<p>Before we plot, it's a good practice to set limits on our data to ensure that extreme values don't skew the visualization. This helps in highlighting the relevant ranges of our data. Here's how we do it:</p> <ol> <li><p>Setting Lower and Upper Limits: We impose a lower limit to avoid plotting extremely low concentrations (which might be less relevant or below detection limits) and an upper limit to avoid letting very high concentrations dominate the plot.</p> </li> <li><p>Option for Logarithmic Scale: For data with a wide range of values, using a logarithmic scale (e.g., <code>np.log10(concentration)</code>) can make the plot more informative by compressing the scale and emphasizing the variations across orders of magnitude.</p> </li> </ol>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#creating-the-contour-plot","title":"Creating the Contour Plot\u00b6","text":"<p>With our data prepared, we can now create the contour plot. This type of plot will use different colors or shades to represent the concentration of particles at various sizes and times.</p> <ul> <li>X-Axis (Epoch Time): Represents the time dimension of our data.</li> <li>Y-Axis (Diameter in nm): Represents the different size bins of particles.</li> <li>Color Intensity: Indicates the concentration of particles at each size and time.</li> </ul> <p>Using <code>plt.contourf</code>, we create a filled contour plot with a logarithmic y-scale, which is particularly useful for size distribution data that typically spans several orders of magnitude in particle sizes.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#simplifying-data-import-with-the-settings-generator","title":"Simplifying Data Import with the Settings Generator\u00b6","text":"<p>In the same way we handled 1-dimensional (1d) data, we can also streamline the import process for 2-dimensional (2d) data using the settings generator. This tool is particularly useful for creating a structured approach to loading complex datasets. By using the <code>settings_generator.for_general_sizer_1d_2d_load()</code> function, we can generate a comprehensive settings dictionary that directs how data should be imported and formatted.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#understanding-the-settings-generator-function","title":"Understanding the Settings Generator Function\u00b6","text":"<p>This function is designed to be flexible and accommodate a wide range of data types and formats. It comes with numerous arguments that allow you to specify details like data checks, column information, time format, etc., tailored to your specific dataset. However, it's important to note that you don't have to be overwhelmed by these options.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#using-default-settings","title":"Using Default Settings\u00b6","text":"<p>For many users, especially those just starting out or working with standard data formats, the default settings of the <code>settings_generator</code> function may work. These defaults are configured to align with the example data provided in the Particula package. This means that if your data structure is similar to the example data, you can call this function without passing any arguments, and it will automatically set up the settings for you.</p> <p>This approach not only saves time but also reduces the potential for errors in the data import process, making it a quick and reliable way to get your data ready for analysis.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#efficient-data-loading-with-the-interface","title":"Efficient Data Loading with the Interface\u00b6","text":"<p>After configuring our settings dictionaries for 1-dimensional and 2-dimensional data, we're set to leverage the interface for data loading. This interface, a key component of the Particula package, streamlines the process, making it more efficient and straightforward, especially after you have a good grasp of how the settings work.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#understanding-the-interfaces-role","title":"Understanding the Interface's Role\u00b6","text":"<p>The interface acts as a facilitator that intelligently uses the settings we've established to manage the data loading process. It eliminates the need for manual execution of multiple steps, thereby integrating and automating the data import based on our predefined preferences.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#the-advantages-of-mastery","title":"The Advantages of Mastery\u00b6","text":"<p>Once you're comfortable with setting up your data parameters, using the interface offers several key benefits:</p> <ul> <li>Enhanced Efficiency: It consolidates several operations into a single action, significantly speeding up the data loading process.</li> <li>Consistent Results: By automating the data import process with predefined settings, it ensures uniformity and accuracy across different datasets.</li> <li>Optimized Workflow: For users who understand the settings, the interface offers a simplified and more effective way to handle data loading. It removes the repetitive task of manually calling functions, allowing you to focus more on data analysis.</li> </ul> <p>In the following section, we'll demonstrate how to utilize the interface with our prepared settings to efficiently load and process our data.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#printing-data-stream-summaries","title":"Printing Data Stream Summaries\u00b6","text":"<p>After loading our data using the loader interface, it's a good practice to take a moment and review what we have loaded. This helps us confirm that the data is imported correctly and gives us an initial overview of its structure. We'll do this by printing summaries of the <code>data_stream_1d</code> and <code>data_stream_2d</code> objects.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#understanding-the-data-stream-summary","title":"Understanding the Data Stream Summary\u00b6","text":"<p>When we print a <code>data_stream</code> object, it provides us with a summary of its contents. This includes information like the size of the data, the headers (which represent different data types or measurements), and a glimpse into the actual data values. These summaries are especially useful for:</p> <ul> <li>Verifying Data Integrity: Ensuring that the data has been loaded as expected and is ready for analysis.</li> <li>Quick Overview: Getting a high-level understanding of the data's structure, such as the number of data points and the range of measurements.</li> </ul>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#code-for-printing-summaries","title":"Code for Printing Summaries\u00b6","text":"<p>Here's how we print the summaries for our 1-dimensional and 2-dimensional data streams:</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#plotting-again","title":"Plotting Again\u00b6","text":"<p>Plotting is an integral part of the data analysis process. It transforms raw data into visual representations that can reveal insights, patterns, and anomalies that might not be immediately apparent in numerical form. Let's delve into why plotting is so crucial in understanding your data.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part2/#summary-of-loading-data-part-2","title":"Summary of Loading Data Part 2\u00b6","text":"<p>In this section, we delved into the process of loading and handling 2-dimensional data, focusing on a size distribution dataset. We walked through several crucial steps, providing a comprehensive guide to managing and visualizing complex data structures. Here's a recap of what we covered:</p> <ul> <li>Setting the Working Path: We began by establishing the working directory for our data, a foundational step in ensuring our scripts access the correct files.</li> <li>Loading the Data: Using the loader module, we demonstrated how to import raw data from a file, setting the stage for further processing.</li> <li>Formatting the Data: We then tackled the challenge of formatting 2-dimensional data, extracting size bins as headers, and preparing the dataset for analysis.</li> <li>Initial Plotting: To get a preliminary understanding of our data, we created initial plots. This step is crucial for visually inspecting the data and confirming its integrity.</li> <li>Generating the Settings Dictionary: We utilized the settings_generator to create settings dictionaries for both 1-dimensional and 2-dimensional data. This streamlines the data loading process, especially for complex datasets.</li> <li>Loading Data with the Interface: We showcased how to use the loader interface to efficiently load data using the predefined settings, emphasizing the ease and efficiency it brings to the data loading process.</li> <li>Advanced Plotting of the Data Stream: Lastly, we explored more advanced data visualization techniques, including creating contour plots. This allowed us to visualize how particle concentration varied across different sizes and times, offering valuable insights into our dataset.</li> </ul> <p>Throughout this section, we focused on making each step clear and accessible, particularly for those new to working with 2-dimensional datasets in Python. By following these steps, you can effectively manage and analyze complex data, gaining deeper insights into your research or projects.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part3/","title":"Loading Part 3: Lake","text":"In\u00a0[1]: Copied! <pre># Import necessary modules\nimport os  # Provides functions for interacting with the operating system.\n\n# Matplotlib is a library for creating visualizations and plots.\nimport matplotlib.pyplot as plt\nfrom particula_beta.data import (\n    loader_interface,  # This module allows you to load data from files.\n    # This module provides statistics for a collection of data streams.\n    lake_stats,\n)\nfrom particula_beta.data.loader_setting_builders import (\n    # These functions create settings for loading data from files.\n    DataChecksBuilder,\n    SizerDataReaderBuilder,\n    Loader1DSettingsBuilder,\n    LoaderSizerSettingsBuilder,\n)\nfrom particula_beta.data.tests.example_data.get_example_data import (\n    get_data_folder,\n)\n\n# Lake is a container for multiple data streams.\nfrom particula_beta.data.lake import Lake\n\n# Set the parent directory of the data folder\npath = get_data_folder()\n</pre> # Import necessary modules import os  # Provides functions for interacting with the operating system.  # Matplotlib is a library for creating visualizations and plots. import matplotlib.pyplot as plt from particula_beta.data import (     loader_interface,  # This module allows you to load data from files.     # This module provides statistics for a collection of data streams.     lake_stats, ) from particula_beta.data.loader_setting_builders import (     # These functions create settings for loading data from files.     DataChecksBuilder,     SizerDataReaderBuilder,     Loader1DSettingsBuilder,     LoaderSizerSettingsBuilder, ) from particula_beta.data.tests.example_data.get_example_data import (     get_data_folder, )  # Lake is a container for multiple data streams. from particula_beta.data.lake import Lake  # Set the parent directory of the data folder path = get_data_folder() In\u00a0[2]: Copied! <pre># settings for the CPC data\ndata_checks_cpc = (\n    DataChecksBuilder()\n    .set_characters([10, 100])\n    .set_char_counts({\",\": 4})\n    .set_skip_rows(0)\n    .set_skip_end(0)\n    .build()\n)\n\ncpc_settings = (\n    Loader1DSettingsBuilder()\n    .set_relative_data_folder(\"CPC_3010_data\")\n    .set_filename_regex(\"*.csv\")\n    .set_header_row(0)\n    .set_data_checks(data_checks_cpc)  # from above\n    .set_data_column([1, 2])\n    .set_data_header([\"CPC_count[#/sec]\", \"Temperature[degC]\"])\n    .set_time_column([0])\n    .set_time_format(\"epoch\")\n    .set_delimiter(\",\")\n    .set_timezone_identifier(\"UTC\")\n    .build()\n)\n\n# settings for the SMPS data\ndata_checks_sizer = (\n    DataChecksBuilder()\n    .set_characters([250])\n    .set_skip_rows(25)\n    .set_char_counts({\"/\": 2, \":\": 2})\n    .build()\n)\ndata_sizer_reader = (\n    SizerDataReaderBuilder()\n    .set_sizer_start_keyword(\"20.72\")\n    .set_sizer_end_keyword(\"784.39\")\n    .set_sizer_concentration_convert_from(\"dw/dlogdp\")\n    .build()\n)\nsmps_1d_settings, smps_2d_settings = (\n    LoaderSizerSettingsBuilder()\n    .set_relative_data_folder(\"SMPS_data\")\n    .set_filename_regex(\"*.csv\")\n    .set_header_row(24)\n    .set_data_checks(data_checks_sizer)\n    .set_data_column(\n        [\n            \"Lower Size (nm)\",\n            \"Upper Size (nm)\",\n            \"Sample Temp (C)\",\n            \"Sample Pressure (kPa)\",\n            \"Relative Humidity (%)\",\n            \"Median (nm)\",\n            \"Mean (nm)\",\n            \"Geo. Mean (nm)\",\n            \"Mode (nm)\",\n            \"Geo. Std. Dev.\",\n            \"Total Conc. (#/cm\u00b3)\",\n        ]\n    )\n    .set_data_header(\n        [\n            \"Lower_Size_(nm)\",\n            \"Upper_Size_(nm)\",\n            \"Sample_Temp_(C)\",\n            \"Sample_Pressure_(kPa)\",\n            \"Relative_Humidity_(%)\",\n            \"Median_(nm)\",\n            \"Mean_(nm)\",\n            \"Geo_Mean_(nm)\",\n            \"Mode_(nm)\",\n            \"Geo_Std_Dev.\",\n            \"Total_Conc_(#/cc)\",\n        ]\n    )\n    .set_data_sizer_reader(data_sizer_reader)\n    .set_time_column([1, 2])\n    .set_time_format(\"%m/%d/%Y %H:%M:%S\")\n    .set_delimiter(\",\")\n    .set_timezone_identifier(\"UTC\")\n    .build()\n)\n\n# collect settings into a dictionary\ncombined_settings = {\n    \"cpc\": cpc_settings,\n    \"smps_1d\": smps_1d_settings,\n    \"smps_2d\": smps_2d_settings,\n}\n\n# now call the loader interface for files\nlake = loader_interface.load_folders_interface(\n    path=path,\n    folder_settings=combined_settings,\n)\n\nprint(\" \")\nprint(lake)\n</pre> # settings for the CPC data data_checks_cpc = (     DataChecksBuilder()     .set_characters([10, 100])     .set_char_counts({\",\": 4})     .set_skip_rows(0)     .set_skip_end(0)     .build() )  cpc_settings = (     Loader1DSettingsBuilder()     .set_relative_data_folder(\"CPC_3010_data\")     .set_filename_regex(\"*.csv\")     .set_header_row(0)     .set_data_checks(data_checks_cpc)  # from above     .set_data_column([1, 2])     .set_data_header([\"CPC_count[#/sec]\", \"Temperature[degC]\"])     .set_time_column([0])     .set_time_format(\"epoch\")     .set_delimiter(\",\")     .set_timezone_identifier(\"UTC\")     .build() )  # settings for the SMPS data data_checks_sizer = (     DataChecksBuilder()     .set_characters([250])     .set_skip_rows(25)     .set_char_counts({\"/\": 2, \":\": 2})     .build() ) data_sizer_reader = (     SizerDataReaderBuilder()     .set_sizer_start_keyword(\"20.72\")     .set_sizer_end_keyword(\"784.39\")     .set_sizer_concentration_convert_from(\"dw/dlogdp\")     .build() ) smps_1d_settings, smps_2d_settings = (     LoaderSizerSettingsBuilder()     .set_relative_data_folder(\"SMPS_data\")     .set_filename_regex(\"*.csv\")     .set_header_row(24)     .set_data_checks(data_checks_sizer)     .set_data_column(         [             \"Lower Size (nm)\",             \"Upper Size (nm)\",             \"Sample Temp (C)\",             \"Sample Pressure (kPa)\",             \"Relative Humidity (%)\",             \"Median (nm)\",             \"Mean (nm)\",             \"Geo. Mean (nm)\",             \"Mode (nm)\",             \"Geo. Std. Dev.\",             \"Total Conc. (#/cm\u00b3)\",         ]     )     .set_data_header(         [             \"Lower_Size_(nm)\",             \"Upper_Size_(nm)\",             \"Sample_Temp_(C)\",             \"Sample_Pressure_(kPa)\",             \"Relative_Humidity_(%)\",             \"Median_(nm)\",             \"Mean_(nm)\",             \"Geo_Mean_(nm)\",             \"Mode_(nm)\",             \"Geo_Std_Dev.\",             \"Total_Conc_(#/cc)\",         ]     )     .set_data_sizer_reader(data_sizer_reader)     .set_time_column([1, 2])     .set_time_format(\"%m/%d/%Y %H:%M:%S\")     .set_delimiter(\",\")     .set_timezone_identifier(\"UTC\")     .build() )  # collect settings into a dictionary combined_settings = {     \"cpc\": cpc_settings,     \"smps_1d\": smps_1d_settings,     \"smps_2d\": smps_2d_settings, }  # now call the loader interface for files lake = loader_interface.load_folders_interface(     path=path,     folder_settings=combined_settings, )  print(\" \") print(lake) <pre>Folder Settings: cpc\n  Loading file: CPC_3010_data_20220709_Jul.csv\n  Loading file: CPC_3010_data_20220710_Jul.csv\nFolder Settings: smps_1d\n  Loading file: 2022-07-07_095151_SMPS.csv\n  Loading file: 2022-07-10_094659_SMPS.csv\nFolder Settings: smps_2d\n  Loading file: 2022-07-07_095151_SMPS.csv\n  Loading file: 2022-07-10_094659_SMPS.csv\n \nLake with streams: ['cpc', 'smps_1d', 'smps_2d']\n</pre> In\u00a0[3]: Copied! <pre># get the names of the streams\nprint(\" \")\nprint(\"Names of the streams:\")\nprint(dir(lake))\n</pre> # get the names of the streams print(\" \") print(\"Names of the streams:\") print(dir(lake)) <pre> \nNames of the streams:\n['cpc', 'smps_1d', 'smps_2d']\n</pre> In\u00a0[4]: Copied! <pre># get the streams\nprint(\" \")\nprint(\"The streams:\")\nfor stream in lake:\n    print(stream)\n</pre> # get the streams print(\" \") print(\"The streams:\") for stream in lake:     print(stream) <pre> \nThe streams:\n('cpc', Stream(header=['CPC_count[#/sec]', 'Temperature[degC]'], data=array([[3.3510e+04, 1.7000e+01],\n       [3.3465e+04, 1.7100e+01],\n       [3.2171e+04, 1.7000e+01],\n       ...,\n       [1.9403e+04, 1.6900e+01],\n       [2.0230e+04, 1.7000e+01],\n       [1.9521e+04, 1.6800e+01]]), time=array([1.65734280e+09, 1.65734281e+09, 1.65734281e+09, ...,\n       1.65751559e+09, 1.65751560e+09, 1.65751560e+09]), files=[['CPC_3010_data_20220709_Jul.csv', 1044534], ['CPC_3010_data_20220710_Jul.csv', 1113488]]))\n('smps_1d', Stream(header=['Lower_Size_(nm)', 'Upper_Size_(nm)', 'Sample_Temp_(C)', 'Sample_Pressure_(kPa)', 'Relative_Humidity_(%)', 'Median_(nm)', 'Mean_(nm)', 'Geo_Mean_(nm)', 'Mode_(nm)', 'Geo_Std_Dev.', 'Total_Conc_(#/cc)'], data=array([[2.05000e+01, 7.91500e+02, 2.37000e+01, ..., 2.07210e+01,\n        2.17900e+00, 2.16900e+03],\n       [2.05000e+01, 7.91500e+02, 2.36000e+01, ..., 2.52550e+01,\n        2.10100e+00, 2.39408e+03],\n       [2.05000e+01, 7.91500e+02, 2.37000e+01, ..., 2.18700e+01,\n        2.13600e+00, 2.27861e+03],\n       ...,\n       [2.05000e+01, 7.91500e+02, 2.35000e+01, ..., 2.07210e+01,\n        2.31800e+00, 2.08056e+03],\n       [2.05000e+01, 7.91500e+02, 2.33000e+01, ..., 2.10970e+01,\n        2.31800e+00, 2.10616e+03],\n       [2.05000e+01, 7.91500e+02, 2.35000e+01, ..., 2.07210e+01,\n        2.24800e+00, 2.45781e+03]]), time=array([1.65718376e+09, 1.65718385e+09, 1.65718394e+09, ...,\n       1.65753440e+09, 1.65753450e+09, 1.65753459e+09]), files=[['2022-07-07_095151_SMPS.csv', 5620804], ['2022-07-10_094659_SMPS.csv', 2004838]]))\n('smps_2d', Stream(header=['20.72', '21.10', '21.48', '21.87', '22.27', '22.67', '23.08', '23.50', '23.93', '24.36', '24.80', '25.25', '25.71', '26.18', '26.66', '27.14', '27.63', '28.13', '28.64', '29.16', '29.69', '30.23', '30.78', '31.34', '31.91', '32.49', '33.08', '33.68', '34.29', '34.91', '35.55', '36.19', '36.85', '37.52', '38.20', '38.89', '39.60', '40.32', '41.05', '41.79', '42.55', '43.32', '44.11', '44.91', '45.73', '46.56', '47.40', '48.26', '49.14', '50.03', '50.94', '51.86', '52.80', '53.76', '54.74', '55.73', '56.74', '57.77', '58.82', '59.89', '60.98', '62.08', '63.21', '64.36', '65.52', '66.71', '67.93', '69.16', '70.41', '71.69', '72.99', '74.32', '75.67', '77.04', '78.44', '79.86', '81.31', '82.79', '84.29', '85.82', '87.38', '88.96', '90.58', '92.22', '93.90', '95.60', '97.34', '99.10', '100.90', '102.74', '104.60', '106.50', '108.43', '110.40', '112.40', '114.44', '116.52', '118.64', '120.79', '122.98', '125.21', '127.49', '129.80', '132.16', '134.56', '137.00', '139.49', '142.02', '144.60', '147.22', '149.89', '152.61', '155.38', '158.20', '161.08', '164.00', '166.98', '170.01', '173.09', '176.24', '179.43', '182.69', '186.01', '189.38', '192.82', '196.32', '199.89', '203.51', '207.21', '210.97', '214.80', '218.70', '222.67', '226.71', '230.82', '235.01', '239.28', '243.62', '248.05', '252.55', '257.13', '261.80', '266.55', '271.39', '276.32', '281.33', '286.44', '291.64', '296.93', '302.32', '307.81', '313.40', '319.08', '324.88', '330.77', '336.78', '342.89', '349.12', '355.45', '361.90', '368.47', '375.16', '381.97', '388.91', '395.96', '403.15', '410.47', '417.92', '425.51', '433.23', '441.09', '449.10', '457.25', '465.55', '474.00', '482.61', '491.37', '500.29', '509.37', '518.61', '528.03', '537.61', '547.37', '557.31', '567.42', '577.72', '588.21', '598.89', '609.76', '620.82', '632.09', '643.57', '655.25', '667.14', '679.25', '691.58', '704.14', '716.92', '729.93', '743.18', '756.67', '770.40', '784.39'], data=array([[ 6103.186,  2832.655,  4733.553, ...,    93.413,   122.992,\n            0.   ],\n       [ 5621.118,  5867.747,  6233.403, ...,     0.   ,     0.   ,\n           75.377],\n       [ 5165.139,  4969.987,  4312.386, ...,     0.   ,   122.992,\n          124.085],\n       ...,\n       [ 9962.036,  7986.823,  8682.258, ...,     0.   ,     0.   ,\n          124.153],\n       [ 8765.782, 11175.603,  8148.945, ...,     0.   ,     0.   ,\n          372.433],\n       [14380.528, 11524.35 , 13632.727, ...,     0.   ,     0.   ,\n            0.   ]]), time=array([1.65718376e+09, 1.65718385e+09, 1.65718394e+09, ...,\n       1.65753440e+09, 1.65753450e+09, 1.65753459e+09]), files=[['2022-07-07_095151_SMPS.csv', 5620804], ['2022-07-10_094659_SMPS.csv', 2004838]]))\n</pre> In\u00a0[5]: Copied! <pre># get just the keys\nprint(\" \")\nprint(\"The keys:\")\nfor key in lake.keys():\n    print(key)\n</pre> # get just the keys print(\" \") print(\"The keys:\") for key in lake.keys():     print(key) <pre> \nThe keys:\ncpc\nsmps_1d\nsmps_2d\n</pre> In\u00a0[6]: Copied! <pre># get just the values\nprint(\" \")\nprint(\"The values:\")\nfor value in lake.values():\n    print(value)\n</pre> # get just the values print(\" \") print(\"The values:\") for value in lake.values():     print(value) <pre> \nThe values:\nStream(header=['CPC_count[#/sec]', 'Temperature[degC]'], data=array([[3.3510e+04, 1.7000e+01],\n       [3.3465e+04, 1.7100e+01],\n       [3.2171e+04, 1.7000e+01],\n       ...,\n       [1.9403e+04, 1.6900e+01],\n       [2.0230e+04, 1.7000e+01],\n       [1.9521e+04, 1.6800e+01]]), time=array([1.65734280e+09, 1.65734281e+09, 1.65734281e+09, ...,\n       1.65751559e+09, 1.65751560e+09, 1.65751560e+09]), files=[['CPC_3010_data_20220709_Jul.csv', 1044534], ['CPC_3010_data_20220710_Jul.csv', 1113488]])\nStream(header=['Lower_Size_(nm)', 'Upper_Size_(nm)', 'Sample_Temp_(C)', 'Sample_Pressure_(kPa)', 'Relative_Humidity_(%)', 'Median_(nm)', 'Mean_(nm)', 'Geo_Mean_(nm)', 'Mode_(nm)', 'Geo_Std_Dev.', 'Total_Conc_(#/cc)'], data=array([[2.05000e+01, 7.91500e+02, 2.37000e+01, ..., 2.07210e+01,\n        2.17900e+00, 2.16900e+03],\n       [2.05000e+01, 7.91500e+02, 2.36000e+01, ..., 2.52550e+01,\n        2.10100e+00, 2.39408e+03],\n       [2.05000e+01, 7.91500e+02, 2.37000e+01, ..., 2.18700e+01,\n        2.13600e+00, 2.27861e+03],\n       ...,\n       [2.05000e+01, 7.91500e+02, 2.35000e+01, ..., 2.07210e+01,\n        2.31800e+00, 2.08056e+03],\n       [2.05000e+01, 7.91500e+02, 2.33000e+01, ..., 2.10970e+01,\n        2.31800e+00, 2.10616e+03],\n       [2.05000e+01, 7.91500e+02, 2.35000e+01, ..., 2.07210e+01,\n        2.24800e+00, 2.45781e+03]]), time=array([1.65718376e+09, 1.65718385e+09, 1.65718394e+09, ...,\n       1.65753440e+09, 1.65753450e+09, 1.65753459e+09]), files=[['2022-07-07_095151_SMPS.csv', 5620804], ['2022-07-10_094659_SMPS.csv', 2004838]])\nStream(header=['20.72', '21.10', '21.48', '21.87', '22.27', '22.67', '23.08', '23.50', '23.93', '24.36', '24.80', '25.25', '25.71', '26.18', '26.66', '27.14', '27.63', '28.13', '28.64', '29.16', '29.69', '30.23', '30.78', '31.34', '31.91', '32.49', '33.08', '33.68', '34.29', '34.91', '35.55', '36.19', '36.85', '37.52', '38.20', '38.89', '39.60', '40.32', '41.05', '41.79', '42.55', '43.32', '44.11', '44.91', '45.73', '46.56', '47.40', '48.26', '49.14', '50.03', '50.94', '51.86', '52.80', '53.76', '54.74', '55.73', '56.74', '57.77', '58.82', '59.89', '60.98', '62.08', '63.21', '64.36', '65.52', '66.71', '67.93', '69.16', '70.41', '71.69', '72.99', '74.32', '75.67', '77.04', '78.44', '79.86', '81.31', '82.79', '84.29', '85.82', '87.38', '88.96', '90.58', '92.22', '93.90', '95.60', '97.34', '99.10', '100.90', '102.74', '104.60', '106.50', '108.43', '110.40', '112.40', '114.44', '116.52', '118.64', '120.79', '122.98', '125.21', '127.49', '129.80', '132.16', '134.56', '137.00', '139.49', '142.02', '144.60', '147.22', '149.89', '152.61', '155.38', '158.20', '161.08', '164.00', '166.98', '170.01', '173.09', '176.24', '179.43', '182.69', '186.01', '189.38', '192.82', '196.32', '199.89', '203.51', '207.21', '210.97', '214.80', '218.70', '222.67', '226.71', '230.82', '235.01', '239.28', '243.62', '248.05', '252.55', '257.13', '261.80', '266.55', '271.39', '276.32', '281.33', '286.44', '291.64', '296.93', '302.32', '307.81', '313.40', '319.08', '324.88', '330.77', '336.78', '342.89', '349.12', '355.45', '361.90', '368.47', '375.16', '381.97', '388.91', '395.96', '403.15', '410.47', '417.92', '425.51', '433.23', '441.09', '449.10', '457.25', '465.55', '474.00', '482.61', '491.37', '500.29', '509.37', '518.61', '528.03', '537.61', '547.37', '557.31', '567.42', '577.72', '588.21', '598.89', '609.76', '620.82', '632.09', '643.57', '655.25', '667.14', '679.25', '691.58', '704.14', '716.92', '729.93', '743.18', '756.67', '770.40', '784.39'], data=array([[ 6103.186,  2832.655,  4733.553, ...,    93.413,   122.992,\n            0.   ],\n       [ 5621.118,  5867.747,  6233.403, ...,     0.   ,     0.   ,\n           75.377],\n       [ 5165.139,  4969.987,  4312.386, ...,     0.   ,   122.992,\n          124.085],\n       ...,\n       [ 9962.036,  7986.823,  8682.258, ...,     0.   ,     0.   ,\n          124.153],\n       [ 8765.782, 11175.603,  8148.945, ...,     0.   ,     0.   ,\n          372.433],\n       [14380.528, 11524.35 , 13632.727, ...,     0.   ,     0.   ,\n            0.   ]]), time=array([1.65718376e+09, 1.65718385e+09, 1.65718394e+09, ...,\n       1.65753440e+09, 1.65753450e+09, 1.65753459e+09]), files=[['2022-07-07_095151_SMPS.csv', 5620804], ['2022-07-10_094659_SMPS.csv', 2004838]])\n</pre> In\u00a0[7]: Copied! <pre># Load and Plot Data from Lake\n\n# Access CPC data from the Lake\ncpc_time = lake[\"cpc\"].datetime64\ncpc_data = lake[\"cpc\"][\"CPC_count[#/sec]\"]\n\n# Access SMPS data from the Lake\nsmps_time = lake[\"smps_1d\"].datetime64\nsmps_data = lake[\"smps_1d\"][\"Mode_(nm)\"]\n\n# Plot the Data on Twinx Axis\nfig, ax = plt.subplots()\n\n# Plot CPC data\nax.plot(cpc_time, cpc_data, label=\"CPC\", color=\"blue\")\n\n# Rotate x-axis labels for better readability\nplt.xticks(rotation=45)\n\n# Create a twin y-axis for SMPS data\naxb = ax.twinx()\n\n# Plot SMPS data\naxb.plot(smps_time, smps_data, label=\"SMPS\", color=\"orange\")\n\n# Set y-axis limits for SMPS data\naxb.set_ylim(0, 200)\n\n# Set axis labels\nax.set_xlabel(\"Time (UTC)\")\nax.set_ylabel(\"CPC Counts [#/sec]\")\naxb.set_ylabel(\"SMPS Mode [nm]\")\n\n# Display the legend and show the plot\nplt.show()\n\n# Adjust layout for better visualization\nfig.tight_layout()\n</pre> # Load and Plot Data from Lake  # Access CPC data from the Lake cpc_time = lake[\"cpc\"].datetime64 cpc_data = lake[\"cpc\"][\"CPC_count[#/sec]\"]  # Access SMPS data from the Lake smps_time = lake[\"smps_1d\"].datetime64 smps_data = lake[\"smps_1d\"][\"Mode_(nm)\"]  # Plot the Data on Twinx Axis fig, ax = plt.subplots()  # Plot CPC data ax.plot(cpc_time, cpc_data, label=\"CPC\", color=\"blue\")  # Rotate x-axis labels for better readability plt.xticks(rotation=45)  # Create a twin y-axis for SMPS data axb = ax.twinx()  # Plot SMPS data axb.plot(smps_time, smps_data, label=\"SMPS\", color=\"orange\")  # Set y-axis limits for SMPS data axb.set_ylim(0, 200)  # Set axis labels ax.set_xlabel(\"Time (UTC)\") ax.set_ylabel(\"CPC Counts [#/sec]\") axb.set_ylabel(\"SMPS Mode [nm]\")  # Display the legend and show the plot plt.show()  # Adjust layout for better visualization fig.tight_layout() In\u00a0[8]: Copied! <pre># Compute the average and standard deviation of data within a time interval\n# of 600 seconds for each stream in the lake, and create a new lake\n# containing the averaged data.\nlake_averaged = lake_stats.average_std(\n    lake=lake,\n    average_interval=600,\n    clone=True,  # Create a new lake instead of modifying the original\n)\n\n# Print the resulting lake with averaged data and standard deviation.\nprint(lake_averaged)\n</pre> # Compute the average and standard deviation of data within a time interval # of 600 seconds for each stream in the lake, and create a new lake # containing the averaged data. lake_averaged = lake_stats.average_std(     lake=lake,     average_interval=600,     clone=True,  # Create a new lake instead of modifying the original )  # Print the resulting lake with averaged data and standard deviation. print(lake_averaged) <pre>Lake with streams: ['cpc', 'smps_1d', 'smps_2d']\n</pre> In\u00a0[9]: Copied! <pre># Extract datetime and CPC data from the averaged lake\ncpc_time = lake_averaged[\"cpc\"].datetime64\ncpc_data = lake_averaged[\"cpc\"][\"CPC_count[#/sec]\"]\n\n# Extract datetime and SMPS data from the averaged lake\nsmps_time = lake_averaged[\"smps_1d\"].datetime64\nsmps_data = lake_averaged[\"smps_1d\"][\"Mode_(nm)\"]\n\n# Create a plot with two y-axes (twinx) for CPC and SMPS data\nfig, ax = plt.subplots()\nax.plot(cpc_time, cpc_data, label=\"CPC\", color=\"blue\")\nplt.xticks(rotation=45)\n\n# Create a twinx axis for SMPS data\naxb = ax.twinx()\naxb.plot(\n    smps_time,\n    smps_data,\n    label=\"SMPS\",\n    color=\"orange\",\n)\naxb.set_ylim(0, 200)\n\n# Set labels for both y-axes and the x-axis\nax.set_xlabel(\"Time (UTC)\")\nax.set_ylabel(\"CPC_counts[#/sec]\")\naxb.set_ylabel(\"SMPS_Mode[nm]\")\n\n# Show the plot and adjust layout for better presentation\nplt.show()\nfig.tight_layout()\n</pre> # Extract datetime and CPC data from the averaged lake cpc_time = lake_averaged[\"cpc\"].datetime64 cpc_data = lake_averaged[\"cpc\"][\"CPC_count[#/sec]\"]  # Extract datetime and SMPS data from the averaged lake smps_time = lake_averaged[\"smps_1d\"].datetime64 smps_data = lake_averaged[\"smps_1d\"][\"Mode_(nm)\"]  # Create a plot with two y-axes (twinx) for CPC and SMPS data fig, ax = plt.subplots() ax.plot(cpc_time, cpc_data, label=\"CPC\", color=\"blue\") plt.xticks(rotation=45)  # Create a twinx axis for SMPS data axb = ax.twinx() axb.plot(     smps_time,     smps_data,     label=\"SMPS\",     color=\"orange\", ) axb.set_ylim(0, 200)  # Set labels for both y-axes and the x-axis ax.set_xlabel(\"Time (UTC)\") ax.set_ylabel(\"CPC_counts[#/sec]\") axb.set_ylabel(\"SMPS_Mode[nm]\")  # Show the plot and adjust layout for better presentation plt.show() fig.tight_layout() In\u00a0[10]: Copied! <pre>help(Lake)\n</pre> help(Lake) <pre>Help on class Lake in module particula_beta.data.lake:\n\nclass Lake(builtins.object)\n |  Lake(streams: Dict[str, particula_beta.data.stream.Stream] = &lt;factory&gt;) -&gt; None\n |  \n |  A class representing a lake which is a collection of streams.\n |  \n |  Attributes:\n |      streams (Dict[str, Stream]): A dictionary to hold streams with their\n |      names as keys.\n |  \n |  Methods defined here:\n |  \n |  __delitem__(self, key: str) -&gt; None\n |      Remove a stream by name.\n |      Example: del lake['stream_name']\n |  \n |  __dir__(self) -&gt; list\n |      List available streams.\n |      Example: dir(lake)\n |  \n |  __eq__(self, other)\n |      Return self==value.\n |  \n |  __getattr__(self, name: str) -&gt; Any\n |      Allow accessing streams as an attributes.\n |      Raises:\n |          AttributeError: If the stream name is not in the lake.\n |      Example: lake.stream_name\n |  \n |  __getitem__(self, key: str) -&gt; Any\n |      Get a stream by name.\n |      Example: lake['stream_name']\n |  \n |  __init__(self, streams: Dict[str, particula_beta.data.stream.Stream] = &lt;factory&gt;) -&gt; None\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __iter__(self) -&gt; Iterator[Any]\n |      Iterate over the streams in the lake.\n |      Example: [stream.header for stream in lake]\"\"\n |  \n |  __len__(self) -&gt; int\n |      Return the number of streams in the lake.\n |      Example: len(lake)\n |  \n |  __repr__(self) -&gt; str\n |      Return a string representation of the lake.\n |      Example: print(lake)\n |  \n |  __setitem__(self, key: str, value: particula_beta.data.stream.Stream) -&gt; None\n |      Set a stream by name.\n |      Example: lake['stream_name'] = new_stream\n |  \n |  add_stream(self, stream: particula_beta.data.stream.Stream, name: str) -&gt; None\n |      Add a stream to the lake.\n |      \n |      Args:\n |          stream (Stream): The stream object to be added.\n |          name (str): The name of the stream.\n |      \n |      Raises:\n |      -------\n |          ValueError: If the stream name is already in use or not a valid\n |          identifier.\n |  \n |  items(self) -&gt; Iterator[Tuple[Any, Any]]\n |      Return an iterator over the key-value pairs.\n |  \n |  keys(self) -&gt; Iterator[Any]\n |      Return an iterator over the keys.\n |  \n |  values(self) -&gt; Iterator[Any]\n |      Return an iterator over the values.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  summary\n |      Return a string summary iterating over each stream\n |          and print Stream.header.\n |      Example: lake.summary\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables\n |  \n |  __weakref__\n |      list of weak references to the object\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __annotations__ = {'streams': typing.Dict[str, particula_beta.data.str...\n |  \n |  __dataclass_fields__ = {'streams': Field(name='streams',type=typing.Di...\n |  \n |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n |  \n |  __hash__ = None\n |  \n |  __match_args__ = ('streams',)\n\n</pre>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part3/#loading-part-3-lake","title":"Loading Part 3: Lake\u00b6","text":"<p>In this example, we explore the process of working with data from multiple instruments and consolidating them into a single <code>Lake</code> object. A <code>Lake</code> object serves as a convenient container for aggregating multiple <code>Stream</code>s, each representing data from individual instruments.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part3/#setting-the-working-path","title":"Setting the Working Path\u00b6","text":"<p>To begin, you need to establish the working path where your data is stored. For this demonstration, we will use provided example data located in the current directory. However, keep in mind that the path can be anywhere on your computer. For instance, if you have a folder named \"data\" in your home directory, you can set the path as follows:</p> <pre>path = \"U:\\\\data\\\\processing\\\\Campaign2023_of_awesome\\\\data\"\n</pre> <p>Your folder structure should resemble the following:</p> <pre><code>data\n\u251c\u2500\u2500 CPC_3010_data\n\u2502   \u251c\u2500\u2500 CPC_3010_data_20220709_Jul.csv\n\u2502   \u251c\u2500\u2500 CPC_3010_data_20220709_Jul.csv\n\u251c\u2500\u2500 SMPS_data\n\u2502   \u251c\u2500\u2500 2022-07-07_095151_SMPS.csv\n\u2502   \u251c\u2500\u2500 2022-07-10_094659_SMPS.csv\n</code></pre> <p>Here, the path points to the \"data\" folder. Within this folder, you'll find two subfolders: one for CPC data and another for SMPS data. These subfolders correspond to the relative_data_folder keywords used in the settings dictionary. The data within these subfolders will be loaded as Stream objects.</p> <p>Inside each of these subfolders, you'll find data files that match the specified filename_regex. A regular expression is used to select files based on specific criteria. In this case, we are matching all files ending with \".csv\" and loading them into the respective Stream objects. This approach allows you to efficiently manage and consolidate data from various instruments for further analysis and visualization.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part3/#load-the-data","title":"Load the Data\u00b6","text":"<p>In this example, we'll work with provided example data. However, you have the flexibility to change the path to any folder on your computer. We will use the settings generator to efficiently load the data.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part3/#lake-class-overview","title":"Lake Class Overview\u00b6","text":"<p>The <code>Lake</code> is a collection of <code>Stream</code> objects stored as a dictionary. The keys represent the names of the streams, and the values are the stream objects themselves. It provides a convenient way to organize and manage multiple datasets. Let's explore its key attributes and methods:</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part3/#attributes","title":"Attributes:\u00b6","text":"<ul> <li><code>streams</code> (Dict[str, Stream]): A dictionary where keys are stream names, and values are the corresponding <code>Stream</code> objects.</li> </ul>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part3/#methods","title":"Methods:\u00b6","text":"<ul> <li><p><code>__getitem__(self, key: str) -&gt; Any</code>: Retrieve a specific <code>Stream</code> by its name.</p> <ul> <li>Example: To access the CPC stream, you can use <code>lake['cpc']</code>.</li> </ul> </li> <li><p><code>__delitem__(self, key: str) -&gt; None</code>: Remove a <code>Stream</code> from the <code>Lake</code> using its name.</p> <ul> <li>Example: To remove a stream named 'cpc', you can use <code>del lake['cpc']</code>.</li> </ul> </li> <li><p><code>__getattr__(self, name: str) -&gt; Any</code>: Access streams as attributes for easier navigation.</p> <ul> <li>Example: You can directly access the 'cpc' stream with <code>lake.cpc</code>.</li> </ul> </li> <li><p><code>add_stream(self, stream: particula_beta.data.stream.Stream, name: str) -&gt; None</code>: Add a new <code>Stream</code> to the <code>Lake</code>.</p> <ul> <li>Example: To add a new stream, you can use <code>lake.add_stream(new_stream, 'stream_name')</code>.</li> </ul> </li> <li><p><code>__len__(self) -&gt; int</code>: Determine the number of streams in the <code>Lake</code>.</p> <ul> <li>Example: To find out how many streams are in the <code>Lake</code>, use <code>len(lake)</code>.</li> </ul> </li> <li><p><code>__iter__(self) -&gt; Iterator[Any]</code>: Iterate over the streams in the <code>Lake</code>.</p> <ul> <li>Example: To loop through all streams, you can use <code>[stream.header for stream in lake]</code>.</li> </ul> </li> <li><p><code>summary</code> (Readonly property): Generate a summary by iterating through each stream and printing their headers.</p> <ul> <li>Example: To get a summary of all streams, use <code>lake.summary</code>.</li> </ul> </li> </ul>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part3/#usage","title":"Usage:\u00b6","text":"<p>The <code>Lake</code> class simplifies the management of multiple datasets. You can access individual streams by name, add new streams, and iterate through them efficiently. This class is particularly helpful when dealing with various data sources within your analysis.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part3/#pause-to-plot-the-data","title":"Pause to Plot the data\u00b6","text":"<p>In this code snippet, we retrieve data from the Lake object and create a dual-axis plot to visualize both CPC and SMPS data over time.</p> <ul> <li>We access the CPC data from the Lake using lake['cpc']. We retrieve the datetime and CPC count data.</li> <li>Similarly, we access the SMPS data from the Lake using lake['smps_1d'] and retrieve the datetime and Mode data.</li> <li>We create a plot with a blue line for CPC data using ax.plot(), and an orange line for SMPS data on a twin y-axis axb.</li> <li>To improve readability, we rotate the x-axis labels using plt.xticks(rotation=45).</li> <li>We set y-axis limits for the SMPS data to be in the range [0, 200] using axb.set_ylim(0, 200).</li> <li>Axis labels and legends are added for both datasets.</li> <li>Finally, we display the plot and adjust the layout for better visualization using plt.show() and fig.tight_layout().</li> </ul>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part3/#data-averaging","title":"Data Averaging\u00b6","text":"<p>Now that we have loaded the data, we can perform data averaging over time. To achieve this, we will utilize the 'particula_beta.data.lake_stats' module, which provides a convenient function called 'averaged_std.' This function takes a stream object as input and returns a new stream object containing both the averaged data and the standard deviation of the data.</p> <p>It's worth noting that this function follows a similar naming convention to 'stream_stats.average_std,' which operates on individual stream objects.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part3/#plot-the-averaged-data","title":"Plot the Averaged Data\u00b6","text":"<p>Let's plot the averaged data to see how it compares to the raw data. We will use the same approach as before, but this time we will use the averaged data from the Lake object.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part3/#summary","title":"Summary\u00b6","text":"<p>In this part of the tutorial, we learned how to work with multiple streams of data and load them into a <code>Lake</code> object, which is a collection of streams. We explored operations on the data, including averaging it over time using the <code>particula_beta.data.lake_stats</code> module. This allowed us to create more meaningful visualizations by comparing the averaged and non-averaged data. The example demonstrated the power of the <code>particula_beta.data</code> package in handling and analyzing scientific data efficiently.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part4/","title":"Loading Part 4: Settings Files","text":"In\u00a0[1]: Copied! <pre># Import the necessary libraries and modules\nimport matplotlib.pyplot as plt\nfrom particula_beta.data import loader_interface, settings_generator\nfrom particula_beta.data.tests.example_data.get_example_data import (\n    get_data_folder,\n)\nfrom particula_beta.data.lake import Lake\nfrom particula_beta.data.loader_setting_builders import (\n    # These functions create settings for loading data from files.\n    DataChecksBuilder,\n    SizerDataReaderBuilder,\n    Loader1DSettingsBuilder,\n    LoaderSizerSettingsBuilder,\n)\n\n# Set the parent directory where the data folders are located\npath = get_data_folder()\nprint(\"Path to data folder:\")\nprint(path.rsplit(\"particula\")[-1])\n</pre> # Import the necessary libraries and modules import matplotlib.pyplot as plt from particula_beta.data import loader_interface, settings_generator from particula_beta.data.tests.example_data.get_example_data import (     get_data_folder, ) from particula_beta.data.lake import Lake from particula_beta.data.loader_setting_builders import (     # These functions create settings for loading data from files.     DataChecksBuilder,     SizerDataReaderBuilder,     Loader1DSettingsBuilder,     LoaderSizerSettingsBuilder, )  # Set the parent directory where the data folders are located path = get_data_folder() print(\"Path to data folder:\") print(path.rsplit(\"particula\")[-1]) <pre>Path to data folder:\n_beta\\data\\tests\\example_data\n</pre> In\u00a0[2]: Copied! <pre># settings for the CPC data\ndata_checks_cpc = (\n    DataChecksBuilder()\n    .set_characters([10, 100])\n    .set_char_counts({\",\": 4})\n    .build()\n)\n\ncpc_settings = (\n    Loader1DSettingsBuilder()\n    .set_relative_data_folder(\"CPC_3010_data\")\n    .set_filename_regex(\"*.csv\")\n    .set_header_row(0)\n    .set_data_checks(data_checks_cpc)  # from above\n    .set_data_column([1, 2])\n    .set_data_header([\"CPC_count[#/sec]\", \"Temperature[degC]\"])\n    .set_time_column([0])\n    .set_time_format(\"epoch\")\n    .set_delimiter(\",\")\n    .set_timezone_identifier(\"UTC\")\n    .build()\n)\n\n# save the settings to a file\nsettings_generator.save_settings_for_stream(\n    settings=cpc_settings,\n    path=path,\n    subfolder=\"CPC_3010_data\",\n    settings_suffix=\"_cpc\",\n)\n</pre> # settings for the CPC data data_checks_cpc = (     DataChecksBuilder()     .set_characters([10, 100])     .set_char_counts({\",\": 4})     .build() )  cpc_settings = (     Loader1DSettingsBuilder()     .set_relative_data_folder(\"CPC_3010_data\")     .set_filename_regex(\"*.csv\")     .set_header_row(0)     .set_data_checks(data_checks_cpc)  # from above     .set_data_column([1, 2])     .set_data_header([\"CPC_count[#/sec]\", \"Temperature[degC]\"])     .set_time_column([0])     .set_time_format(\"epoch\")     .set_delimiter(\",\")     .set_timezone_identifier(\"UTC\")     .build() )  # save the settings to a file settings_generator.save_settings_for_stream(     settings=cpc_settings,     path=path,     subfolder=\"CPC_3010_data\",     settings_suffix=\"_cpc\", ) In\u00a0[3]: Copied! <pre># settings for the SMPS data\ndata_checks_sizer = (\n    DataChecksBuilder()\n    .set_characters([250])\n    .set_skip_rows(25)\n    .set_char_counts({\"/\": 2, \":\": 2})\n    .build()\n)\ndata_sizer_reader = (\n    SizerDataReaderBuilder()\n    .set_sizer_start_keyword(\"20.72\")\n    .set_sizer_end_keyword(\"784.39\")\n    .set_sizer_concentration_convert_from(\"dw/dlogdp\")\n    .build()\n)\nsmps_1d_settings, smps_2d_settings = (\n    LoaderSizerSettingsBuilder()\n    .set_relative_data_folder(\"SMPS_data\")\n    .set_filename_regex(\"*.csv\")\n    .set_header_row(24)\n    .set_data_checks(data_checks_sizer)\n    .set_data_column(\n        [\n            \"Lower Size (nm)\",\n            \"Upper Size (nm)\",\n            \"Sample Temp (C)\",\n            \"Sample Pressure (kPa)\",\n            \"Relative Humidity (%)\",\n            \"Median (nm)\",\n            \"Mean (nm)\",\n            \"Geo. Mean (nm)\",\n            \"Mode (nm)\",\n            \"Geo. Std. Dev.\",\n            \"Total Conc. (#/cm\u00b3)\",\n        ]\n    )\n    .set_data_header(\n        [\n            \"Lower_Size_(nm)\",\n            \"Upper_Size_(nm)\",\n            \"Sample_Temp_(C)\",\n            \"Sample_Pressure_(kPa)\",\n            \"Relative_Humidity_(%)\",\n            \"Median_(nm)\",\n            \"Mean_(nm)\",\n            \"Geo_Mean_(nm)\",\n            \"Mode_(nm)\",\n            \"Geo_Std_Dev.\",\n            \"Total_Conc_(#/cc)\",\n        ]\n    )\n    .set_data_sizer_reader(data_sizer_reader)\n    .set_time_column([1, 2])\n    .set_time_format(\"%m/%d/%Y %H:%M:%S\")\n    .set_delimiter(\",\")\n    .set_timezone_identifier(\"UTC\")\n    .build()\n)\n\n# save the settings to a file\nsettings_generator.save_settings_for_stream(\n    settings=smps_1d_settings,\n    path=path,\n    subfolder=\"SMPS_data\",\n    settings_suffix=\"_smps_1d\",\n)\nsettings_generator.save_settings_for_stream(\n    settings=smps_2d_settings,\n    path=path,\n    subfolder=\"SMPS_data\",\n    settings_suffix=\"_smps_2d\",\n)\n</pre> # settings for the SMPS data data_checks_sizer = (     DataChecksBuilder()     .set_characters([250])     .set_skip_rows(25)     .set_char_counts({\"/\": 2, \":\": 2})     .build() ) data_sizer_reader = (     SizerDataReaderBuilder()     .set_sizer_start_keyword(\"20.72\")     .set_sizer_end_keyword(\"784.39\")     .set_sizer_concentration_convert_from(\"dw/dlogdp\")     .build() ) smps_1d_settings, smps_2d_settings = (     LoaderSizerSettingsBuilder()     .set_relative_data_folder(\"SMPS_data\")     .set_filename_regex(\"*.csv\")     .set_header_row(24)     .set_data_checks(data_checks_sizer)     .set_data_column(         [             \"Lower Size (nm)\",             \"Upper Size (nm)\",             \"Sample Temp (C)\",             \"Sample Pressure (kPa)\",             \"Relative Humidity (%)\",             \"Median (nm)\",             \"Mean (nm)\",             \"Geo. Mean (nm)\",             \"Mode (nm)\",             \"Geo. Std. Dev.\",             \"Total Conc. (#/cm\u00b3)\",         ]     )     .set_data_header(         [             \"Lower_Size_(nm)\",             \"Upper_Size_(nm)\",             \"Sample_Temp_(C)\",             \"Sample_Pressure_(kPa)\",             \"Relative_Humidity_(%)\",             \"Median_(nm)\",             \"Mean_(nm)\",             \"Geo_Mean_(nm)\",             \"Mode_(nm)\",             \"Geo_Std_Dev.\",             \"Total_Conc_(#/cc)\",         ]     )     .set_data_sizer_reader(data_sizer_reader)     .set_time_column([1, 2])     .set_time_format(\"%m/%d/%Y %H:%M:%S\")     .set_delimiter(\",\")     .set_timezone_identifier(\"UTC\")     .build() )  # save the settings to a file settings_generator.save_settings_for_stream(     settings=smps_1d_settings,     path=path,     subfolder=\"SMPS_data\",     settings_suffix=\"_smps_1d\", ) settings_generator.save_settings_for_stream(     settings=smps_2d_settings,     path=path,     subfolder=\"SMPS_data\",     settings_suffix=\"_smps_2d\", ) In\u00a0[4]: Copied! <pre>smps_1d_stream_settings = settings_generator.load_settings_for_stream(\n    path=path,\n    subfolder=\"SMPS_data\",\n    settings_suffix=\"_smps_1d\",\n)\n\nstream_smps_1d = loader_interface.load_files_interface(\n    path=path, settings=smps_1d_stream_settings\n)\n\nprint(stream_smps_1d.header)\n</pre> smps_1d_stream_settings = settings_generator.load_settings_for_stream(     path=path,     subfolder=\"SMPS_data\",     settings_suffix=\"_smps_1d\", )  stream_smps_1d = loader_interface.load_files_interface(     path=path, settings=smps_1d_stream_settings )  print(stream_smps_1d.header) <pre>  Loading file: 2022-07-07_095151_SMPS.csv\n  Loading file: 2022-07-10_094659_SMPS.csv\n['Lower_Size_(nm)', 'Upper_Size_(nm)', 'Sample_Temp_(C)', 'Sample_Pressure_(kPa)', 'Relative_Humidity_(%)', 'Median_(nm)', 'Mean_(nm)', 'Geo_Mean_(nm)', 'Mode_(nm)', 'Geo_Std_Dev.', 'Total_Conc_(#/cc)']\n</pre> In\u00a0[5]: Copied! <pre># collect settings into a dictionary\ncombined_settings = {\n    \"cpc\": cpc_settings,\n    \"smps_1d\": smps_1d_settings,\n    \"smps_2d\": smps_2d_settings,\n}\n\n# save the lake settings to a file\nsettings_generator.save_settings_for_lake(\n    settings=combined_settings,\n    path=path,\n    subfolder=\"\",\n    settings_suffix=\"_cpc_smps\",\n)\n</pre> # collect settings into a dictionary combined_settings = {     \"cpc\": cpc_settings,     \"smps_1d\": smps_1d_settings,     \"smps_2d\": smps_2d_settings, }  # save the lake settings to a file settings_generator.save_settings_for_lake(     settings=combined_settings,     path=path,     subfolder=\"\",     settings_suffix=\"_cpc_smps\", ) In\u00a0[6]: Copied! <pre>lake_settings = settings_generator.load_settings_for_lake(\n    path=path,\n    subfolder=\"\",\n    settings_suffix=\"_cpc_smps\",\n)\n\n# now call the loader interface for files\nlake = loader_interface.load_folders_interface(\n    path=path,\n    folder_settings=combined_settings,\n)\n\nprint(\" \")\nprint(lake)\n</pre> lake_settings = settings_generator.load_settings_for_lake(     path=path,     subfolder=\"\",     settings_suffix=\"_cpc_smps\", )  # now call the loader interface for files lake = loader_interface.load_folders_interface(     path=path,     folder_settings=combined_settings, )  print(\" \") print(lake) <pre>Folder Settings: cpc\n  Loading file: CPC_3010_data_20220709_Jul.csv\n  Loading file: CPC_3010_data_20220710_Jul.csv\nFolder Settings: smps_1d\n  Loading file: 2022-07-07_095151_SMPS.csv\n  Loading file: 2022-07-10_094659_SMPS.csv\nFolder Settings: smps_2d\n  Loading file: 2022-07-07_095151_SMPS.csv\n  Loading file: 2022-07-10_094659_SMPS.csv\n \nLake with streams: ['cpc', 'smps_1d', 'smps_2d']\n</pre>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part4/#loading-part-4-settings-files","title":"Loading Part 4: Settings Files\u00b6","text":"<p>In this part of the tutorial, we will explore how to save and load stream and lake settings dictionaries. This can be incredibly useful for preserving your settings, sharing them with others, or simply avoiding the need to retype everything.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part4/#working-path","title":"Working Path\u00b6","text":"<p>In your working path, you will find a couple of <code>.json</code> files. These files are the settings files. The <code>lake_settings.json</code> file stores the settings for the lake, while the <code>stream_settings.json</code> file stores the settings for the stream. These settings are the same ones you created in the previous example, but now they are saved to files for easy access and sharing.</p> <pre><code>data\n\u251c\u2500\u2500 CPC_3010_data\n\u2502   \u251c\u2500\u2500 CPC_3010_data_20220709_Jul.csv\n\u2502   \u251c\u2500\u2500 CPC_3010_data_20220709_Jul.csv\n    \u251c\u2500\u2500 stream_settings_cpc.json\n\u251c\u2500\u2500 SMPS_data\n\u2502   \u251c\u2500\u2500 2022-07-07_095151_SMPS.csv\n\u2502   \u251c\u2500\u2500 2022-07-10_094659_SMPS.csv\n\u2502   \u251c\u2500\u2500 stream_settings_smps_1d.json\n\u2502   \u251c\u2500\u2500 stream_settings_smps_2d.json\n\u251c\u2500\u2500 lake_settings.json\n</code></pre>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part4/#generate-and-save-settings","title":"Generate and Save Settings\u00b6","text":"<p>First, we generate the settings for the CPC data using the <code>settings_generator.for_general_1d_load</code> function. These settings include details such as the data file location, file format, column names, and more.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part4/#next-save-the-smps-settings","title":"Next save the SMPS settings\u00b6","text":""},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part4/#loading-stream-settings","title":"Loading Stream Settings\u00b6","text":"<p>If you are still exploring your analysis pipeline, you may want to load settings for individual streams. To do so, you can use the generate_settings.load_settings_for_stream function. This function takes the path to the settings file as an argument and returns a dictionary containing the stream settings.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part4/#lake-settings","title":"Lake settings\u00b6","text":"<p>If you wanted to load everything for a reanalysis, instead of calling each individual stream, you can first save a lake settings file.  This is done with <code>generate_settings.save_settings_for_lake</code>.  This function takes the path to the lake settings file as an argument.  It returns a dictionary with the settings.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part4/#load-the-lake","title":"Load the Lake\u00b6","text":"<p>To load the lake settings use <code>generate_settings.load_settings_for_lake</code>.  This function takes the path to the lake settings file as an argument.  It returns a dictionary with the settings.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/loading_data_part4/#summary","title":"Summary\u00b6","text":"<p>This example showed how to save and load the settings for a stream and a lake.  This is useful if you want to save your settings and then load them later.  This is also useful if you want to share your settings with someone else.  Or just stop from having to retype everything.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/stream_stats_part1/","title":"Load the data","text":"In\u00a0[1]: Copied! <pre># all the imports, but we'll go through them one by one as we use them\nimport os\nimport matplotlib.pyplot as plt\nfrom particula_beta.data import (\n    loader_interface,\n    settings_generator,\n    stream_stats,\n)\nfrom particula_beta.data.tests.example_data.get_example_data import (\n    get_data_folder,\n)\nfrom particula_beta.data.loader_setting_builders import (\n    # These functions create settings for loading data from files.\n    DataChecksBuilder,\n    Loader1DSettingsBuilder,\n)\n\n# set the parent directory of the data folder\npath = get_data_folder()\nprint(\"Path to data folder:\")\nprint(path.rsplit(\"particula\")[-1])\n</pre> # all the imports, but we'll go through them one by one as we use them import os import matplotlib.pyplot as plt from particula_beta.data import (     loader_interface,     settings_generator,     stream_stats, ) from particula_beta.data.tests.example_data.get_example_data import (     get_data_folder, ) from particula_beta.data.loader_setting_builders import (     # These functions create settings for loading data from files.     DataChecksBuilder,     Loader1DSettingsBuilder, )  # set the parent directory of the data folder path = get_data_folder() print(\"Path to data folder:\") print(path.rsplit(\"particula\")[-1]) <pre>Path to data folder:\n_beta\\data\\tests\\example_data\n</pre> In\u00a0[2]: Copied! <pre># settings for the CPC data\ndata_checks_cpc = (\n    DataChecksBuilder()\n    .set_characters([10, 100])\n    .set_char_counts({\",\": 4})\n    .build()\n)\n\nsettings = (\n    Loader1DSettingsBuilder()\n    .set_relative_data_folder(\"CPC_3010_data\")\n    .set_filename_regex(\"*.csv\")\n    .set_header_row(0)\n    .set_data_checks(data_checks_cpc)  # from above\n    .set_data_column([1, 2])\n    .set_data_header([\"CPC_count[#/sec]\", \"Temperature[degC]\"])\n    .set_time_column([0])\n    .set_time_format(\"epoch\")\n    .set_delimiter(\",\")\n    .set_timezone_identifier(\"UTC\")\n    .build()\n)\n\n# now call the loader interface\ndata_stream = loader_interface.load_files_interface(\n    path=path,\n    settings=settings,\n)\n</pre> # settings for the CPC data data_checks_cpc = (     DataChecksBuilder()     .set_characters([10, 100])     .set_char_counts({\",\": 4})     .build() )  settings = (     Loader1DSettingsBuilder()     .set_relative_data_folder(\"CPC_3010_data\")     .set_filename_regex(\"*.csv\")     .set_header_row(0)     .set_data_checks(data_checks_cpc)  # from above     .set_data_column([1, 2])     .set_data_header([\"CPC_count[#/sec]\", \"Temperature[degC]\"])     .set_time_column([0])     .set_time_format(\"epoch\")     .set_delimiter(\",\")     .set_timezone_identifier(\"UTC\")     .build() )  # now call the loader interface data_stream = loader_interface.load_files_interface(     path=path,     settings=settings, ) <pre>  Loading file: CPC_3010_data_20220709_Jul.csv\n  Loading file: CPC_3010_data_20220710_Jul.csv\n</pre> In\u00a0[3]: Copied! <pre># print data stream summary\nprint(\"Stream:\")\nprint(data_stream)\n</pre> # print data stream summary print(\"Stream:\") print(data_stream) <pre>Stream:\nStream(header=['CPC_count[#/sec]', 'Temperature[degC]'], data=array([[3.3510e+04, 1.7000e+01],\n       [3.3465e+04, 1.7100e+01],\n       [3.2171e+04, 1.7000e+01],\n       ...,\n       [1.9403e+04, 1.6900e+01],\n       [2.0230e+04, 1.7000e+01],\n       [1.9521e+04, 1.6800e+01]]), time=array([1.65734280e+09, 1.65734281e+09, 1.65734281e+09, ...,\n       1.65751559e+09, 1.65751560e+09, 1.65751560e+09]), files=[['CPC_3010_data_20220709_Jul.csv', 1044534], ['CPC_3010_data_20220710_Jul.csv', 1113488]])\n</pre> In\u00a0[4]: Copied! <pre># plot the data\nfig, ax = plt.subplots()\nax.plot(\n    data_stream.datetime64,\n    data_stream.data[:, 0],  # data_stream.data is a 2d array, so we need\n    # to specify which column we want to plot\n    label=data_stream.header[0],\n    linestyle=\"none\",\n    marker=\".\",\n)\nplt.xticks(rotation=45)\nax.set_xlabel(\"Time (UTC)\")\nax.set_ylabel(data_stream.header[0])\nplt.show()\nfig.tight_layout()\n</pre> # plot the data fig, ax = plt.subplots() ax.plot(     data_stream.datetime64,     data_stream.data[:, 0],  # data_stream.data is a 2d array, so we need     # to specify which column we want to plot     label=data_stream.header[0],     linestyle=\"none\",     marker=\".\", ) plt.xticks(rotation=45) ax.set_xlabel(\"Time (UTC)\") ax.set_ylabel(data_stream.header[0]) plt.show() fig.tight_layout() In\u00a0[5]: Copied! <pre>stream_averaged = stream_stats.average_std(\n    stream=data_stream,\n    average_interval=600,\n)\nstream_averaged.standard_deviation.shape\n</pre> stream_averaged = stream_stats.average_std(     stream=data_stream,     average_interval=600, ) stream_averaged.standard_deviation.shape Out[5]: <pre>(288, 2)</pre> In\u00a0[6]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(\n    stream_averaged.datetime64,\n    stream_averaged.data[:, 0],\n    label=stream_averaged.header[0],\n    marker=\".\",\n)\nplt.xticks(rotation=45)\nax.set_xlabel(\"Time (UTC)\")\nax.set_ylabel(stream_averaged.header[0])\nplt.show()\nfig.tight_layout()\n</pre> fig, ax = plt.subplots() ax.plot(     stream_averaged.datetime64,     stream_averaged.data[:, 0],     label=stream_averaged.header[0],     marker=\".\", ) plt.xticks(rotation=45) ax.set_xlabel(\"Time (UTC)\") ax.set_ylabel(stream_averaged.header[0]) plt.show() fig.tight_layout() In\u00a0[7]: Copied! <pre>stream_filtered = stream_stats.filtering(\n    stream=data_stream,\n    top=250000,\n    drop=True,\n)\nfig, ax = plt.subplots()\nax.plot(\n    stream_filtered.datetime64,\n    stream_filtered.data[:, 0],\n    label=stream_filtered.header[0],\n    marker=\".\",\n)\nplt.xticks(rotation=45)\nax.set_xlabel(\"Time (UTC)\")\nax.set_ylabel(stream_filtered.header[0])\nplt.show()\nfig.tight_layout()\n</pre> stream_filtered = stream_stats.filtering(     stream=data_stream,     top=250000,     drop=True, ) fig, ax = plt.subplots() ax.plot(     stream_filtered.datetime64,     stream_filtered.data[:, 0],     label=stream_filtered.header[0],     marker=\".\", ) plt.xticks(rotation=45) ax.set_xlabel(\"Time (UTC)\") ax.set_ylabel(stream_filtered.header[0]) plt.show() fig.tight_layout()"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/stream_stats_part1/#stream-averaging-and-outliers","title":"Stream: Averaging and Outliers\u00b6","text":"<p>This example shows how to clean up a stream of data by removing outliers and and averaging the values over time.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/stream_stats_part1/#working-path","title":"Working path\u00b6","text":"<p>Set the working path where the data is stored. For now we'll use the provided example data in this current directory.</p> <p>But the path could be any where on your computer. For example, if you have a folder called \"data\" in your home directory, you could set the path to: <code>path = \"U:\\\\data\\\\processing\\\\Campgain2023_of_aswsome\\\\data\"</code></p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/stream_stats_part1/#load-the-data","title":"Load the data\u00b6","text":"<p>For this example we'll use the provided example data. But you can change the path to any folder on your computer. We then can used the settings generator to load the data.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/stream_stats_part1/#average-the-data","title":"Average the data\u00b6","text":"<p>Now that we have the data loaded, we can average the data over time. We'll use the 'particula_beta.data.stream_stats' module to do this. The module has a function called 'averaged_std' that will take stream object and return a new stream object with the averaged data and the standard deviation of the data.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/stream_stats_part1/#plot-the-averaged-data","title":"Plot the averaged data\u00b6","text":""},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/stream_stats_part1/#clean-up-the-data","title":"Clean up the data\u00b6","text":"<p>Now we may see some outliers in the data. We can use the 'particula_beta.data.stream_stats' module to remove the outliers. The module has a function called 'filtering' that will take stream object and return a new stream object with the outliers removed.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/stream_stats_part1/#summary","title":"Summary\u00b6","text":"<p>This example shows how to clean up a stream of data by removing outliers and and averaging the values over time.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/stream_stats_size_distribution_part2/","title":"Stream stats size distribution part2","text":"In\u00a0[1]: Copied! <pre># all the imports\nimport matplotlib.pyplot as plt\nfrom particula_beta.data import loader_interface, settings_generator\nfrom particula_beta.data.tests.example_data.get_example_data import (\n    get_data_folder,\n)\nfrom particula_beta.data.loader_setting_builders import (\n    # These functions create settings for loading data from files.\n    DataChecksBuilder,\n    SizerDataReaderBuilder,\n    LoaderSizerSettingsBuilder,\n)\n\n# the new step\nfrom particula_beta.data.process import size_distribution\n\n# set the parent directory of the data folder\npath = get_data_folder()\n</pre> # all the imports import matplotlib.pyplot as plt from particula_beta.data import loader_interface, settings_generator from particula_beta.data.tests.example_data.get_example_data import (     get_data_folder, ) from particula_beta.data.loader_setting_builders import (     # These functions create settings for loading data from files.     DataChecksBuilder,     SizerDataReaderBuilder,     LoaderSizerSettingsBuilder, )  # the new step from particula_beta.data.process import size_distribution  # set the parent directory of the data folder path = get_data_folder() In\u00a0[2]: Copied! <pre># settings for the SMPS data\ndata_checks_sizer = (\n    DataChecksBuilder()\n    .set_characters([250])\n    .set_skip_rows(25)\n    .set_char_counts({\"/\": 2, \":\": 2})\n    .build()\n)\ndata_sizer_reader = (\n    SizerDataReaderBuilder()\n    .set_sizer_start_keyword(\"20.72\")\n    .set_sizer_end_keyword(\"784.39\")\n    .set_sizer_concentration_convert_from(\"dw/dlogdp\")\n    .build()\n)\nsmps_1d_settings, smps_2d_settings = (\n    LoaderSizerSettingsBuilder()\n    .set_relative_data_folder(\"SMPS_data\")\n    .set_filename_regex(\"*.csv\")\n    .set_header_row(24)\n    .set_data_checks(data_checks_sizer)\n    .set_data_column(\n        [\n            \"Lower Size (nm)\",\n            \"Upper Size (nm)\",\n            \"Sample Temp (C)\",\n            \"Sample Pressure (kPa)\",\n            \"Relative Humidity (%)\",\n            \"Median (nm)\",\n            \"Mean (nm)\",\n            \"Geo. Mean (nm)\",\n            \"Mode (nm)\",\n            \"Geo. Std. Dev.\",\n            \"Total Conc. (#/cm\u00b3)\",\n        ]\n    )\n    .set_data_header(\n        [\n            \"Lower_Size_(nm)\",\n            \"Upper_Size_(nm)\",\n            \"Sample_Temp_(C)\",\n            \"Sample_Pressure_(kPa)\",\n            \"Relative_Humidity_(%)\",\n            \"Median_(nm)\",\n            \"Mean_(nm)\",\n            \"Geo_Mean_(nm)\",\n            \"Mode_(nm)\",\n            \"Geo_Std_Dev.\",\n            \"Total_Conc_(#/cc)\",\n        ]\n    )\n    .set_data_sizer_reader(data_sizer_reader)\n    .set_time_column([1, 2])\n    .set_time_format(\"%m/%d/%Y %H:%M:%S\")\n    .set_delimiter(\",\")\n    .set_timezone_identifier(\"UTC\")\n    .build()\n)\n\n# collect settings into a dictionary\ncombined_settings = {\n    \"smps_1d\": smps_1d_settings,\n    \"smps_2d\": smps_2d_settings,\n}\n\n# now call the loader interface for files\nlake = loader_interface.load_folders_interface(\n    path=path,\n    folder_settings=combined_settings,\n)\n\nprint(\" \")\nprint(lake)\n</pre> # settings for the SMPS data data_checks_sizer = (     DataChecksBuilder()     .set_characters([250])     .set_skip_rows(25)     .set_char_counts({\"/\": 2, \":\": 2})     .build() ) data_sizer_reader = (     SizerDataReaderBuilder()     .set_sizer_start_keyword(\"20.72\")     .set_sizer_end_keyword(\"784.39\")     .set_sizer_concentration_convert_from(\"dw/dlogdp\")     .build() ) smps_1d_settings, smps_2d_settings = (     LoaderSizerSettingsBuilder()     .set_relative_data_folder(\"SMPS_data\")     .set_filename_regex(\"*.csv\")     .set_header_row(24)     .set_data_checks(data_checks_sizer)     .set_data_column(         [             \"Lower Size (nm)\",             \"Upper Size (nm)\",             \"Sample Temp (C)\",             \"Sample Pressure (kPa)\",             \"Relative Humidity (%)\",             \"Median (nm)\",             \"Mean (nm)\",             \"Geo. Mean (nm)\",             \"Mode (nm)\",             \"Geo. Std. Dev.\",             \"Total Conc. (#/cm\u00b3)\",         ]     )     .set_data_header(         [             \"Lower_Size_(nm)\",             \"Upper_Size_(nm)\",             \"Sample_Temp_(C)\",             \"Sample_Pressure_(kPa)\",             \"Relative_Humidity_(%)\",             \"Median_(nm)\",             \"Mean_(nm)\",             \"Geo_Mean_(nm)\",             \"Mode_(nm)\",             \"Geo_Std_Dev.\",             \"Total_Conc_(#/cc)\",         ]     )     .set_data_sizer_reader(data_sizer_reader)     .set_time_column([1, 2])     .set_time_format(\"%m/%d/%Y %H:%M:%S\")     .set_delimiter(\",\")     .set_timezone_identifier(\"UTC\")     .build() )  # collect settings into a dictionary combined_settings = {     \"smps_1d\": smps_1d_settings,     \"smps_2d\": smps_2d_settings, }  # now call the loader interface for files lake = loader_interface.load_folders_interface(     path=path,     folder_settings=combined_settings, )  print(\" \") print(lake) <pre>Folder Settings: smps_1d\n  Loading file: 2022-07-07_095151_SMPS.csv\n  Loading file: 2022-07-10_094659_SMPS.csv\nFolder Settings: smps_2d\n  Loading file: 2022-07-07_095151_SMPS.csv\n  Loading file: 2022-07-10_094659_SMPS.csv\n \nLake with streams: ['smps_1d', 'smps_2d']\n</pre> In\u00a0[3]: Copied! <pre>lake[\"mean_properties\"] = size_distribution.sizer_mean_properties(\n    stream=lake[\"smps_2d\"],\n    diameter_units=\"nm\",\n)\n\n# list out the header\nfor header in lake[\"mean_properties\"].header:\n    print(header)\n</pre> lake[\"mean_properties\"] = size_distribution.sizer_mean_properties(     stream=lake[\"smps_2d\"],     diameter_units=\"nm\", )  # list out the header for header in lake[\"mean_properties\"].header:     print(header) <pre>Total_Conc_(#/cc)\nMean_Diameter_(nm)\nGeometric_Mean_Diameter_(nm)\nMode_Diameter_(nm)\nMean_Diameter_Vol_(nm)\nMode_Diameter_Vol_(nm)\nUnit_Mass_(ug/m3)\nMass_(ug/m3)\nTotal_Conc_(#/cc)_N100\nUnit_Mass_(ug/m3)_N100\nMass_(ug/m3)_N100\nTotal_Conc_(#/cc)_PM1\nUnit_Mass_(ug/m3)_PM1\nMass_(ug/m3)_PM1\nTotal_Conc_(#/cc)_PM2.5\nUnit_Mass_(ug/m3)_PM2.5\nMass_(ug/m3)_PM2.5\nTotal_Conc_(#/cc)_PM10\nUnit_Mass_(ug/m3)_PM10\nMass_(ug/m3)_PM10\n</pre> In\u00a0[4]: Copied! <pre>mean_prop_stream = lake[\"mean_properties\"]\n\n# plot the data on twinx axis\nfig, ax = plt.subplots()\nax.plot(\n    mean_prop_stream.datetime64,\n    mean_prop_stream[\"Mass_(ug/m3)_PM2.5\"],\n    label=\"PM 2.5\",\n    color=\"blue\",\n)\nax.plot(\n    mean_prop_stream.datetime64,\n    mean_prop_stream[\"Mass_(ug/m3)_N100\"],\n    label=\"N100 mass\",\n    color=\"red\",\n)\nax.set_ylim(0, 50)\nplt.xticks(rotation=45)\nax.set_xlabel(\"Time (UTC)\")\nax.set_ylabel(\"PM mass (ug/m3)\")\nax.legend()\nplt.show()\nfig.tight_layout()\n</pre> mean_prop_stream = lake[\"mean_properties\"]  # plot the data on twinx axis fig, ax = plt.subplots() ax.plot(     mean_prop_stream.datetime64,     mean_prop_stream[\"Mass_(ug/m3)_PM2.5\"],     label=\"PM 2.5\",     color=\"blue\", ) ax.plot(     mean_prop_stream.datetime64,     mean_prop_stream[\"Mass_(ug/m3)_N100\"],     label=\"N100 mass\",     color=\"red\", ) ax.set_ylim(0, 50) plt.xticks(rotation=45) ax.set_xlabel(\"Time (UTC)\") ax.set_ylabel(\"PM mass (ug/m3)\") ax.legend() plt.show() fig.tight_layout()"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/stream_stats_size_distribution_part2/#size-distribution-stats","title":"Size Distribution Stats\u00b6","text":"<p>This example shows how to process size distribution data from an SMPS. The processing returns mean properties of the size distribution, such as the mean diameter, median diameter, and total PM2.5 mass.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/stream_stats_size_distribution_part2/#load-the-data","title":"Load the data\u00b6","text":"<p>For this example we'll use the provided example data. But you can change the path to any folder on your computer. We then can used the settings generator to</p> <p>If you think this settings generator is getting tedious, we hear you. We'll show the fix to that soon.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/stream_stats_size_distribution_part2/#processing-the-stream","title":"Processing the Stream\u00b6","text":"<p>The lake is a collection of streams, stored as a dictionary. The next step can be average the data or you may have a processing step that you want to apply to all the data. For example, you may want to calculate the total PM2.5 mass from the SMPS data. You could do this by looping through the streams and applying a cusutom processing function to each stream. Or you could use some standard process already built in to <code>particula_beta.data.process</code>. In this example we'll use <code>process.size_distribution</code> to calculate the PM2.5 mass from the</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/stream_stats_size_distribution_part2/#plot-the-data","title":"Plot the Data\u00b6","text":"<p>With that processing done we can plot some useful summary plots. For example, we can plot the total PM2.5 mass as a function of time. And on the same plot we can add the N100 mass.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/stream_stats_size_distribution_part2/#tip-calling-the-header-directly","title":"Tip: Calling the header directly\u00b6","text":"<p>Note below how we can call the data header directly from the stream. This is because the get items property defined in the stream class accepts, either and index or a header name. So we can call the header name directly from the stream and get back that specific time series.</p> <p>This is incontrast to callint <code>stream.data['header_name']</code> which would return an error. As that line first calls <code>stream.data</code> returning the np.ndarray, then calls the header name, which is not a valid index for a np.ndarray.</p>"},{"location":"How-To-Guides/Data_Streams_and_Lakes/notebooks/stream_stats_size_distribution_part2/#summary","title":"Summary\u00b6","text":"<p>This example showed how to process size distribution data from an SMPS. The processing returns mean properties of the size distribution, such as the mean diameter, median diameter, and total PM2.5 mass.</p>"},{"location":"How-To-Guides/Lagrangian/","title":"Index: Lagrangian Particle BETA","text":"<p>This example will cover the basics of the Lagrangian particle model. Starting with a simple initial condition, and then adding more complexity to the model.</p>"},{"location":"How-To-Guides/Lagrangian/#notebooks","title":"Notebooks","text":"<ul> <li>Basic Lagrangian Particle Model</li> <li>Faster via Sweep and Prune</li> <li>Drag Force</li> <li>Turbulence Approximation</li> </ul>"},{"location":"How-To-Guides/Lagrangian/notebooks/basic_lagrangian_box/","title":"Introduction to the Lagrangian Box Model","text":"In\u00a0[22]: Copied! <pre># Code Section: Importing Necessary Libraries and Initializing Variables\nimport time\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom particula_beta.lagrangian import (\n    boundary,\n    integration,\n    collisions,\n    particle_property,\n    particle_pairs,\n)\n\n# Initializing the Torch Generator and setting the data type\ntorch.manual_seed(1234)  # Set the seed for reproducibility\nt_type = torch.float32\n</pre> # Code Section: Importing Necessary Libraries and Initializing Variables import time import torch import numpy as np import matplotlib.pyplot as plt from particula_beta.lagrangian import (     boundary,     integration,     collisions,     particle_property,     particle_pairs, )  # Initializing the Torch Generator and setting the data type torch.manual_seed(1234)  # Set the seed for reproducibility t_type = torch.float32 In\u00a0[23]: Copied! <pre># Setting up the Simulation Parameters and Initial Conditions\n\n# Define fixed parameters\nTOTAL_NUMBER_OF_PARTICLES = 500\nTIME_STEP = 0.01\nSIMULATION_TIME = 100\nMASS = 3\nCUBE_SIDE = 50\nspeed = 5\nsave_points = 50\n\n# Initialize particle positions randomly within the cube\nposition = (\n    torch.rand(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * CUBE_SIDE\n    - CUBE_SIDE / 2\n)\n\n# Initialize particle velocities randomly\nvelocity = (\n    torch.rand(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * speed - speed / 2\n)\n\n# Initialize force as zero for all particles\nforce = torch.zeros(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type)\n\n# Set constant mass and density for all particles\nmass = torch.ones(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * MASS\ndensity = torch.ones(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * 1\n\n# Generate indices for particles, could be integer type\nindices = torch.arange(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type)  # could be int\n\n# Calculate the total number of iterations for the simulation\ntotal_iterations = int(SIMULATION_TIME / TIME_STEP)\n\n# Initialize a tensor to track the total mass over iterations\ntotal_mass = torch.zeros(total_iterations, dtype=t_type)\n\n# Define gravity acting on all particles\ngravity = (\n    torch.tensor([0, -9.81, 0])\n    .repeat(TOTAL_NUMBER_OF_PARTICLES, 1)\n    .transpose(0, 1)\n)\n</pre> # Setting up the Simulation Parameters and Initial Conditions  # Define fixed parameters TOTAL_NUMBER_OF_PARTICLES = 500 TIME_STEP = 0.01 SIMULATION_TIME = 100 MASS = 3 CUBE_SIDE = 50 speed = 5 save_points = 50  # Initialize particle positions randomly within the cube position = (     torch.rand(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * CUBE_SIDE     - CUBE_SIDE / 2 )  # Initialize particle velocities randomly velocity = (     torch.rand(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * speed - speed / 2 )  # Initialize force as zero for all particles force = torch.zeros(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type)  # Set constant mass and density for all particles mass = torch.ones(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * MASS density = torch.ones(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * 1  # Generate indices for particles, could be integer type indices = torch.arange(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type)  # could be int  # Calculate the total number of iterations for the simulation total_iterations = int(SIMULATION_TIME / TIME_STEP)  # Initialize a tensor to track the total mass over iterations total_mass = torch.zeros(total_iterations, dtype=t_type)  # Define gravity acting on all particles gravity = (     torch.tensor([0, -9.81, 0])     .repeat(TOTAL_NUMBER_OF_PARTICLES, 1)     .transpose(0, 1) ) In\u00a0[24]: Copied! <pre># Initializing Arrays for Saving Position and Mass Data\n\n# Create arrays to store position and mass data at each save point\nsave_position = np.zeros((3, TOTAL_NUMBER_OF_PARTICLES, save_points))\nsave_mass = np.zeros((TOTAL_NUMBER_OF_PARTICLES, save_points))\n\n# Determine which iterations will correspond to the save points\nsave_iterations = np.linspace(0, total_iterations, save_points, dtype=int)\n</pre> # Initializing Arrays for Saving Position and Mass Data  # Create arrays to store position and mass data at each save point save_position = np.zeros((3, TOTAL_NUMBER_OF_PARTICLES, save_points)) save_mass = np.zeros((TOTAL_NUMBER_OF_PARTICLES, save_points))  # Determine which iterations will correspond to the save points save_iterations = np.linspace(0, total_iterations, save_points, dtype=int) In\u00a0[25]: Copied! <pre># Initialize counter for saving data\nsave_counter = 0\nstart_time = time.time()\n# Start the simulation\nfor i in range(total_iterations):\n\n    # Calculate pairwise distances between particles\n    distance_matrix = particle_pairs.calculate_pairwise_distance(\n        position=position\n    )\n\n    # Adjust distances for the radius of each particle (surface-to-surface\n    # distance)\n    radius = particle_property.radius_calculation(mass=mass, density=density)\n    distance_matrix -= radius.unsqueeze(1) + radius.unsqueeze(0)\n\n    # Identify pairs of particles that have collided\n    valid_collision_indices_pairs = collisions.find_collisions(\n        distance_matrix=distance_matrix, indices=indices, mass=mass\n    )\n\n    if valid_collision_indices_pairs.shape[0] &gt; 0:\n        # Coalesce particles that have collided and update their velocity and\n        # mass\n        velocity, mass = collisions.coalescence(\n            position=position,\n            velocity=velocity,\n            mass=mass,\n            radius=radius,\n            collision_indices_pairs=valid_collision_indices_pairs,\n        )\n\n    # Calculate the force acting on the particles (e.g., gravity)\n    force = mass * gravity\n\n    # Integrate the equations of motion to update position and velocity\n    position, velocity = integration.leapfrog(\n        position=position,\n        velocity=velocity,\n        force=force,\n        mass=mass,\n        time_step=TIME_STEP,\n    )\n\n    # Apply boundary conditions for the cube (wrap-around)\n    position = boundary.wrapped_cube(position=position, cube_side=CUBE_SIDE)\n\n    # Save the position and mass data at designated save points\n    if i == save_iterations[save_counter]:\n        save_position[:, :, save_counter] = position.detach().cpu().numpy()\n        save_mass[:, save_counter] = mass.detach().cpu().numpy()\n        save_counter += 1\n\n# Perform a final save of the position and mass data\nsave_position[:, :, -1] = position.detach().cpu().numpy()\nsave_mass[:, -1] = mass.detach().cpu().numpy()\n\n# Calculate the total simulation time\nend_time = time.time()\nprint(f\"Total wall time: {end_time - start_time} seconds\")\nprint(\n    f\"Ratio of wall time to simulation time: {(end_time - start_time) / SIMULATION_TIME}\"\n)\n</pre> # Initialize counter for saving data save_counter = 0 start_time = time.time() # Start the simulation for i in range(total_iterations):      # Calculate pairwise distances between particles     distance_matrix = particle_pairs.calculate_pairwise_distance(         position=position     )      # Adjust distances for the radius of each particle (surface-to-surface     # distance)     radius = particle_property.radius_calculation(mass=mass, density=density)     distance_matrix -= radius.unsqueeze(1) + radius.unsqueeze(0)      # Identify pairs of particles that have collided     valid_collision_indices_pairs = collisions.find_collisions(         distance_matrix=distance_matrix, indices=indices, mass=mass     )      if valid_collision_indices_pairs.shape[0] &gt; 0:         # Coalesce particles that have collided and update their velocity and         # mass         velocity, mass = collisions.coalescence(             position=position,             velocity=velocity,             mass=mass,             radius=radius,             collision_indices_pairs=valid_collision_indices_pairs,         )      # Calculate the force acting on the particles (e.g., gravity)     force = mass * gravity      # Integrate the equations of motion to update position and velocity     position, velocity = integration.leapfrog(         position=position,         velocity=velocity,         force=force,         mass=mass,         time_step=TIME_STEP,     )      # Apply boundary conditions for the cube (wrap-around)     position = boundary.wrapped_cube(position=position, cube_side=CUBE_SIDE)      # Save the position and mass data at designated save points     if i == save_iterations[save_counter]:         save_position[:, :, save_counter] = position.detach().cpu().numpy()         save_mass[:, save_counter] = mass.detach().cpu().numpy()         save_counter += 1  # Perform a final save of the position and mass data save_position[:, :, -1] = position.detach().cpu().numpy() save_mass[:, -1] = mass.detach().cpu().numpy()  # Calculate the total simulation time end_time = time.time() print(f\"Total wall time: {end_time - start_time} seconds\") print(     f\"Ratio of wall time to simulation time: {(end_time - start_time) / SIMULATION_TIME}\" ) <pre>Total wall time: 19.743536710739136 seconds\nRatio of wall time to simulation time: 0.19743536710739135\n</pre> In\u00a0[26]: Copied! <pre># Processing the Final Data for Visualization\n\n# Select the final time index for the data\ntime_index = -1\nposition_final = save_position[:, :, time_index]\nmass_final = save_mass[:, time_index]\n\n# Filter out particles with zero mass\nfilter_zero_mass = mass_final &gt; 0\n\n# Calculate the radius and area of each particle\n\nradius_final = (3 * mass_final / (4 * np.pi * density.cpu().numpy())) ** (\n    1 / 3\n)\nparticle_area = np.pi * radius_final**2\n\n# Display the number of remaining particles\nprint(f\"Number of particles at the end: {filter_zero_mass.sum()}\")\n\n# Creating a 3D Plot for Visualization\nfig = plt.figure()\nax = fig.add_subplot(projection=\"3d\")\n\n# Choose a color map for the scatter plot\ncmap = plt.cm.viridis\n\n# Plot the final positions of particles with non-zero mass\nscatter_plot = ax.scatter(\n    position_final[0, filter_zero_mass],\n    position_final[1, filter_zero_mass],\n    position_final[2, filter_zero_mass],\n    c=mass_final[filter_zero_mass],\n    cmap=cmap,\n    s=particle_area[filter_zero_mass],  # Particle size based on area\n)\n\n# Set axis limits based on cube dimensions\nax.set_xlim(-CUBE_SIDE / 2, CUBE_SIDE / 2)\nax.set_ylim(-CUBE_SIDE / 2, CUBE_SIDE / 2)\nax.set_zlim(-CUBE_SIDE / 2, CUBE_SIDE / 2)\n\n# Add a color bar indicating particle mass\ncolor_bar = plt.colorbar(scatter_plot, ax=ax)\ncolor_bar.set_label(\"Mass\")\n\n# Show the plot with optimized layout\nplt.show()\nfig.tight_layout()\n</pre> # Processing the Final Data for Visualization  # Select the final time index for the data time_index = -1 position_final = save_position[:, :, time_index] mass_final = save_mass[:, time_index]  # Filter out particles with zero mass filter_zero_mass = mass_final &gt; 0  # Calculate the radius and area of each particle  radius_final = (3 * mass_final / (4 * np.pi * density.cpu().numpy())) ** (     1 / 3 ) particle_area = np.pi * radius_final**2  # Display the number of remaining particles print(f\"Number of particles at the end: {filter_zero_mass.sum()}\")  # Creating a 3D Plot for Visualization fig = plt.figure() ax = fig.add_subplot(projection=\"3d\")  # Choose a color map for the scatter plot cmap = plt.cm.viridis  # Plot the final positions of particles with non-zero mass scatter_plot = ax.scatter(     position_final[0, filter_zero_mass],     position_final[1, filter_zero_mass],     position_final[2, filter_zero_mass],     c=mass_final[filter_zero_mass],     cmap=cmap,     s=particle_area[filter_zero_mass],  # Particle size based on area )  # Set axis limits based on cube dimensions ax.set_xlim(-CUBE_SIDE / 2, CUBE_SIDE / 2) ax.set_ylim(-CUBE_SIDE / 2, CUBE_SIDE / 2) ax.set_zlim(-CUBE_SIDE / 2, CUBE_SIDE / 2)  # Add a color bar indicating particle mass color_bar = plt.colorbar(scatter_plot, ax=ax) color_bar.set_label(\"Mass\")  # Show the plot with optimized layout plt.show() fig.tight_layout() <pre>Number of particles at the end: 56\n</pre> In\u00a0[27]: Copied! <pre># Visualizing the Mass Distribution at Different Stages\n\n# Create a new figure for the histogram\nfig = plt.figure()\nax = fig.add_subplot()\n\n# Normalizing by initial MASS to observe distribution changes\nnormalized_mass = save_mass[filter_zero_mass, :] / MASS\nmax_mass = normalized_mass.max()\n\n# Plot histograms of mass distribution at different stages\nax.hist(\n    normalized_mass[:, 0],\n    bins=25,\n    alpha=0.8,\n    label=\"Initial\",\n    range=(0, max_mass),\n)\nax.hist(\n    normalized_mass[:, 24],\n    bins=25,\n    alpha=0.6,\n    label=\"Middle\",\n    range=(0, max_mass),\n)\nax.hist(\n    normalized_mass[:, -1],\n    bins=25,\n    alpha=0.5,\n    label=\"Final\",\n    range=(0, max_mass),\n)\n\n# Setting labels and title for the plot\nax.set_xlabel(\"Mass / Initial MASS\")\nax.set_ylabel(\"Number of Particles\")\n\n# Add a legend to the plot\nax.legend()\n\n# Display the plot\nplt.show()\n\n# Adjust layout for optimal visualization\nfig.tight_layout()\n</pre> # Visualizing the Mass Distribution at Different Stages  # Create a new figure for the histogram fig = plt.figure() ax = fig.add_subplot()  # Normalizing by initial MASS to observe distribution changes normalized_mass = save_mass[filter_zero_mass, :] / MASS max_mass = normalized_mass.max()  # Plot histograms of mass distribution at different stages ax.hist(     normalized_mass[:, 0],     bins=25,     alpha=0.8,     label=\"Initial\",     range=(0, max_mass), ) ax.hist(     normalized_mass[:, 24],     bins=25,     alpha=0.6,     label=\"Middle\",     range=(0, max_mass), ) ax.hist(     normalized_mass[:, -1],     bins=25,     alpha=0.5,     label=\"Final\",     range=(0, max_mass), )  # Setting labels and title for the plot ax.set_xlabel(\"Mass / Initial MASS\") ax.set_ylabel(\"Number of Particles\")  # Add a legend to the plot ax.legend()  # Display the plot plt.show()  # Adjust layout for optimal visualization fig.tight_layout()"},{"location":"How-To-Guides/Lagrangian/notebooks/basic_lagrangian_box/#introduction-to-the-lagrangian-box-model","title":"Introduction to the Lagrangian Box Model\u00b6","text":"<p>In this notebook, we will develop a simple box model for a Lagrangian particle. This model serves as a foundational tool to understand how various processes influence the evolution of a particle. We will start with a basic framework and gradually introduce more complex elements.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/basic_lagrangian_box/#understanding-particle-dynamics-in-a-cube","title":"Understanding Particle Dynamics in a Cube\u00b6","text":"<p>Here, we simulate particles moving within a cubic space. When these particles collide, they coagulate. This initial model does not include processes like condensation, evaporation, or forces such as drag. However, these elements will be integrated in subsequent steps to enhance realism.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/basic_lagrangian_box/#enhancing-particle-realism-and-model-complexity","title":"Enhancing Particle Realism and Model Complexity\u00b6","text":"<p>Currently, the particles lack realistic properties, but we plan to incorporate these features progressively. The model's complexity will also be scaled up, which is where the flexibility of the <code>particula_beta.lagrangian</code> module becomes crucial.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/basic_lagrangian_box/#leveraging-pytorch-for-enhanced-computational-performance","title":"Leveraging PyTorch for Enhanced Computational Performance\u00b6","text":"<p>This model is designed with a focus on PyTorch, allowing us to seamlessly switch between CPU and GPU computations. This capability becomes increasingly important as we add more particles and complexity to the model, necessitating the use of GPU for efficient computation.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/basic_lagrangian_box/#defining-the-system-parameters-and-initial-conditions","title":"Defining the System: Parameters and Initial Conditions\u00b6","text":"<p>The first step in our simulation is to define the system. This involves setting up a combination of fixed parameters and initializing the conditions for our particles. These parameters include the total number of particles, time step for the simulation, mass of each particle, dimensions of the cubic space, and other physical properties. The initial positions and velocities of the particles are generated randomly within this defined space, laying the groundwork for the subsequent simulation dynamics.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/basic_lagrangian_box/#setting-up-data-saving-mechanism","title":"Setting Up Data Saving Mechanism\u00b6","text":"<p>To effectively monitor and analyze the simulation, we will record specific variables at predetermined intervals. In this case, we'll focus on saving the position and mass of each particle. These variables will be captured at a set number of 'save points' throughout the simulation, allowing us to track the dynamic changes over time.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/basic_lagrangian_box/#executing-the-box-model-simulation","title":"Executing the Box Model Simulation\u00b6","text":"<p>With our system defined and data saving mechanisms in place, we now proceed to run the simulation. The process involves a series of steps that are iteratively executed to simulate the dynamics of the particles within the box. The steps are as follows:</p> <ul> <li>Boundary Check: Verify if any particles have moved outside the boundaries of the box and handle accordingly.</li> <li>Center Distance Calculation: Compute the pairwise distances between particles to their center.</li> <li>Surface Distance Calculation: Determine the pairwise distances from the surfaces of the particles.</li> <li>Collision Detection: Identify instances where particles collide with each other.</li> <li>Coalescence: Merge particles that have collided, simulating coalescence.</li> <li>Force Calculations: Apply force calculations to the particles, influencing their motion.</li> <li>Motion Integration: Integrate the equations of motion to update the position and velocity of each particle.</li> <li>Data Recording: At designated intervals, save the position and mass data of the particles for later analysis.</li> </ul>"},{"location":"How-To-Guides/Lagrangian/notebooks/basic_lagrangian_box/#visualizing-the-final-3d-positions-of-particles","title":"Visualizing the Final 3D Positions of Particles\u00b6","text":"<p>In the final step of our analysis, we focus on visualizing the 3D positions of the particles at the end of the simulation. To accurately represent the final state, we will exclude particles that have effectively zero mass, as these have merged with other particles during the coalescence process. This visualization will provide us with an insightful view of the particle distribution and the results of the dynamic interactions that have occurred throughout the simulation.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/basic_lagrangian_box/#analyzing-the-mass-distribution-of-particles","title":"Analyzing the Mass Distribution of Particles\u00b6","text":"<p>Next, we turn our attention to understanding how the mass distribution of the particles has evolved over the course of the simulation. To achieve this, we will analyze the mass data normalized by the initial MASS value. This normalization allows us to observe changes in the mass distribution as multiples of the initial mass, providing insights into the extent of coalescence and mass variation among the particles.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/basic_lagrangian_box/#summary-of-the-lagrangian-particle-simulation","title":"Summary of the Lagrangian Particle Simulation\u00b6","text":"<p>In this Jupyter Notebook, we have explored the dynamics of a Lagrangian particle system within a defined cubic space. Our focus has been on simulating and analyzing the interactions and evolution of particles under a set of initial conditions and physical laws. Here is a brief overview of what we covered:</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/basic_lagrangian_box/#system-definition-and-initialization","title":"System Definition and Initialization\u00b6","text":"<ul> <li>We began by defining the simulation space and initial conditions for our particle system. This included setting parameters such as the number of particles, mass, and dimensions of the cubic space, as well as initializing the positions and velocities of the particles.</li> </ul>"},{"location":"How-To-Guides/Lagrangian/notebooks/basic_lagrangian_box/#simulation-process","title":"Simulation Process\u00b6","text":"<ul> <li>The core of our simulation involved a series of iterative steps to simulate particle dynamics. These steps included checking boundary conditions, calculating distances, detecting and handling collisions, coalescing particles, applying forces, and integrating the equations of motion.</li> </ul>"},{"location":"How-To-Guides/Lagrangian/notebooks/basic_lagrangian_box/#data-saving-and-analysis","title":"Data Saving and Analysis\u00b6","text":"<ul> <li>Throughout the simulation, we saved key data points, such as the position and mass of particles at specified intervals. This allowed us to track and analyze the changes in the system over time.</li> </ul>"},{"location":"How-To-Guides/Lagrangian/notebooks/basic_lagrangian_box/#visualization-and-interpretation","title":"Visualization and Interpretation\u00b6","text":"<ul> <li>We utilized various visualization techniques to interpret the simulation results. This included creating 3D plots to visualize the final positions of particles and histograms to analyze the distribution of mass at different stages of the simulation.</li> </ul>"},{"location":"How-To-Guides/Lagrangian/notebooks/basic_lagrangian_box/#insights-gained","title":"Insights Gained\u00b6","text":"<ul> <li>The simulation provided valuable insights into the behavior of particles in a Lagrangian framework. We observed how particles interact, coalesce, and evolve over time under the influence of set physical parameters and forces.</li> </ul>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/","title":"Realistic Drag, Mass, and Velocity","text":"In\u00a0[\u00a0]: Copied! <pre># Code Section: Importing Necessary Libraries and Initializing Variables\nimport time\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom particula_beta.lagrangian import (\n    boundary,\n    integration,\n    collisions,\n    particle_property,\n    particle_pairs,\n)\nfrom particula.util import machine_limit\n\n# Initializing the Torch Generator and setting the data type\nseed = 1234\ntorch.manual_seed(seed)  # Set the seed for reproducibility\nt_type = torch.float32\n</pre> # Code Section: Importing Necessary Libraries and Initializing Variables import time import torch import numpy as np import matplotlib.pyplot as plt from particula_beta.lagrangian import (     boundary,     integration,     collisions,     particle_property,     particle_pairs, ) from particula.util import machine_limit  # Initializing the Torch Generator and setting the data type seed = 1234 torch.manual_seed(seed)  # Set the seed for reproducibility t_type = torch.float32 In\u00a0[132]: Copied! <pre># Setting up the Simulation Parameters and Initial Conditions\n\n# Define fixed parameters\nTOTAL_NUMBER_OF_PARTICLES = 2000\nTIME_STEP = 0.005\nSIMULATION_TIME = 100\nCUBE_SIDE = 0.010  # Size of the simulation cube in meters\nsave_points = 50  # Number of points to save the simulation data\n\n# Defining new parameters for more realistic simulation\ndensity_uniform = 1.5e3  # Uniform density of particles in kg/m^3\nmean_radius_nm = 50000  # Mean radius of particles in nanometers\nstd_dev_nm = 2  # Standard deviation of particle radius in nanometers\nsystem_temperature_kelvin = 300  # System temperature in Kelvin\nsystem_pressure_pascal = 1e5  # System pressure in Pascal\n\n# Define intervals for updating interaction factors\ninterval_friction_factor = 1000  # Interval for updating friction factor\ninterval_coagulation = 10  # Interval for updating coagulation\n\n# Generating particle masses using a log-normal distribution\nmass = particle_property.generate_particle_masses(\n    mean_radius=mean_radius_nm,\n    std_dev_radius=std_dev_nm,\n    density=density_uniform,\n    num_particles=TOTAL_NUMBER_OF_PARTICLES,\n    radius_input_units=\"nm\",\n)\n\n# Initializing particle velocities using thermal velocity\nvelocity = particle_property.random_thermal_velocity(\n    temperature_kelvin=system_temperature_kelvin,\n    mass_kg=mass,\n    number_of_particles=TOTAL_NUMBER_OF_PARTICLES,\n    t_type=t_type,\n    random_seed=seed,\n)\n\n# Setting constant density for all particles\ndensity = torch.ones(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * density_uniform\n\n# Initialize particle positions randomly within the cube\nposition = (\n    torch.rand(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * CUBE_SIDE\n    - CUBE_SIDE / 2\n)\n\n# Initialize force as zero for all particles\nforce = torch.zeros(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type)\n\n# Generating indices for particles\nindices = torch.arange(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type)  # Integer type\n\n# Calculating the total number of iterations for the simulation\ntotal_iterations = int(SIMULATION_TIME / TIME_STEP)\n\n# Initializing tensor to track total mass over iterations\ntotal_mass = torch.zeros(total_iterations, dtype=t_type)\n\n# Defining gravity acting on all particles\ngravity = (\n    torch.tensor([0, 0, -9.81])\n    .repeat(TOTAL_NUMBER_OF_PARTICLES, 1)\n    .transpose(0, 1)\n)\n\n# Initializing Arrays for Saving Position and Mass Data\nsave_position = np.zeros((3, TOTAL_NUMBER_OF_PARTICLES, save_points))\nsave_mass = np.zeros((TOTAL_NUMBER_OF_PARTICLES, save_points))\nsave_iterations = np.linspace(0, total_iterations, save_points, dtype=int)\n\n# Calculating particle radius from mass and density\nradius = particle_property.radius_calculation(mass=mass, density=density)\n</pre> # Setting up the Simulation Parameters and Initial Conditions  # Define fixed parameters TOTAL_NUMBER_OF_PARTICLES = 2000 TIME_STEP = 0.005 SIMULATION_TIME = 100 CUBE_SIDE = 0.010  # Size of the simulation cube in meters save_points = 50  # Number of points to save the simulation data  # Defining new parameters for more realistic simulation density_uniform = 1.5e3  # Uniform density of particles in kg/m^3 mean_radius_nm = 50000  # Mean radius of particles in nanometers std_dev_nm = 2  # Standard deviation of particle radius in nanometers system_temperature_kelvin = 300  # System temperature in Kelvin system_pressure_pascal = 1e5  # System pressure in Pascal  # Define intervals for updating interaction factors interval_friction_factor = 1000  # Interval for updating friction factor interval_coagulation = 10  # Interval for updating coagulation  # Generating particle masses using a log-normal distribution mass = particle_property.generate_particle_masses(     mean_radius=mean_radius_nm,     std_dev_radius=std_dev_nm,     density=density_uniform,     num_particles=TOTAL_NUMBER_OF_PARTICLES,     radius_input_units=\"nm\", )  # Initializing particle velocities using thermal velocity velocity = particle_property.random_thermal_velocity(     temperature_kelvin=system_temperature_kelvin,     mass_kg=mass,     number_of_particles=TOTAL_NUMBER_OF_PARTICLES,     t_type=t_type,     random_seed=seed, )  # Setting constant density for all particles density = torch.ones(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * density_uniform  # Initialize particle positions randomly within the cube position = (     torch.rand(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * CUBE_SIDE     - CUBE_SIDE / 2 )  # Initialize force as zero for all particles force = torch.zeros(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type)  # Generating indices for particles indices = torch.arange(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type)  # Integer type  # Calculating the total number of iterations for the simulation total_iterations = int(SIMULATION_TIME / TIME_STEP)  # Initializing tensor to track total mass over iterations total_mass = torch.zeros(total_iterations, dtype=t_type)  # Defining gravity acting on all particles gravity = (     torch.tensor([0, 0, -9.81])     .repeat(TOTAL_NUMBER_OF_PARTICLES, 1)     .transpose(0, 1) )  # Initializing Arrays for Saving Position and Mass Data save_position = np.zeros((3, TOTAL_NUMBER_OF_PARTICLES, save_points)) save_mass = np.zeros((TOTAL_NUMBER_OF_PARTICLES, save_points)) save_iterations = np.linspace(0, total_iterations, save_points, dtype=int)  # Calculating particle radius from mass and density radius = particle_property.radius_calculation(mass=mass, density=density) In\u00a0[133]: Copied! <pre># Plotting the Initial Radius Distribution\n\n# Calculate the initial radius of particles\nradius_initial = particle_property.radius_calculation(\n    mass=mass, density=density\n)\n\n# Plotting the histogram for radius distribution\nfig, ax = plt.subplots()\nax.hist(machine_limit.safe_log10(radius_initial), bins=20, range=(-9, 0))\nax.set_xlabel(\"log10 Radius (m)\")\nax.set_ylabel(\"Number of Particles\")\nax.set_title(\"Initial Particle Radius Distribution\")\nplt.show()\n\n# Plotting the Initial Velocity Distribution\n\n# Calculate the speeds of particles from their velocities\ninitial_speeds = particle_property.speed(velocity)\n\n# Plotting the histogram for velocity distribution\nfig, ax = plt.subplots()\nax.hist(machine_limit.safe_log10(initial_speeds), bins=20)\nax.set_xlabel(\"log10 Velocity (m/s)\")\nax.set_ylabel(\"Number of Particles\")\nax.set_title(\"Initial Particle Velocity Distribution\")\nplt.show()\n</pre> # Plotting the Initial Radius Distribution  # Calculate the initial radius of particles radius_initial = particle_property.radius_calculation(     mass=mass, density=density )  # Plotting the histogram for radius distribution fig, ax = plt.subplots() ax.hist(machine_limit.safe_log10(radius_initial), bins=20, range=(-9, 0)) ax.set_xlabel(\"log10 Radius (m)\") ax.set_ylabel(\"Number of Particles\") ax.set_title(\"Initial Particle Radius Distribution\") plt.show()  # Plotting the Initial Velocity Distribution  # Calculate the speeds of particles from their velocities initial_speeds = particle_property.speed(velocity)  # Plotting the histogram for velocity distribution fig, ax = plt.subplots() ax.hist(machine_limit.safe_log10(initial_speeds), bins=20) ax.set_xlabel(\"log10 Velocity (m/s)\") ax.set_ylabel(\"Number of Particles\") ax.set_title(\"Initial Particle Velocity Distribution\") plt.show() In\u00a0[136]: Copied! <pre># Printing the Maximum and Minimum Radii of the Particles\nprint(\"Max Radius [m]: \", radius.max())\nprint(\"Min Radius [m]: \", radius.min())\n\n# Initializing the Range of Radii for Friction Factor Calculation\n# We use a linspace to cover a range from the smallest to 10 times the\n# largest radius.\nfriction_factor_radius = torch.linspace(\n    start=radius.min(),\n    end=radius.max() * 10,\n    steps=1000,\n)\n\n# Calculating the Friction Factor\n# This calculation considers the radius, temperature, and pressure to\n# determine the friction factor.\nfriction_factor_array = particle_property.friction_factor_wrapper(\n    radius_meter=friction_factor_radius,\n    temperature_kelvin=system_temperature_kelvin,\n    pressure_pascal=system_pressure_pascal,\n)\n\n# Plotting Friction Factor vs Radius\n# The plot helps in visualizing how friction factor varies with particle\n# radius.\nfig, ax = plt.subplots()\nax.plot(friction_factor_radius, friction_factor_array)\nax.set_xscale(\"log\")\nax.set_yscale(\"log\")\nax.set_xlabel(\"Radius (m)\")\nax.set_ylabel(\"Friction Factor\")\nplt.show()\n</pre> # Printing the Maximum and Minimum Radii of the Particles print(\"Max Radius [m]: \", radius.max()) print(\"Min Radius [m]: \", radius.min())  # Initializing the Range of Radii for Friction Factor Calculation # We use a linspace to cover a range from the smallest to 10 times the # largest radius. friction_factor_radius = torch.linspace(     start=radius.min(),     end=radius.max() * 10,     steps=1000, )  # Calculating the Friction Factor # This calculation considers the radius, temperature, and pressure to # determine the friction factor. friction_factor_array = particle_property.friction_factor_wrapper(     radius_meter=friction_factor_radius,     temperature_kelvin=system_temperature_kelvin,     pressure_pascal=system_pressure_pascal, )  # Plotting Friction Factor vs Radius # The plot helps in visualizing how friction factor varies with particle # radius. fig, ax = plt.subplots() ax.plot(friction_factor_radius, friction_factor_array) ax.set_xscale(\"log\") ax.set_yscale(\"log\") ax.set_xlabel(\"Radius (m)\") ax.set_ylabel(\"Friction Factor\") plt.show() <pre>Max Radius [m]:  tensor(0.0006)\nMin Radius [m]:  tensor(6.2121e-06)\n</pre> In\u00a0[137]: Copied! <pre># Initialize Counter for Saving Data and Start Timer\nsave_counter = 0\nstart_time = time.time()\n\n# Initial Calculations Before Starting the Simulation Loop\nradius = particle_property.radius_calculation(mass=mass, density=density)\nfriction_factor_iter = particle_property.nearest_match(\n    x_values=friction_factor_radius,\n    y_values=friction_factor_array,\n    x_new=radius,\n)\n\n# Main Simulation Loop\nfor i in range(total_iterations):\n\n    # Coagulation Step (Optional, based on specified interval)\n    if i % interval_coagulation == 0:\n        # Update radius for collision detection\n        radius = particle_property.radius_calculation(\n            mass=mass, density=density\n        )\n        # Detect potential collision pairs\n        valid_collision_indices_pairs = particle_pairs.full_sweep_and_prune(\n            position=position, radius=radius\n        )\n\n        # Process collisions and update particle properties\n        if valid_collision_indices_pairs.shape[0] &gt; 0:\n            velocity, mass = collisions.coalescence(\n                position=position,\n                velocity=velocity,\n                mass=mass,\n                radius=radius,\n                collision_indices_pairs=valid_collision_indices_pairs,\n            )\n\n    # Update Friction Factor (Optional, based on specified interval)\n    if i % interval_friction_factor == 0:\n        radius = particle_property.radius_calculation(\n            mass=mass, density=density\n        )\n        friction_factor_iter = particle_property.nearest_match(\n            x_values=friction_factor_radius,\n            y_values=friction_factor_array,\n            x_new=radius,\n        )\n\n    # Calculate Forces (including updated friction factor)\n    force = mass * gravity - velocity * friction_factor_iter\n\n    # Integrate Equations of Motion (Leapfrog Method)\n    position, velocity = integration.leapfrog(\n        position=position,\n        velocity=velocity,\n        force=force,\n        mass=mass,\n        time_step=TIME_STEP,\n    )\n\n    # Apply Boundary Conditions (Cube Wrap-around)\n    position = boundary.wrapped_cube(position=position, cube_side=CUBE_SIDE)\n\n    # Save Data at Predefined Intervals\n    if i == save_iterations[save_counter]:\n        save_position[:, :, save_counter] = position.detach().numpy()\n        save_mass[:, save_counter] = mass.detach().numpy()\n        save_counter += 1\n\n# Final Data Save and Calculation of Total Simulation Time\nsave_position[:, :, -1] = position.detach().numpy()\nsave_mass[:, -1] = mass.detach().numpy()\nend_time = time.time()\nprint(f\"Total wall time: {end_time - start_time} seconds\")\nprint(\n    f\"Ratio of wall time to simulation time: {(end_time - start_time) / SIMULATION_TIME}\"\n)\n</pre> # Initialize Counter for Saving Data and Start Timer save_counter = 0 start_time = time.time()  # Initial Calculations Before Starting the Simulation Loop radius = particle_property.radius_calculation(mass=mass, density=density) friction_factor_iter = particle_property.nearest_match(     x_values=friction_factor_radius,     y_values=friction_factor_array,     x_new=radius, )  # Main Simulation Loop for i in range(total_iterations):      # Coagulation Step (Optional, based on specified interval)     if i % interval_coagulation == 0:         # Update radius for collision detection         radius = particle_property.radius_calculation(             mass=mass, density=density         )         # Detect potential collision pairs         valid_collision_indices_pairs = particle_pairs.full_sweep_and_prune(             position=position, radius=radius         )          # Process collisions and update particle properties         if valid_collision_indices_pairs.shape[0] &gt; 0:             velocity, mass = collisions.coalescence(                 position=position,                 velocity=velocity,                 mass=mass,                 radius=radius,                 collision_indices_pairs=valid_collision_indices_pairs,             )      # Update Friction Factor (Optional, based on specified interval)     if i % interval_friction_factor == 0:         radius = particle_property.radius_calculation(             mass=mass, density=density         )         friction_factor_iter = particle_property.nearest_match(             x_values=friction_factor_radius,             y_values=friction_factor_array,             x_new=radius,         )      # Calculate Forces (including updated friction factor)     force = mass * gravity - velocity * friction_factor_iter      # Integrate Equations of Motion (Leapfrog Method)     position, velocity = integration.leapfrog(         position=position,         velocity=velocity,         force=force,         mass=mass,         time_step=TIME_STEP,     )      # Apply Boundary Conditions (Cube Wrap-around)     position = boundary.wrapped_cube(position=position, cube_side=CUBE_SIDE)      # Save Data at Predefined Intervals     if i == save_iterations[save_counter]:         save_position[:, :, save_counter] = position.detach().numpy()         save_mass[:, save_counter] = mass.detach().numpy()         save_counter += 1  # Final Data Save and Calculation of Total Simulation Time save_position[:, :, -1] = position.detach().numpy() save_mass[:, -1] = mass.detach().numpy() end_time = time.time() print(f\"Total wall time: {end_time - start_time} seconds\") print(     f\"Ratio of wall time to simulation time: {(end_time - start_time) / SIMULATION_TIME}\" ) <pre>Total wall time: 22.354511499404907 seconds\nRatio of wall time to simulation time: 0.22354511499404908\n</pre> In\u00a0[130]: Copied! <pre># Processing the Final Data for Visualization\n\n# Select the final time index for the data\ntime_index = -1\nposition_final = save_position[:, :, time_index]\nmass_final = save_mass[:, time_index]\n\n# Filter out particles with zero mass\nfilter_zero_mass = mass_final &gt; 0\n\n# Calculate the radius and area of each particle\nradius_final = particle_property.radius_calculation(\n    mass=mass_final, density=density\n)\nparticle_area = np.pi * radius_final**2\n# normalize the area\nparticle_area = particle_area / particle_area.max()\n\n\n# Display the number of remaining particles\nprint(f\"Number of particles at the end: {filter_zero_mass.sum()}\")\n\n# Creating a 3D Plot for Visualization\nfig = plt.figure()\nax = fig.add_subplot(projection=\"3d\")\n\n# Choose a color map for the scatter plot\ncmap = plt.cm.viridis\n\n# Plot the final positions of particles with non-zero mass\nscatter_plot = ax.scatter(\n    position_final[0, filter_zero_mass],\n    position_final[1, filter_zero_mass],\n    position_final[2, filter_zero_mass],\n    c=mass_final[filter_zero_mass],\n    cmap=cmap,\n    s=particle_area[filter_zero_mass]\n    * 100,  # scalled Particle size based on normalized area\n)\n\n# # Set axis limits based on cube dimensions\nax.set_xlim(-CUBE_SIDE / 2, CUBE_SIDE / 2)\nax.set_ylim(-CUBE_SIDE / 2, CUBE_SIDE / 2)\nax.set_zlim(-CUBE_SIDE / 2, CUBE_SIDE / 2)\n\n# Add a color bar indicating particle mass\ncolor_bar = plt.colorbar(scatter_plot, ax=ax)\ncolor_bar.set_label(\"Mass [kg]\")\n\n# Show the plot with optimized layout\nplt.show()\nfig.tight_layout()\n</pre> # Processing the Final Data for Visualization  # Select the final time index for the data time_index = -1 position_final = save_position[:, :, time_index] mass_final = save_mass[:, time_index]  # Filter out particles with zero mass filter_zero_mass = mass_final &gt; 0  # Calculate the radius and area of each particle radius_final = particle_property.radius_calculation(     mass=mass_final, density=density ) particle_area = np.pi * radius_final**2 # normalize the area particle_area = particle_area / particle_area.max()   # Display the number of remaining particles print(f\"Number of particles at the end: {filter_zero_mass.sum()}\")  # Creating a 3D Plot for Visualization fig = plt.figure() ax = fig.add_subplot(projection=\"3d\")  # Choose a color map for the scatter plot cmap = plt.cm.viridis  # Plot the final positions of particles with non-zero mass scatter_plot = ax.scatter(     position_final[0, filter_zero_mass],     position_final[1, filter_zero_mass],     position_final[2, filter_zero_mass],     c=mass_final[filter_zero_mass],     cmap=cmap,     s=particle_area[filter_zero_mass]     * 100,  # scalled Particle size based on normalized area )  # # Set axis limits based on cube dimensions ax.set_xlim(-CUBE_SIDE / 2, CUBE_SIDE / 2) ax.set_ylim(-CUBE_SIDE / 2, CUBE_SIDE / 2) ax.set_zlim(-CUBE_SIDE / 2, CUBE_SIDE / 2)  # Add a color bar indicating particle mass color_bar = plt.colorbar(scatter_plot, ax=ax) color_bar.set_label(\"Mass [kg]\")  # Show the plot with optimized layout plt.show() fig.tight_layout() <pre>Number of particles at the end: 1260\n</pre> In\u00a0[138]: Copied! <pre># Visualizing the Mass Distribution at Different Stages\n\n# plot initial radius distribution\nfig, ax = plt.subplots()\nax.hist(\n    machine_limit.safe_log10(radius_initial),\n    bins=20,\n    range=(-6, -2),\n    label=\"Initial\",\n    alpha=0.25,\n)\nax.hist(\n    machine_limit.safe_log10(radius_final[filter_zero_mass]),\n    bins=20,\n    range=(-6, -2),\n    label=\"Final\",\n    alpha=0.5,\n)\nax.set_yscale(\"log\")\nax.set_xlabel(\"log10 Radius (m)\")\nax.set_ylabel(\"Number of Particles\")\nax.set_title(\"Initial Particle Radius Distribution\")\nax.legend()\nplt.show()\n\nprint(\"Initial Max Radius [m]: \", radius_initial.max().item())\nprint(\"Final Max Radius [m]: \", radius_final[filter_zero_mass].max().item())\n</pre> # Visualizing the Mass Distribution at Different Stages  # plot initial radius distribution fig, ax = plt.subplots() ax.hist(     machine_limit.safe_log10(radius_initial),     bins=20,     range=(-6, -2),     label=\"Initial\",     alpha=0.25, ) ax.hist(     machine_limit.safe_log10(radius_final[filter_zero_mass]),     bins=20,     range=(-6, -2),     label=\"Final\",     alpha=0.5, ) ax.set_yscale(\"log\") ax.set_xlabel(\"log10 Radius (m)\") ax.set_ylabel(\"Number of Particles\") ax.set_title(\"Initial Particle Radius Distribution\") ax.legend() plt.show()  print(\"Initial Max Radius [m]: \", radius_initial.max().item()) print(\"Final Max Radius [m]: \", radius_final[filter_zero_mass].max().item()) <pre>Initial Max Radius [m]:  0.0005869632004760206\nFinal Max Radius [m]:  0.0007475933229391738\n</pre>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#realistic-drag-mass-and-velocity","title":"Realistic Drag, Mass, and Velocity\u00b6","text":"<p>This notebook section focuses on simulating cloud droplet-sized aerosol particles (around 100 microns) with accurate mass and velocity. Key improvements include:</p> <ol> <li>Refining Initial Conditions: Setting precise starting values for particle mass and velocity.</li> <li>Modeling Forces Accurately: Better representation of forces acting on particles to influence their movement.</li> </ol> <p>These steps are crucial for enhancing the realism of the aerosol particle model.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#initialization-of-particles","title":"Initialization of Particles\u00b6","text":"<p>In this section, we'll enhance particle initialization for realism:</p> <ul> <li>Particle Masses: Implementing a narrow log-normal distribution to better represent real-world mass variation.</li> <li>Particle Velocities: Utilizing thermal velocity, influenced by Brownian motion, for initial velocity settings.</li> <li>Initial Positions: Particles will be randomly placed within a defined box area to simulate natural distribution.</li> </ul>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#plotting-initial-conditions","title":"Plotting Initial Conditions\u00b6","text":"<p>To verify the realism of our particle model, we'll plot the initial distributions of particle size and velocity. This step ensures that the initial setup aligns with our expectations and the intended simulation parameters.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#particle-size-distribution","title":"Particle Size Distribution\u00b6","text":"<p>First, we examine the distribution of particle radii. The goal is to confirm that the radii follow the specified log-normal distribution.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#particle-velocity-distribution","title":"Particle Velocity Distribution\u00b6","text":"<p>Next, we assess the distribution of particle velocities. This is crucial for ensuring that the initial velocities reflect thermal motion as intended.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#friction-factor-in-particle-dynamics","title":"Friction Factor in Particle Dynamics\u00b6","text":""},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#understanding-the-friction-force","title":"Understanding the Friction Force\u00b6","text":"<p>As we simulate particles at a realistic scale, accounting for forces acting on them becomes crucial. The first significant force is friction, which opposes the particle's motion and is proportional to its velocity.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#mathematical-representation","title":"Mathematical Representation\u00b6","text":"<p>The friction factor $(f_p)$ is described by:</p> <p>$$ f_p = \\frac{3 \\pi D_p \\mu}{D_c} $$</p> <p>where $D_p$ is the particle diameter, $\\mu$ is the dynamic viscosity, and $D_c$ is the diffusion coefficient.</p> <p>The drag force, $F_d$, is then:</p> <p>$$ F_d = f_p v_p $$</p> <p>with $v_p$ representing the particle velocity.</p> <p>Considering gravitational settling in a fluid at rest $(u_{fluid} = 0)$, the motion equation becomes:</p> <p>$$ m_p \\frac{dv}{dt} = m_pg + f_p (u_{fluid}-v_p) $$</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#implementing-the-friction-factor-in-the-simulation","title":"Implementing the Friction Factor in the Simulation\u00b6","text":""},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#efficiency-considerations","title":"Efficiency Considerations\u00b6","text":"<p>Calculating the friction factor at each simulation step could be computationally intensive. However, it's important to note that $f_p$ is dependent on environmental variables like pressure and temperature, which might change.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#current-approach","title":"Current Approach\u00b6","text":"<p>For the current stage of the simulation, we'll compute $f_p$ initially and use nearest neighbor interpolation for each particle. This approach balances efficiency with accuracy, keeping in mind that future iterations may include dynamic adjustments for varying environmental conditions.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#running-the-particle-simulation","title":"Running the Particle Simulation\u00b6","text":"<p>In this part of the notebook, we execute the core simulation loop. This involves integrating the motion of the particles over time while considering the forces acting on them, including friction and gravity. Several key updates and optimizations are included:</p> <ol> <li><p>Friction Factor Initialization: Before entering the main loop, the friction factor for each particle is calculated. This is an essential step for accurately simulating the drag force on particles.</p> </li> <li><p>Conditional Coagulation Step: The simulation includes an optional coagulation step, executed at a specified interval. This step simulates the merging of particles upon collision, affecting their mass and velocity.</p> </li> <li><p>Friction Factor Update: The friction factor is updated at regular intervals to reflect changes in particle size or other environmental conditions. This step ensures that the drag force remains accurate throughout the simulation.</p> </li> <li><p>Force Calculation and Integration: Within each iteration, we calculate the forces acting on the particles and update their positions and velocities using the leapfrog integration method.</p> </li> <li><p>Boundary Conditions and Data Saving: The simulation includes boundary conditions to mimic a wrapped cube environment. Additionally, particle position and mass data are saved at predefined intervals for analysis.</p> </li> </ol> <p>This simulation loop provides a comprehensive and dynamic model of particle motion, taking into account physical factors and interactions.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#plotting-particle-positions","title":"Plotting Particle Positions\u00b6","text":"<p>After running the simulation, we'll plot the particle positions to visualize their movement.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#size-distribution-of-particles","title":"Size Distribution of Particles\u00b6","text":"<p>Given particle collision and coagulation, we'll plot the distribution of particle sizes at the end of the simulation. This step ensures that the particle size distribution aligns with our expectations and the intended simulation parameters. That being, both a decrease in the number of particles and an increase in the average particle size.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#summary-of-cloud-droplet-simulation","title":"Summary of Cloud Droplet Simulation\u00b6","text":""},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#achievements-in-realism","title":"Achievements in Realism\u00b6","text":"<p>In this iteration of the cloud droplet simulation, we've significantly advanced towards realism:</p> <ul> <li>Enhanced Initial Conditions: The simulation now starts with more realistically distributed particle sizes and velocities.</li> <li>Inclusion of Friction Force: We've incorporated the friction force, an essential factor in accurately simulating particle dynamics.</li> <li>Optional Coagulation: The simulation allows for the inclusion of coagulation, further adding to its complexity and realism.</li> </ul>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#performance-considerations","title":"Performance Considerations\u00b6","text":"<p>While these enhancements bring us closer to real-world scenarios, they also slow down the simulation. However, it still runs at a reasonable pace relative to real time.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#challenges-with-sub-micron-aerosols","title":"Challenges with Sub-Micron Aerosols\u00b6","text":"<p>The next goal is to simulate sub-micron aerosol sizes accurately. The main challenge here lies in particle velocity:</p> <ul> <li>High Speeds and Domain Size: At sub-micron scales, particles can travel multiple domain lengths in a single time step. This necessitates extremely small time steps for accuracy, which is computationally inefficient.</li> <li>Domain Wrapping Issue: As observed in the final position plot, larger droplets end up at the domain's top due to their high speeds and the wrap-around nature of the domain. This indicates the need for a different approach to handle fast-moving particles.</li> </ul>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#positive-observations","title":"Positive Observations\u00b6","text":"<p>Despite these challenges, there are encouraging signs:</p> <ul> <li>Evolving Size Distribution: The simulation shows a trend towards fewer, larger particles over time. This behavior mirrors what is expected in cloud formation, where larger particles are more likely to collide and grow.</li> </ul>"},{"location":"How-To-Guides/Lagrangian/notebooks/friction_force/#conclusion","title":"Conclusion\u00b6","text":"<p>These observations suggest that we are moving in the right direction towards a realistic and efficient simulation of particle dynamics. Future improvements will focus on addressing the challenges with sub-micron aerosols and further refining the simulation model.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/sweep_and_prune/","title":"Sweep and Prune Algorithm","text":"In\u00a0[41]: Copied! <pre># Code Section: Importing Necessary Libraries and Initializing Variables\nimport time\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom particula_beta.lagrangian import (\n    boundary,\n    integration,\n    collisions,\n    particle_property,\n    particle_pairs,\n)\n\n# device = torch.device('cuda')  # or 'cuda:0' to specify a particular GPU\n# torch.set_default_device(device)\n\n\n# Initializing the Torch Generator and setting the data type\ntorch.manual_seed(1234)  # Set the seed for reproducibility\nt_type = torch.float32\n</pre> # Code Section: Importing Necessary Libraries and Initializing Variables import time import torch import numpy as np import matplotlib.pyplot as plt from particula_beta.lagrangian import (     boundary,     integration,     collisions,     particle_property,     particle_pairs, )  # device = torch.device('cuda')  # or 'cuda:0' to specify a particular GPU # torch.set_default_device(device)   # Initializing the Torch Generator and setting the data type torch.manual_seed(1234)  # Set the seed for reproducibility t_type = torch.float32 In\u00a0[42]: Copied! <pre># Setting up the Simulation Parameters and Initial Conditions\n# Define fixed parameters\nTOTAL_NUMBER_OF_PARTICLES = 500\nTIME_STEP = 0.01\nSIMULATION_TIME = 10\nMASS = 3\nCUBE_SIDE = 50\nspeed = 5\nsave_points = 50\n\n# Initialize particle positions randomly within the cube\nposition = (\n    torch.rand(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * CUBE_SIDE\n    - CUBE_SIDE / 2\n)\n\n# Initialize particle velocities randomly\nvelocity = (\n    torch.rand(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * speed - speed / 2\n)\n\n# Initialize force as zero for all particles\nforce = torch.zeros(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type)\n\n# Set constant mass and density for all particles\nmass = torch.ones(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * MASS\ndensity = torch.ones(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * 1\n\n# Generate indices for particles, could be integer type\nindices = torch.arange(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type)  # could be int\n\n# Calculate the total number of iterations for the simulation\ntotal_iterations = int(SIMULATION_TIME / TIME_STEP)\n\n# Initialize a tensor to track the total mass over iterations\ntotal_mass = torch.zeros(total_iterations, dtype=t_type)\n\n# Define gravity acting on all particles\ngravity = (\n    torch.tensor([0, -9.81, 0])\n    .repeat(TOTAL_NUMBER_OF_PARTICLES, 1)\n    .transpose(0, 1)\n)\n\n# Initializing Arrays for Saving Position and Mass Data\n# Create arrays to store position and mass data at each save point\nsave_position = np.zeros((3, TOTAL_NUMBER_OF_PARTICLES, save_points))\nsave_mass = np.zeros((TOTAL_NUMBER_OF_PARTICLES, save_points))\n\n# Determine which iterations will correspond to the save points\nsave_iterations = np.linspace(0, total_iterations, save_points, dtype=int)\nradius = particle_property.radius_calculation(mass=mass, density=density)\n</pre> # Setting up the Simulation Parameters and Initial Conditions # Define fixed parameters TOTAL_NUMBER_OF_PARTICLES = 500 TIME_STEP = 0.01 SIMULATION_TIME = 10 MASS = 3 CUBE_SIDE = 50 speed = 5 save_points = 50  # Initialize particle positions randomly within the cube position = (     torch.rand(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * CUBE_SIDE     - CUBE_SIDE / 2 )  # Initialize particle velocities randomly velocity = (     torch.rand(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * speed - speed / 2 )  # Initialize force as zero for all particles force = torch.zeros(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type)  # Set constant mass and density for all particles mass = torch.ones(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * MASS density = torch.ones(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * 1  # Generate indices for particles, could be integer type indices = torch.arange(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type)  # could be int  # Calculate the total number of iterations for the simulation total_iterations = int(SIMULATION_TIME / TIME_STEP)  # Initialize a tensor to track the total mass over iterations total_mass = torch.zeros(total_iterations, dtype=t_type)  # Define gravity acting on all particles gravity = (     torch.tensor([0, -9.81, 0])     .repeat(TOTAL_NUMBER_OF_PARTICLES, 1)     .transpose(0, 1) )  # Initializing Arrays for Saving Position and Mass Data # Create arrays to store position and mass data at each save point save_position = np.zeros((3, TOTAL_NUMBER_OF_PARTICLES, save_points)) save_mass = np.zeros((TOTAL_NUMBER_OF_PARTICLES, save_points))  # Determine which iterations will correspond to the save points save_iterations = np.linspace(0, total_iterations, save_points, dtype=int) radius = particle_property.radius_calculation(mass=mass, density=density) In\u00a0[43]: Copied! <pre># Initialize counter for saving data\nsave_counter = 0\nstart_time = time.time()\n\nfor i in range(total_iterations):\n\n    # NEW calculate sweep and prune collision pairs\n    radius = particle_property.radius_calculation(mass=mass, density=density)\n    valid_collision_indices_pairs = particle_pairs.full_sweep_and_prune(\n        position=position, radius=radius\n    )\n\n    if valid_collision_indices_pairs.shape[0] &gt; 0:\n        # Coalesce particles that have collided and update their velocity and mass\n        velocity, mass = collisions.coalescence(\n            position=position,\n            velocity=velocity,\n            mass=mass,\n            radius=radius,\n            collision_indices_pairs=valid_collision_indices_pairs,\n        )\n\n    # Calculate the force acting on the particles (e.g., gravity)\n    force = mass * gravity\n\n    # Integrate the equations of motion to update position and velocity\n    position, velocity = integration.leapfrog(\n        position=position,\n        velocity=velocity,\n        force=force,\n        mass=mass,\n        time_step=TIME_STEP,\n    )\n\n    # Apply boundary conditions for the cube (wrap-around)\n    position = boundary.wrapped_cube(position=position, cube_side=CUBE_SIDE)\n\n    # Save the position and mass data at designated save points\n    if i == save_iterations[save_counter]:\n        save_position[:, :, save_counter] = position.detach().cpu().numpy()\n        save_mass[:, save_counter] = mass.detach().cpu().numpy()\n        save_counter += 1\n\n# Perform a final save of the position and mass data\nsave_position[:, :, -1] = position.detach().cpu().numpy()\nsave_mass[:, -1] = mass.detach().cpu().numpy()\n\n# Calculate the total simulation time\nend_time = time.time()\nprint(f\"Total wall time: {end_time - start_time} seconds\")\nprint(\n    f\"Ratio of wall time to simulation time: {(end_time - start_time) / SIMULATION_TIME}\"\n)\n</pre> # Initialize counter for saving data save_counter = 0 start_time = time.time()  for i in range(total_iterations):      # NEW calculate sweep and prune collision pairs     radius = particle_property.radius_calculation(mass=mass, density=density)     valid_collision_indices_pairs = particle_pairs.full_sweep_and_prune(         position=position, radius=radius     )      if valid_collision_indices_pairs.shape[0] &gt; 0:         # Coalesce particles that have collided and update their velocity and mass         velocity, mass = collisions.coalescence(             position=position,             velocity=velocity,             mass=mass,             radius=radius,             collision_indices_pairs=valid_collision_indices_pairs,         )      # Calculate the force acting on the particles (e.g., gravity)     force = mass * gravity      # Integrate the equations of motion to update position and velocity     position, velocity = integration.leapfrog(         position=position,         velocity=velocity,         force=force,         mass=mass,         time_step=TIME_STEP,     )      # Apply boundary conditions for the cube (wrap-around)     position = boundary.wrapped_cube(position=position, cube_side=CUBE_SIDE)      # Save the position and mass data at designated save points     if i == save_iterations[save_counter]:         save_position[:, :, save_counter] = position.detach().cpu().numpy()         save_mass[:, save_counter] = mass.detach().cpu().numpy()         save_counter += 1  # Perform a final save of the position and mass data save_position[:, :, -1] = position.detach().cpu().numpy() save_mass[:, -1] = mass.detach().cpu().numpy()  # Calculate the total simulation time end_time = time.time() print(f\"Total wall time: {end_time - start_time} seconds\") print(     f\"Ratio of wall time to simulation time: {(end_time - start_time) / SIMULATION_TIME}\" ) <pre>Total wall time: 8.405936002731323 seconds\nRatio of wall time to simulation time: 0.8405936002731323\n</pre> In\u00a0[44]: Copied! <pre># Processing the Final Data for Visualization\n\n# Select the final time index for the data\ntime_index = -2\nposition_final = save_position[:, :, time_index]\nmass_final = save_mass[:, time_index]\n\n# Filter out particles with zero mass\nfilter_zero_mass = mass_final &gt; 0\n\n# Calculate the radius and area of each particle\nradius_final = (3 * mass_final / (4 * np.pi * density.cpu().numpy())) ** (\n    1 / 3\n)\nparticle_area = np.pi * radius_final**2\n\n# Display the number of remaining particles\nprint(f\"Number of particles at the end: {filter_zero_mass.sum()}\")\n\n# Creating a 3D Plot for Visualization\nfig = plt.figure()\nax = fig.add_subplot(projection=\"3d\")\n\n# Choose a color map for the scatter plot\ncmap = plt.cm.viridis\n\n# Plot the final positions of particles with non-zero mass\nscatter_plot = ax.scatter(\n    position_final[0, filter_zero_mass],\n    position_final[1, filter_zero_mass],\n    position_final[2, filter_zero_mass],\n    c=mass_final[filter_zero_mass],\n    cmap=cmap,\n    s=particle_area[filter_zero_mass],  # Particle size based on area\n)\n\n# Set axis limits based on cube dimensions\nax.set_xlim(-CUBE_SIDE / 2, CUBE_SIDE / 2)\nax.set_ylim(-CUBE_SIDE / 2, CUBE_SIDE / 2)\nax.set_zlim(-CUBE_SIDE / 2, CUBE_SIDE / 2)\n\n# Add a color bar indicating particle mass\ncolor_bar = plt.colorbar(scatter_plot, ax=ax)\ncolor_bar.set_label(\"Mass\")\n\n# Show the plot with optimized layout\nplt.show()\nfig.tight_layout()\n</pre> # Processing the Final Data for Visualization  # Select the final time index for the data time_index = -2 position_final = save_position[:, :, time_index] mass_final = save_mass[:, time_index]  # Filter out particles with zero mass filter_zero_mass = mass_final &gt; 0  # Calculate the radius and area of each particle radius_final = (3 * mass_final / (4 * np.pi * density.cpu().numpy())) ** (     1 / 3 ) particle_area = np.pi * radius_final**2  # Display the number of remaining particles print(f\"Number of particles at the end: {filter_zero_mass.sum()}\")  # Creating a 3D Plot for Visualization fig = plt.figure() ax = fig.add_subplot(projection=\"3d\")  # Choose a color map for the scatter plot cmap = plt.cm.viridis  # Plot the final positions of particles with non-zero mass scatter_plot = ax.scatter(     position_final[0, filter_zero_mass],     position_final[1, filter_zero_mass],     position_final[2, filter_zero_mass],     c=mass_final[filter_zero_mass],     cmap=cmap,     s=particle_area[filter_zero_mass],  # Particle size based on area )  # Set axis limits based on cube dimensions ax.set_xlim(-CUBE_SIDE / 2, CUBE_SIDE / 2) ax.set_ylim(-CUBE_SIDE / 2, CUBE_SIDE / 2) ax.set_zlim(-CUBE_SIDE / 2, CUBE_SIDE / 2)  # Add a color bar indicating particle mass color_bar = plt.colorbar(scatter_plot, ax=ax) color_bar.set_label(\"Mass\")  # Show the plot with optimized layout plt.show() fig.tight_layout() <pre>Number of particles at the end: 367\n</pre> In\u00a0[45]: Copied! <pre># Visualizing the Mass Distribution at Different Stages\n\n# Create a new figure for the histogram\nfig = plt.figure()\nax = fig.add_subplot()\n# Normalizing by initial MASS to observe distribution changes\nnormalized_mass = save_mass[filter_zero_mass, :] / MASS\nmax_mass = normalized_mass.max()\n\n# Plot histograms of mass distribution at different stages\nax.hist(\n    normalized_mass[:, 0],\n    bins=25,\n    alpha=0.8,\n    label=\"Initial\",\n    range=(0, max_mass),\n)\nax.hist(\n    normalized_mass[:, 24],\n    bins=25,\n    alpha=0.6,\n    label=\"Middle\",\n    range=(0, max_mass),\n)\nax.hist(\n    normalized_mass[:, -1],\n    bins=25,\n    alpha=0.5,\n    label=\"Final\",\n    range=(0, max_mass),\n)\n\n# Setting labels and title for the plot\nax.set_xlabel(\"Mass / Initial MASS\")\nax.set_ylabel(\"Number of Particles\")\n\n# Add a legend to the plot\nax.legend()\n\n# Display the plot\nplt.show()\n\n# Adjust layout for optimal visualization\nfig.tight_layout()\n</pre> # Visualizing the Mass Distribution at Different Stages  # Create a new figure for the histogram fig = plt.figure() ax = fig.add_subplot() # Normalizing by initial MASS to observe distribution changes normalized_mass = save_mass[filter_zero_mass, :] / MASS max_mass = normalized_mass.max()  # Plot histograms of mass distribution at different stages ax.hist(     normalized_mass[:, 0],     bins=25,     alpha=0.8,     label=\"Initial\",     range=(0, max_mass), ) ax.hist(     normalized_mass[:, 24],     bins=25,     alpha=0.6,     label=\"Middle\",     range=(0, max_mass), ) ax.hist(     normalized_mass[:, -1],     bins=25,     alpha=0.5,     label=\"Final\",     range=(0, max_mass), )  # Setting labels and title for the plot ax.set_xlabel(\"Mass / Initial MASS\") ax.set_ylabel(\"Number of Particles\")  # Add a legend to the plot ax.legend()  # Display the plot plt.show()  # Adjust layout for optimal visualization fig.tight_layout() In\u00a0[46]: Copied! <pre>## sweep and prune along one axis\nhelp(particle_pairs.single_axis_sweep_and_prune)\n</pre> ## sweep and prune along one axis help(particle_pairs.single_axis_sweep_and_prune) <pre>Help on function single_axis_sweep_and_prune in module particula_beta.lagrangian.particle_pairs:\n\nsingle_axis_sweep_and_prune(position_axis: torch.Tensor, radius: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]\n    Sweep and prune algorithm for collision detection along a single axis.\n    This function identifies pairs of particles that are close enough to\n    potentially collide.\n    \n    Args:\n        position_axis (torch.Tensor): The position of particles along a single\n            axis.\n        radius (torch.Tensor): The radius of particles.\n    \n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: Two tensors containing the indices\n        of potentially colliding particles.\n\n</pre>"},{"location":"How-To-Guides/Lagrangian/notebooks/sweep_and_prune/#sweep-and-prune-algorithm","title":"Sweep and Prune Algorithm\u00b6","text":"<p>In our previous approach to collision detection, we compared each particle with every other particle, resulting in a computationally expensive $O(N^2)$ operation, where $N$ is the number of particles. To enhance efficiency, we now implement the sweep and prune algorithm.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/sweep_and_prune/#concept-of-sweep-and-prune","title":"Concept of Sweep and Prune\u00b6","text":"<p>The sweep and prune algorithm is a more efficient method for collision detection. Its efficiency stems from reducing the number of particle pairs we need to check. The key steps of this algorithm are:</p> <p>Sweep: Sort the particles along one axis, typically the $x$-axis. This step orders the particles in a way that adjacent particles in the sorted list are likely to be close in space.</p> <p>Prune: After sorting, we then check for potential collisions only between neighboring particles in this sorted list. This pruning significantly reduces the number of comparisons.</p> <p>Repeat for Other Axes: The process is repeated for the $y$-axis and the $z$-axis. In each iteration, we eliminate pairs of particles that are too far apart to collide based on their coordinates in the respective axis.</p> <p>Euclidean Distance Check: Finally, we validate potential collisions by checking the actual Euclidean distance between particle pairs that remained after the pruning steps. This step ensures accurate detection of collisions.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/sweep_and_prune/#efficiency-gains","title":"Efficiency Gains\u00b6","text":"<p>This method dramatically reduces the number of comparisons from $O(N^2)$ to approximately $O(N \\log N)$, mainly due to sorting. The actual number of particle pairs checked for collision is usually much less than $N$, especially in sparsely populated spaces. This makes the sweep and prune algorithm significantly more efficient for collision detection in systems with a large number of particles.</p> <p>Note: The exact efficiency depends on the distribution and density of particles. In cases where particles are uniformly distributed, the sweep and prune method shows substantial efficiency improvements. However, in highly clustered scenarios, the performance gain might be less pronounced, but it still outperforms the brute-force $N^2$ approach.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/sweep_and_prune/#initialization","title":"Initialization\u00b6","text":"<p>We'll use the same initialization as in the previous notebook.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/sweep_and_prune/#plot-positions","title":"Plot Positions\u00b6","text":"<p>Compared to the previous notebook, the sweep and prune is about 2x faster for this scenario. The exact speedup depends on the number of particles and the distribution of particles in space.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/sweep_and_prune/#analyzing-the-mass-distribution-of-particles","title":"Analyzing the Mass Distribution of Particles\u00b6","text":""},{"location":"How-To-Guides/Lagrangian/notebooks/sweep_and_prune/#summary-of-the-sweep-and-prune-algorithm","title":"Summary of the Sweep and Prune Algorithm\u00b6","text":"<p>In our recent work, we've successfully implemented the sweep and prune algorithm for efficient collision detection. This method has demonstrated a notable efficiency improvement over the traditional brute-force approach. As we progress, the next notebook will introduce realistic aerosol initialization and dynamics, further enhancing our simulation's applicability.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/sweep_and_prune/#understanding-sweep-and-prune-aabb-algorithm","title":"Understanding Sweep and Prune (AABB Algorithm)\u00b6","text":"<p>The sweep and prune algorithm is often referred to as the Axis-Aligned Bounding Box (AABB) algorithm. This designation stems from the technique's reliance on comparing axis-aligned bounding boxes of particles to detect potential collisions. An axis-aligned bounding box is the smallest box that entirely encapsulates a particle and is aligned with the coordinate axes. By working with these boxes, the algorithm efficiently narrows down collision checks to only those particles whose bounding boxes overlap, significantly reducing the number of necessary calculations.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/sweep_and_prune/#additional-context-on-aabb","title":"Additional Context on AABB\u00b6","text":"<ul> <li>Efficient Sorting: The key to its efficiency lies in sorting particles along each axis and then checking for overlap, which is computationally less intensive than checking every pair of particles.</li> <li>Use in Various Domains: While commonly used in computer graphics and game development for spatial partitioning and collision detection, the AABB approach is also highly relevant in scientific simulations like aerosol dynamics.</li> </ul>"},{"location":"How-To-Guides/Lagrangian/notebooks/sweep_and_prune/#exploring-other-collision-detection-methods","title":"Exploring Other Collision Detection Methods\u00b6","text":"<p>Beyond sweep and prune, several other advanced collision detection techniques offer improved efficiency over brute-force methods, albeit with increased complexity in implementation. Some noteworthy methods include:</p> <ul> <li>K-d Tree (Smarter Space Partitioning): This method involves partitioning space into regions using a k-dimensional tree, allowing for efficient searching and nearest-neighbor queries, particularly useful in sparsely populated spaces.</li> <li>Bounding Volume Hierarchy (BVH): BVH involves creating a tree structure of bounding volumes, where each node encompasses a subset of objects in the space. Collision detection then proceeds by traversing this tree, significantly speeding up the process in complex scenes.</li> <li>GJK Algorithm (Gilbert-Johnson-Keerthi): The GJK algorithm is used for collision detection between convex shapes. It efficiently determines whether two convex shapes intersect and can be extended to calculate the minimum distance between them.</li> </ul>"},{"location":"How-To-Guides/Lagrangian/notebooks/sweep_and_prune/#additional-considerations","title":"Additional Considerations\u00b6","text":"<ul> <li>Choosing the Right Method: The choice of collision detection algorithm depends on the specific requirements of the simulation, such as the number of particles, their distribution, and the required accuracy.</li> <li>Hybrid Approaches: In practice, a combination of these methods can be used, depending on the scale and complexity of the simulation environment. By incorporating these advanced collision detection methods, we can further optimize our simulations for more realistic and computationally efficient aerosol behavior modeling.</li> </ul>"},{"location":"How-To-Guides/Lagrangian/notebooks/turbulence_drops/","title":"Realistic Drag, Mass, and Velocity","text":"In\u00a0[1]: Copied! <pre># Code Section: Importing Necessary Libraries and Initializing Variables\nimport time\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom particula_beta.lagrangian import (\n    boundary,\n    integration,\n    collisions,\n    particle_property,\n    particle_pairs,\n)\nfrom particula.util import machine_limit\n\ndevice = torch.device(\"cuda\")  # or 'cuda:0' to specify a particular GPU\ntorch.set_default_device(device)\n\n\n# Initializing the Torch Generator and setting the data type\nseed = 1234\ntorch.manual_seed(seed)  # Set the seed for reproducibility\nt_type = torch.float32\n\n# print particula version\nfrom particula import __version__\n\nprint(\"particula version: \", __version__)\n</pre> # Code Section: Importing Necessary Libraries and Initializing Variables import time import torch import numpy as np import matplotlib.pyplot as plt from particula_beta.lagrangian import (     boundary,     integration,     collisions,     particle_property,     particle_pairs, ) from particula.util import machine_limit  device = torch.device(\"cuda\")  # or 'cuda:0' to specify a particular GPU torch.set_default_device(device)   # Initializing the Torch Generator and setting the data type seed = 1234 torch.manual_seed(seed)  # Set the seed for reproducibility t_type = torch.float32  # print particula version from particula import __version__  print(\"particula version: \", __version__) <pre>particula version:  0.0.17.dev0\n</pre> In\u00a0[\u00a0]: Copied! <pre># Setting up the Simulation Parameters and Initial Conditions\n\n# Define fixed parameters\nTOTAL_NUMBER_OF_PARTICLES = 100\nTIME_STEP = 0.005\nSIMULATION_TIME = 10\nCUBE_SIDE = 0.010  # Size of the simulation cube in meters\nsave_points = 120  # Number of points to save the simulation data\ntke_sigma = 0.5  # Standard deviation of turbulent kinetic energy, m^2/s^2\n\n# Defining new parameters for more realistic simulation\ndensity_uniform = 1.5e3  # Uniform density of particles in kg/m^3\nmean_radius_nm = 25000  # Mean radius of particles in nanometers\nstd_dev_nm = 1.1  # Standard deviation of particle radius in nanometers\nsystem_temperature_kelvin = 300  # System temperature in Kelvin\nsystem_pressure_pascal = 1e5  # System pressure in Pascal\n\n# Define intervals for updating interaction factors\ninterval_friction_factor = 1000  # Interval for updating friction factor\ninterval_coagulation = 100  # Interval for updating coagulation\nregnerate_interval = 10000  # Interval for regenerating particles\ntke_interval = 100  # Interval for updating turbulent kinetic energy\n\n# Generating particle masses using a log-normal distribution\nmass = particle_property.generate_particle_masses(\n    mean_radius=mean_radius_nm,\n    std_dev_radius=std_dev_nm,\n    density=density_uniform,\n    num_particles=TOTAL_NUMBER_OF_PARTICLES,\n    radius_input_units=\"nm\",\n)\n\n# Initializing particle velocities using thermal velocity\nvelocity = particle_property.random_thermal_velocity(\n    temperature_kelvin=system_temperature_kelvin,\n    mass_kg=mass,\n    number_of_particles=TOTAL_NUMBER_OF_PARTICLES,\n    t_type=t_type,\n    random_seed=seed,\n)\n\n# Setting constant density for all particles\ndensity = torch.ones(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * density_uniform\n\n# Initialize particle positions randomly within the cube\nposition = (\n    torch.rand(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * CUBE_SIDE\n    - CUBE_SIDE / 2\n)\n\n# Initialize force as zero for all particles\nforce = torch.zeros(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type)\n\n# Generating indices for particles\nindices = torch.arange(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type)  # Integer type\n\n# Calculating the total number of iterations for the simulation\ntotal_iterations = int(SIMULATION_TIME / TIME_STEP)\n\n# Initializing tensor to track total mass over iterations\ntotal_mass = torch.zeros(total_iterations, dtype=t_type)\n\n# Defining gravity acting on all particles\ngravity = (\n    torch.tensor([0, 0, -9.81])\n    .repeat(TOTAL_NUMBER_OF_PARTICLES, 1)\n    .transpose(0, 1)\n)\n\n# Initializing Arrays for Saving Position and Mass Data\nsave_position = np.zeros((3, TOTAL_NUMBER_OF_PARTICLES, save_points))\nsave_mass = np.zeros((TOTAL_NUMBER_OF_PARTICLES, save_points))\nsave_iterations = np.linspace(0, total_iterations, save_points, dtype=int)\n\n# Calculating particle radius from mass and density\nradius = particle_property.radius_calculation(mass=mass, density=density)\n</pre> # Setting up the Simulation Parameters and Initial Conditions  # Define fixed parameters TOTAL_NUMBER_OF_PARTICLES = 100 TIME_STEP = 0.005 SIMULATION_TIME = 10 CUBE_SIDE = 0.010  # Size of the simulation cube in meters save_points = 120  # Number of points to save the simulation data tke_sigma = 0.5  # Standard deviation of turbulent kinetic energy, m^2/s^2  # Defining new parameters for more realistic simulation density_uniform = 1.5e3  # Uniform density of particles in kg/m^3 mean_radius_nm = 25000  # Mean radius of particles in nanometers std_dev_nm = 1.1  # Standard deviation of particle radius in nanometers system_temperature_kelvin = 300  # System temperature in Kelvin system_pressure_pascal = 1e5  # System pressure in Pascal  # Define intervals for updating interaction factors interval_friction_factor = 1000  # Interval for updating friction factor interval_coagulation = 100  # Interval for updating coagulation regnerate_interval = 10000  # Interval for regenerating particles tke_interval = 100  # Interval for updating turbulent kinetic energy  # Generating particle masses using a log-normal distribution mass = particle_property.generate_particle_masses(     mean_radius=mean_radius_nm,     std_dev_radius=std_dev_nm,     density=density_uniform,     num_particles=TOTAL_NUMBER_OF_PARTICLES,     radius_input_units=\"nm\", )  # Initializing particle velocities using thermal velocity velocity = particle_property.random_thermal_velocity(     temperature_kelvin=system_temperature_kelvin,     mass_kg=mass,     number_of_particles=TOTAL_NUMBER_OF_PARTICLES,     t_type=t_type,     random_seed=seed, )  # Setting constant density for all particles density = torch.ones(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * density_uniform  # Initialize particle positions randomly within the cube position = (     torch.rand(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type) * CUBE_SIDE     - CUBE_SIDE / 2 )  # Initialize force as zero for all particles force = torch.zeros(3, TOTAL_NUMBER_OF_PARTICLES, dtype=t_type)  # Generating indices for particles indices = torch.arange(TOTAL_NUMBER_OF_PARTICLES, dtype=t_type)  # Integer type  # Calculating the total number of iterations for the simulation total_iterations = int(SIMULATION_TIME / TIME_STEP)  # Initializing tensor to track total mass over iterations total_mass = torch.zeros(total_iterations, dtype=t_type)  # Defining gravity acting on all particles gravity = (     torch.tensor([0, 0, -9.81])     .repeat(TOTAL_NUMBER_OF_PARTICLES, 1)     .transpose(0, 1) )  # Initializing Arrays for Saving Position and Mass Data save_position = np.zeros((3, TOTAL_NUMBER_OF_PARTICLES, save_points)) save_mass = np.zeros((TOTAL_NUMBER_OF_PARTICLES, save_points)) save_iterations = np.linspace(0, total_iterations, save_points, dtype=int)  # Calculating particle radius from mass and density radius = particle_property.radius_calculation(mass=mass, density=density) In\u00a0[\u00a0]: Copied! <pre># random air velcoity TKE generation\nair_velocity_length = total_iterations // tke_interval\nnormal_dist = torch.distributions.normal.Normal(0, tke_sigma)\nair_velocities = normal_dist.sample((3, air_velocity_length))\n\n\n# Calculating the initial turbulent kinetic energy\ndef tke_calculation(air_velocities):\n    \"\"\"Calculates the turbulent kinetic energy from air velocity\"\"\"\n    std_velocities = torch.std(air_velocities, dim=1)\n    tke = 0.5 * sum(std_velocities**2)\n    return tke\n\n\ntke = tke_calculation(air_velocities)\nprint(\"Initial TKE: \", tke)\nprint(\"velocity shape: \", air_velocities.shape)\nprint(\"Air initial velocity: \", air_velocities[:, 0])\n</pre> # random air velcoity TKE generation air_velocity_length = total_iterations // tke_interval normal_dist = torch.distributions.normal.Normal(0, tke_sigma) air_velocities = normal_dist.sample((3, air_velocity_length))   # Calculating the initial turbulent kinetic energy def tke_calculation(air_velocities):     \"\"\"Calculates the turbulent kinetic energy from air velocity\"\"\"     std_velocities = torch.std(air_velocities, dim=1)     tke = 0.5 * sum(std_velocities**2)     return tke   tke = tke_calculation(air_velocities) print(\"Initial TKE: \", tke) print(\"velocity shape: \", air_velocities.shape) print(\"Air initial velocity: \", air_velocities[:, 0]) <pre>Initial TKE:  tensor(0.3742)\nvelocity shape:  torch.Size([3, 86400])\nAir initial velocity:  tensor([ 0.3003, -0.4218, -0.4296])\n</pre> In\u00a0[\u00a0]: Copied! <pre># Plotting the Initial Radius Distribution\n\n# Calculate the initial radius of particles\nradius_initial = particle_property.radius_calculation(\n    mass=mass, density=density\n)\n\n# Plotting the histogram for radius distribution\nfig, ax = plt.subplots()\nax.hist(machine_limit.safe_log10(radius_initial), bins=20, range=(-7, -2))\nax.set_xlabel(\"log10 Radius (m)\")\nax.set_ylabel(\"Number of Particles\")\nax.set_title(\"Initial Particle Radius Distribution\")\nplt.show()\n\n# Plotting the Initial Velocity Distribution\n\n# Calculate the speeds of particles from their velocities\ninitial_speeds = particle_property.speed(velocity)\n\n# Plotting the histogram for velocity distribution\nfig, ax = plt.subplots()\nax.hist(machine_limit.safe_log10(initial_speeds), bins=20)\nax.set_xlabel(\"log10 Velocity (m/s)\")\nax.set_ylabel(\"Number of Particles\")\nax.set_title(\"Initial Particle Velocity Distribution\")\nplt.show()\n</pre> # Plotting the Initial Radius Distribution  # Calculate the initial radius of particles radius_initial = particle_property.radius_calculation(     mass=mass, density=density )  # Plotting the histogram for radius distribution fig, ax = plt.subplots() ax.hist(machine_limit.safe_log10(radius_initial), bins=20, range=(-7, -2)) ax.set_xlabel(\"log10 Radius (m)\") ax.set_ylabel(\"Number of Particles\") ax.set_title(\"Initial Particle Radius Distribution\") plt.show()  # Plotting the Initial Velocity Distribution  # Calculate the speeds of particles from their velocities initial_speeds = particle_property.speed(velocity)  # Plotting the histogram for velocity distribution fig, ax = plt.subplots() ax.hist(machine_limit.safe_log10(initial_speeds), bins=20) ax.set_xlabel(\"log10 Velocity (m/s)\") ax.set_ylabel(\"Number of Particles\") ax.set_title(\"Initial Particle Velocity Distribution\") plt.show() In\u00a0[\u00a0]: Copied! <pre># Printing the Maximum and Minimum Radii of the Particles\nprint(\"Initial Max Diameter [mm]: \", radius_initial.max().item() * 2 * 1e3)\nprint(\"Initial Min Diameter [mm]: \", radius_initial.min().item() * 2 * 1e3)\n\n# Initializing the Range of Radii for Friction Factor Calculation\n# We use a linspace to cover a range from the smallest to 10 times the\n# largest radius.\nfriction_factor_radius = torch.linspace(\n    start=radius.min(),\n    end=radius.max() * 10,\n    steps=1000,\n)\n\n# Calculating the Friction Factor\n# This calculation considers the radius, temperature, and pressure to\n# determine the friction factor.\nfriction_factor_array = particle_property.friction_factor_wrapper(\n    radius_meter=friction_factor_radius,\n    temperature_kelvin=system_temperature_kelvin,\n    pressure_pascal=system_pressure_pascal,\n)\n\n# Plotting Friction Factor vs Radius\n# The plot helps in visualizing how friction factor varies with particle\n# radius.\nfig, ax = plt.subplots()\nax.plot(friction_factor_radius, friction_factor_array)\nax.set_xscale(\"log\")\nax.set_yscale(\"log\")\nax.set_xlabel(\"Radius (m)\")\nax.set_ylabel(\"Friction Factor\")\nplt.show()\n</pre> # Printing the Maximum and Minimum Radii of the Particles print(\"Initial Max Diameter [mm]: \", radius_initial.max().item() * 2 * 1e3) print(\"Initial Min Diameter [mm]: \", radius_initial.min().item() * 2 * 1e3)  # Initializing the Range of Radii for Friction Factor Calculation # We use a linspace to cover a range from the smallest to 10 times the # largest radius. friction_factor_radius = torch.linspace(     start=radius.min(),     end=radius.max() * 10,     steps=1000, )  # Calculating the Friction Factor # This calculation considers the radius, temperature, and pressure to # determine the friction factor. friction_factor_array = particle_property.friction_factor_wrapper(     radius_meter=friction_factor_radius,     temperature_kelvin=system_temperature_kelvin,     pressure_pascal=system_pressure_pascal, )  # Plotting Friction Factor vs Radius # The plot helps in visualizing how friction factor varies with particle # radius. fig, ax = plt.subplots() ax.plot(friction_factor_radius, friction_factor_array) ax.set_xscale(\"log\") ax.set_yscale(\"log\") ax.set_xlabel(\"Radius (m)\") ax.set_ylabel(\"Friction Factor\") plt.show() <pre>Initial Max Diameter [mm]:  0.06673528696410358\nInitial Min Diameter [mm]:  0.03808399196714163\n</pre> In\u00a0[\u00a0]: Copied! <pre># Initialize Counter for Saving Data and Start Timer\nsave_counter = 0\nstart_time = time.time()\n\n# Initial Calculations Before Starting the Simulation Loop\nradius = particle_property.radius_calculation(mass=mass, density=density)\nfriction_factor_iter = particle_property.nearest_match(\n    x_values=friction_factor_radius,\n    y_values=friction_factor_array,\n    x_new=radius,\n)\n\n# regeneration of radius and velocity\nregen_radius = radius.clone()\nregen_velocity = velocity.clone()\nregen_mass = mass.clone()\nair_ones = torch.ones((3, TOTAL_NUMBER_OF_PARTICLES))\nair_velocity = air_ones * air_velocities[:, 0].unsqueeze(1)\n\n# Main Simulation Loop\nfor i in range(total_iterations):\n\n    # Coagulation Step (Optional, based on specified interval)\n    if i % interval_coagulation == 0:\n        # Update radius for collision detection\n        radius = particle_property.radius_calculation(\n            mass=mass, density=density\n        )\n        # Detect potential collision pairs\n        valid_collision_indices_pairs = particle_pairs.full_sweep_and_prune(\n            position=position, radius=radius\n        )\n\n        # Process collisions and update particle properties\n        if valid_collision_indices_pairs.shape[0] &gt; 0:\n            velocity, mass = collisions.coalescence(\n                position=position,\n                velocity=velocity,\n                mass=mass,\n                radius=radius,\n                collision_indices_pairs=valid_collision_indices_pairs,\n            )\n\n    if i % regnerate_interval == 0:\n        # regenerate for constant particle number\n        zero_mass_bool = mass == 0\n        radius[zero_mass_bool] = regen_radius[zero_mass_bool]\n        velocity[:, zero_mass_bool] = regen_velocity[:, zero_mass_bool]\n        mass[zero_mass_bool] = regen_mass[zero_mass_bool]\n\n    if i % tke_interval == 0:\n        # regenerate air velocity\n        air_velocity = air_ones * air_velocities[\n            :, i // tke_interval\n        ].unsqueeze(1)\n\n    # Update Friction Factor (Optional, based on specified interval)\n    if i % interval_friction_factor == 0:\n        radius = particle_property.radius_calculation(\n            mass=mass, density=density\n        )\n        friction_factor_iter = particle_property.nearest_match(\n            x_values=friction_factor_radius,\n            y_values=friction_factor_array,\n            x_new=radius,\n        )\n\n    # Calculate Forces (including updated friction factor)\n    force = mass * gravity + (air_velocity - velocity) * friction_factor_iter\n\n    # Integrate Equations of Motion (Leapfrog Method)\n    position, velocity = integration.leapfrog(\n        position=position,\n        velocity=velocity,\n        force=force,\n        mass=mass,\n        time_step=TIME_STEP,\n    )\n\n    # Apply Boundary Conditions (Cube Wrap-around)\n    position = boundary.wrapped_cube(position=position, cube_side=CUBE_SIDE)\n\n    # Save Data at Predefined Intervals\n    if i == save_iterations[save_counter]:\n        save_position[:, :, save_counter] = position.detach().numpy()\n        save_mass[:, save_counter] = mass.detach().numpy()\n        save_counter += 1\n        print(f\"Saving data at iteration {i} of {total_iterations}\")\n\n# Final Data Save and Calculation of Total Simulation Time\nsave_position[:, :, -1] = position.detach().numpy()\nsave_mass[:, -1] = mass.detach().numpy()\nend_time = time.time()\nprint(f\"Total wall time: {end_time - start_time} seconds\")\nprint(\n    f\"Ratio of wall time to simulation time: {(end_time - start_time) / SIMULATION_TIME}\"\n)\n</pre> # Initialize Counter for Saving Data and Start Timer save_counter = 0 start_time = time.time()  # Initial Calculations Before Starting the Simulation Loop radius = particle_property.radius_calculation(mass=mass, density=density) friction_factor_iter = particle_property.nearest_match(     x_values=friction_factor_radius,     y_values=friction_factor_array,     x_new=radius, )  # regeneration of radius and velocity regen_radius = radius.clone() regen_velocity = velocity.clone() regen_mass = mass.clone() air_ones = torch.ones((3, TOTAL_NUMBER_OF_PARTICLES)) air_velocity = air_ones * air_velocities[:, 0].unsqueeze(1)  # Main Simulation Loop for i in range(total_iterations):      # Coagulation Step (Optional, based on specified interval)     if i % interval_coagulation == 0:         # Update radius for collision detection         radius = particle_property.radius_calculation(             mass=mass, density=density         )         # Detect potential collision pairs         valid_collision_indices_pairs = particle_pairs.full_sweep_and_prune(             position=position, radius=radius         )          # Process collisions and update particle properties         if valid_collision_indices_pairs.shape[0] &gt; 0:             velocity, mass = collisions.coalescence(                 position=position,                 velocity=velocity,                 mass=mass,                 radius=radius,                 collision_indices_pairs=valid_collision_indices_pairs,             )      if i % regnerate_interval == 0:         # regenerate for constant particle number         zero_mass_bool = mass == 0         radius[zero_mass_bool] = regen_radius[zero_mass_bool]         velocity[:, zero_mass_bool] = regen_velocity[:, zero_mass_bool]         mass[zero_mass_bool] = regen_mass[zero_mass_bool]      if i % tke_interval == 0:         # regenerate air velocity         air_velocity = air_ones * air_velocities[             :, i // tke_interval         ].unsqueeze(1)      # Update Friction Factor (Optional, based on specified interval)     if i % interval_friction_factor == 0:         radius = particle_property.radius_calculation(             mass=mass, density=density         )         friction_factor_iter = particle_property.nearest_match(             x_values=friction_factor_radius,             y_values=friction_factor_array,             x_new=radius,         )      # Calculate Forces (including updated friction factor)     force = mass * gravity + (air_velocity - velocity) * friction_factor_iter      # Integrate Equations of Motion (Leapfrog Method)     position, velocity = integration.leapfrog(         position=position,         velocity=velocity,         force=force,         mass=mass,         time_step=TIME_STEP,     )      # Apply Boundary Conditions (Cube Wrap-around)     position = boundary.wrapped_cube(position=position, cube_side=CUBE_SIDE)      # Save Data at Predefined Intervals     if i == save_iterations[save_counter]:         save_position[:, :, save_counter] = position.detach().numpy()         save_mass[:, save_counter] = mass.detach().numpy()         save_counter += 1         print(f\"Saving data at iteration {i} of {total_iterations}\")  # Final Data Save and Calculation of Total Simulation Time save_position[:, :, -1] = position.detach().numpy() save_mass[:, -1] = mass.detach().numpy() end_time = time.time() print(f\"Total wall time: {end_time - start_time} seconds\") print(     f\"Ratio of wall time to simulation time: {(end_time - start_time) / SIMULATION_TIME}\" ) <pre>Saving data at iteration 0 of 8640000\nSaving data at iteration 72605 of 8640000\nSaving data at iteration 145210 of 8640000\nSaving data at iteration 217815 of 8640000\nSaving data at iteration 290420 of 8640000\nSaving data at iteration 363025 of 8640000\nSaving data at iteration 435630 of 8640000\nSaving data at iteration 508235 of 8640000\nSaving data at iteration 580840 of 8640000\nSaving data at iteration 653445 of 8640000\nSaving data at iteration 726050 of 8640000\nSaving data at iteration 798655 of 8640000\nSaving data at iteration 871260 of 8640000\nSaving data at iteration 943865 of 8640000\nSaving data at iteration 1016470 of 8640000\nSaving data at iteration 1089075 of 8640000\nSaving data at iteration 1161680 of 8640000\nSaving data at iteration 1234285 of 8640000\nSaving data at iteration 1306890 of 8640000\nSaving data at iteration 1379495 of 8640000\nSaving data at iteration 1452100 of 8640000\nSaving data at iteration 1524705 of 8640000\nSaving data at iteration 1597310 of 8640000\nSaving data at iteration 1669915 of 8640000\nSaving data at iteration 1742521 of 8640000\nSaving data at iteration 1815126 of 8640000\nSaving data at iteration 1887731 of 8640000\nSaving data at iteration 1960336 of 8640000\nSaving data at iteration 2032941 of 8640000\nSaving data at iteration 2105546 of 8640000\nSaving data at iteration 2178151 of 8640000\nSaving data at iteration 2250756 of 8640000\nSaving data at iteration 2323361 of 8640000\nSaving data at iteration 2395966 of 8640000\nSaving data at iteration 2468571 of 8640000\nSaving data at iteration 2541176 of 8640000\nSaving data at iteration 2613781 of 8640000\nSaving data at iteration 2686386 of 8640000\nSaving data at iteration 2758991 of 8640000\nSaving data at iteration 2831596 of 8640000\nSaving data at iteration 2904201 of 8640000\nSaving data at iteration 2976806 of 8640000\nSaving data at iteration 3049411 of 8640000\nSaving data at iteration 3122016 of 8640000\nSaving data at iteration 3194621 of 8640000\nSaving data at iteration 3267226 of 8640000\nSaving data at iteration 3339831 of 8640000\nSaving data at iteration 3412436 of 8640000\nSaving data at iteration 3485042 of 8640000\nSaving data at iteration 3557647 of 8640000\nSaving data at iteration 3630252 of 8640000\nSaving data at iteration 3702857 of 8640000\nSaving data at iteration 3775462 of 8640000\nSaving data at iteration 3848067 of 8640000\nSaving data at iteration 3920672 of 8640000\nSaving data at iteration 3993277 of 8640000\nSaving data at iteration 4065882 of 8640000\nSaving data at iteration 4138487 of 8640000\nSaving data at iteration 4211092 of 8640000\nSaving data at iteration 4283697 of 8640000\nSaving data at iteration 4356302 of 8640000\nSaving data at iteration 4428907 of 8640000\nSaving data at iteration 4501512 of 8640000\nSaving data at iteration 4574117 of 8640000\nSaving data at iteration 4646722 of 8640000\nSaving data at iteration 4719327 of 8640000\nSaving data at iteration 4791932 of 8640000\nSaving data at iteration 4864537 of 8640000\nSaving data at iteration 4937142 of 8640000\nSaving data at iteration 5009747 of 8640000\nSaving data at iteration 5082352 of 8640000\nSaving data at iteration 5154957 of 8640000\nSaving data at iteration 5227563 of 8640000\nSaving data at iteration 5300168 of 8640000\nSaving data at iteration 5372773 of 8640000\nSaving data at iteration 5445378 of 8640000\nSaving data at iteration 5517983 of 8640000\nSaving data at iteration 5590588 of 8640000\nSaving data at iteration 5663193 of 8640000\nSaving data at iteration 5735798 of 8640000\nSaving data at iteration 5808403 of 8640000\nSaving data at iteration 5881008 of 8640000\nSaving data at iteration 5953613 of 8640000\nSaving data at iteration 6026218 of 8640000\nSaving data at iteration 6098823 of 8640000\nSaving data at iteration 6171428 of 8640000\nSaving data at iteration 6244033 of 8640000\nSaving data at iteration 6316638 of 8640000\nSaving data at iteration 6389243 of 8640000\nSaving data at iteration 6461848 of 8640000\nSaving data at iteration 6534453 of 8640000\nSaving data at iteration 6607058 of 8640000\nSaving data at iteration 6679663 of 8640000\nSaving data at iteration 6752268 of 8640000\nSaving data at iteration 6824873 of 8640000\nSaving data at iteration 6897478 of 8640000\nSaving data at iteration 6970084 of 8640000\nSaving data at iteration 7042689 of 8640000\nSaving data at iteration 7115294 of 8640000\nSaving data at iteration 7187899 of 8640000\nSaving data at iteration 7260504 of 8640000\nSaving data at iteration 7333109 of 8640000\nSaving data at iteration 7405714 of 8640000\nSaving data at iteration 7478319 of 8640000\nSaving data at iteration 7550924 of 8640000\nSaving data at iteration 7623529 of 8640000\nSaving data at iteration 7696134 of 8640000\nSaving data at iteration 7768739 of 8640000\nSaving data at iteration 7841344 of 8640000\nSaving data at iteration 7913949 of 8640000\nSaving data at iteration 7986554 of 8640000\nSaving data at iteration 8059159 of 8640000\nSaving data at iteration 8131764 of 8640000\nSaving data at iteration 8204369 of 8640000\nSaving data at iteration 8276974 of 8640000\nSaving data at iteration 8349579 of 8640000\nSaving data at iteration 8422184 of 8640000\nSaving data at iteration 8494789 of 8640000\nSaving data at iteration 8567394 of 8640000\nTotal wall time: 3740.235196828842 seconds\nRatio of wall time to simulation time: 0.08657951844511208\n</pre> In\u00a0[\u00a0]: Copied! <pre># save data\nnp.savez_compressed(\n    \"data_tke.npz\",\n    position=save_position,\n    mass=save_mass,\n    iterations=save_iterations,\n    time_step=TIME_STEP,\n    total_iterations=total_iterations,\n    total_number_of_particles=TOTAL_NUMBER_OF_PARTICLES,\n    simulation_time=SIMULATION_TIME,\n    cube_side=CUBE_SIDE,\n    density_uniform=density_uniform,\n    mean_radius_nm=mean_radius_nm,\n    std_dev_nm=std_dev_nm,\n    system_temperature_kelvin=system_temperature_kelvin,\n    system_pressure_pascal=system_pressure_pascal,\n    interval_friction_factor=interval_friction_factor,\n    interval_coagulation=interval_coagulation,\n    seed=seed,\n    friction_factor_radius=friction_factor_radius,\n    friction_factor_array=friction_factor_array,\n)\n</pre> # save data np.savez_compressed(     \"data_tke.npz\",     position=save_position,     mass=save_mass,     iterations=save_iterations,     time_step=TIME_STEP,     total_iterations=total_iterations,     total_number_of_particles=TOTAL_NUMBER_OF_PARTICLES,     simulation_time=SIMULATION_TIME,     cube_side=CUBE_SIDE,     density_uniform=density_uniform,     mean_radius_nm=mean_radius_nm,     std_dev_nm=std_dev_nm,     system_temperature_kelvin=system_temperature_kelvin,     system_pressure_pascal=system_pressure_pascal,     interval_friction_factor=interval_friction_factor,     interval_coagulation=interval_coagulation,     seed=seed,     friction_factor_radius=friction_factor_radius,     friction_factor_array=friction_factor_array, ) In\u00a0[\u00a0]: Copied! <pre># Processing the Final Data for Visualization\n\n# Select the final time index for the data\ntime_index = -1\nposition_final = save_position[:, :, time_index]\nmass_final = save_mass[:, time_index]\n\n# Filter out particles with zero mass\nfilter_zero_mass = mass_final &gt; 0\n\n# Calculate the radius and area of each particle\nradius_final = particle_property.radius_calculation(\n    mass=mass_final, density=density\n)\nparticle_area = np.pi * radius_final**2\n# normalize the area\nparticle_area = particle_area / particle_area.max()\n\n\n# Display the number of remaining particles\nprint(f\"Number of particles at the end: {filter_zero_mass.sum()}\")\n\n# Creating a 3D Plot for Visualization\nfig = plt.figure()\nax = fig.add_subplot(projection=\"3d\")\n\n# Choose a color map for the scatter plot\ncmap = plt.cm.viridis\n\n# Plot the final positions of particles with non-zero mass\nscatter_plot = ax.scatter(\n    position_final[0, filter_zero_mass],\n    position_final[1, filter_zero_mass],\n    position_final[2, filter_zero_mass],\n    c=mass_final[filter_zero_mass],\n    cmap=cmap,\n    s=particle_area[filter_zero_mass]\n    * 10,  # scalled Particle size based on normalized area\n)\n\n# # Set axis limits based on cube dimensions\nax.set_xlim(-CUBE_SIDE / 2, CUBE_SIDE / 2)\nax.set_ylim(-CUBE_SIDE / 2, CUBE_SIDE / 2)\nax.set_zlim(-CUBE_SIDE / 2, CUBE_SIDE / 2)\n\n# Add a color bar indicating particle mass\ncolor_bar = plt.colorbar(scatter_plot, ax=ax)\ncolor_bar.set_label(\"Mass [kg]\")\n\n# Show the plot with optimized layout\nplt.show()\nfig.tight_layout()\n</pre> # Processing the Final Data for Visualization  # Select the final time index for the data time_index = -1 position_final = save_position[:, :, time_index] mass_final = save_mass[:, time_index]  # Filter out particles with zero mass filter_zero_mass = mass_final &gt; 0  # Calculate the radius and area of each particle radius_final = particle_property.radius_calculation(     mass=mass_final, density=density ) particle_area = np.pi * radius_final**2 # normalize the area particle_area = particle_area / particle_area.max()   # Display the number of remaining particles print(f\"Number of particles at the end: {filter_zero_mass.sum()}\")  # Creating a 3D Plot for Visualization fig = plt.figure() ax = fig.add_subplot(projection=\"3d\")  # Choose a color map for the scatter plot cmap = plt.cm.viridis  # Plot the final positions of particles with non-zero mass scatter_plot = ax.scatter(     position_final[0, filter_zero_mass],     position_final[1, filter_zero_mass],     position_final[2, filter_zero_mass],     c=mass_final[filter_zero_mass],     cmap=cmap,     s=particle_area[filter_zero_mass]     * 10,  # scalled Particle size based on normalized area )  # # Set axis limits based on cube dimensions ax.set_xlim(-CUBE_SIDE / 2, CUBE_SIDE / 2) ax.set_ylim(-CUBE_SIDE / 2, CUBE_SIDE / 2) ax.set_zlim(-CUBE_SIDE / 2, CUBE_SIDE / 2)  # Add a color bar indicating particle mass color_bar = plt.colorbar(scatter_plot, ax=ax) color_bar.set_label(\"Mass [kg]\")  # Show the plot with optimized layout plt.show() fig.tight_layout() <pre>Number of particles at the end: 500\n</pre> In\u00a0[\u00a0]: Copied! <pre># Visualizing the Mass Distribution at Different Stages\n\n# plot initial radius distribution\nfig, ax = plt.subplots()\nax.hist(\n    machine_limit.safe_log10(radius_initial),\n    bins=20,\n    range=(-5, -4),\n    label=\"Initial\",\n    alpha=0.25,\n)\nax.hist(\n    machine_limit.safe_log10(radius_final[filter_zero_mass]),\n    bins=20,\n    range=(-5, -4),\n    label=\"Final\",\n    alpha=0.5,\n)\nax.set_yscale(\"log\")\nax.set_xlabel(\"log10 Radius (m)\")\nax.set_ylabel(\"Number of Particles\")\nax.set_title(\"Initial Particle Radius Distribution\")\nax.legend()\nplt.show()\n\nprint(\"Initial Max Diameter [mm]: \", radius_initial.max().item() * 2 * 1e3)\nprint(\n    \"Final Max Diameter [mm]: \",\n    radius_final[filter_zero_mass].max().item() * 2 * 1e3,\n)\n</pre> # Visualizing the Mass Distribution at Different Stages  # plot initial radius distribution fig, ax = plt.subplots() ax.hist(     machine_limit.safe_log10(radius_initial),     bins=20,     range=(-5, -4),     label=\"Initial\",     alpha=0.25, ) ax.hist(     machine_limit.safe_log10(radius_final[filter_zero_mass]),     bins=20,     range=(-5, -4),     label=\"Final\",     alpha=0.5, ) ax.set_yscale(\"log\") ax.set_xlabel(\"log10 Radius (m)\") ax.set_ylabel(\"Number of Particles\") ax.set_title(\"Initial Particle Radius Distribution\") ax.legend() plt.show()  print(\"Initial Max Diameter [mm]: \", radius_initial.max().item() * 2 * 1e3) print(     \"Final Max Diameter [mm]: \",     radius_final[filter_zero_mass].max().item() * 2 * 1e3, ) <pre>Initial Max Diameter [mm]:  0.06673528696410358\nFinal Max Diameter [mm]:  0.3941673396242591\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"How-To-Guides/Lagrangian/notebooks/turbulence_drops/#realistic-drag-mass-and-velocity","title":"Realistic Drag, Mass, and Velocity\u00b6","text":"<p>Example Broken</p> <p>This notebook section focuses on simulating cloud droplet-sized aerosol particles (around 100 microns) with accurate mass and velocity. Key improvements include:</p> <ol> <li>Refining Initial Conditions: Setting precise starting values for particle mass and velocity.</li> <li>Modeling Forces Accurately: Better representation of forces acting on particles to influence their movement.</li> </ol> <p>These steps are crucial for enhancing the realism of the aerosol particle model.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/turbulence_drops/#initialization-of-particles","title":"Initialization of Particles\u00b6","text":"<p>In this section, we'll enhance particle initialization for realism:</p> <ul> <li>Particle Masses: Implementing a narrow log-normal distribution to better represent real-world mass variation.</li> <li>Particle Velocities: Utilizing thermal velocity, influenced by Brownian motion, for initial velocity settings.</li> <li>Initial Positions: Particles will be randomly placed within a defined box area to simulate natural distribution.</li> </ul>"},{"location":"How-To-Guides/Lagrangian/notebooks/turbulence_drops/#plotting-initial-conditions","title":"Plotting Initial Conditions\u00b6","text":"<p>To verify the realism of our particle model, we'll plot the initial distributions of particle size and velocity. This step ensures that the initial setup aligns with our expectations and the intended simulation parameters.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/turbulence_drops/#particle-size-distribution","title":"Particle Size Distribution\u00b6","text":"<p>First, we examine the distribution of particle radii. The goal is to confirm that the radii follow the specified log-normal distribution.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/turbulence_drops/#particle-velocity-distribution","title":"Particle Velocity Distribution\u00b6","text":"<p>Next, we assess the distribution of particle velocities. This is crucial for ensuring that the initial velocities reflect thermal motion as intended.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/turbulence_drops/#running-the-particle-simulation","title":"Running the Particle Simulation\u00b6","text":"<p>In this part of the notebook, we execute the core simulation loop. This involves integrating the motion of the particles over time while considering the forces acting on them, including friction and gravity. Several key updates and optimizations are included:</p> <ol> <li><p>Friction Factor Initialization: Before entering the main loop, the friction factor for each particle is calculated. This is an essential step for accurately simulating the drag force on particles.</p> </li> <li><p>Conditional Coagulation Step: The simulation includes an optional coagulation step, executed at a specified interval. This step simulates the merging of particles upon collision, affecting their mass and velocity.</p> </li> <li><p>Friction Factor Update: The friction factor is updated at regular intervals to reflect changes in particle size or other environmental conditions. This step ensures that the drag force remains accurate throughout the simulation.</p> </li> <li><p>Force Calculation and Integration: Within each iteration, we calculate the forces acting on the particles and update their positions and velocities using the leapfrog integration method.</p> </li> <li><p>Boundary Conditions and Data Saving: The simulation includes boundary conditions to mimic a wrapped cube environment. Additionally, particle position and mass data are saved at predefined intervals for analysis.</p> </li> </ol> <p>This simulation loop provides a comprehensive and dynamic model of particle motion, taking into account physical factors and interactions.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/turbulence_drops/#plotting-particle-positions","title":"Plotting Particle Positions\u00b6","text":"<p>After running the simulation, we'll plot the particle positions to visualize their movement.</p>"},{"location":"How-To-Guides/Lagrangian/notebooks/turbulence_drops/#size-distribution-of-particles","title":"Size Distribution of Particles\u00b6","text":"<p>Given particle collision and coagulation, we'll plot the distribution of particle sizes at the end of the simulation. This step ensures that the particle size distribution aligns with our expectations and the intended simulation parameters. That being, both a decrease in the number of particles and an increase in the average particle size.</p>"},{"location":"How-To-Guides/Light_Scattering/","title":"Index: Light Scattering BETA","text":"<p>Aerosol optics examines the interaction between light and aerosol particles suspended in the atmosphere. These interactions, encompassing absorption, scattering, and the emission of light, are pivotal in deciphering the physical properties of aerosols and their environmental ramifications.</p>"},{"location":"How-To-Guides/Light_Scattering/#notebooks","title":"Notebooks","text":"<ul> <li>Mie Scattering Basics</li> <li>Humidified Particle Scattering</li> <li>Kappa-HGF Estimation from Light Extinction</li> <li>Correcting for Scattering Truncation</li> </ul>"},{"location":"How-To-Guides/Light_Scattering/#mie-scattering-theory","title":"Mie Scattering Theory","text":"<p>Central to aerosol optics is Mie scattering theory, formulated by Gustav Mie in 1908. This foundational theory enables the precise calculation of light scattering and absorption by spherical particles, taking into account their size, material composition, and the incident light's wavelength. In this context, we leverage the capabilities of pyMieScatt, a comprehensive Python library designed to facilitate Mie scattering computations.</p>"},{"location":"How-To-Guides/Light_Scattering/#fundamental-concepts","title":"Fundamental Concepts","text":"<ul> <li>Scattering Efficiency: Quantifies the efficacy of particles in deflecting light in various directions.</li> <li>Absorption Efficiency: Assesses the extent to which particles absorb incident light.</li> <li>Single Scattering Albedo (SSA): This ratio of scattering to total light extinction (scattering plus absorption) provides insight into whether particles are more likely to scatter light rather than absorb it.</li> </ul>"},{"location":"How-To-Guides/Light_Scattering/#understanding-particle-distributions","title":"Understanding Particle Distributions","text":"<p>Aerosol particles exhibit a vast diversity in terms of size, shape, and chemical composition, making the study of their distributions crucial for accurate optical modeling.</p>"},{"location":"How-To-Guides/Light_Scattering/#types-of-distributions","title":"Types of Distributions","text":"<ul> <li>Monodisperse: A scenario where all particles are of identical size.</li> <li>Polydisperse: Represents a realistic distribution where particles vary in size, often characterized by statistical distribution models, such as the log-normal distribution.</li> </ul>"},{"location":"How-To-Guides/Light_Scattering/#addressing-truncation-errors-in-measurements","title":"Addressing Truncation Errors in Measurements","text":"<p>Measurements of aerosol optical properties can be compromised by truncation errors, stemming from the inability of instruments to capture the complete angular range of scattered light.</p>"},{"location":"How-To-Guides/Light_Scattering/#consequences-and-mitigation-strategies","title":"Consequences and Mitigation Strategies","text":"<ul> <li>Scattering Coefficient Underestimation: The restricted detection of scattered light may lead to inaccuracies in determining aerosol optical depth (AOD) and other key optical properties.</li> <li>Correction Techniques: A variety of correction methods, including analytical adjustments and empirical calibration, are employed to counteract truncation errors and refine the accuracy of aerosol optical measurements.</li> </ul>"},{"location":"How-To-Guides/Light_Scattering/#overview","title":"Overview","text":"<p>This series offers a detailed exploration of aerosol optical phenomena through the lens of Mie scattering theory, analysis of particle size distributions, and methodologies for correcting truncation errors in aerosol instrumentation. By enhancing our understanding of these areas, we aim to further our knowledge of aerosol behavior and its environmental impact.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/humid_scattering/","title":"Scattering for Humidified Particles","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# particula imports\nfrom particula.util import convert, distribution_discretization\nfrom particula_beta.data.process import mie_bulk\n</pre> import numpy as np import matplotlib.pyplot as plt  # particula imports from particula.util import convert, distribution_discretization from particula_beta.data.process import mie_bulk In\u00a0[2]: Copied! <pre># Define the range and resolution of particle diameters (in nanometers)\ndiameters = np.linspace(20, 1000, 500)  # From 20 nm to 1000 nm with 500 points\n\n# Standard deviation of the log-normal distribution (dimensionless)\nsigma = 1.25\n\n# Define the modes (peak diameters) for a bimodal distribution\nmodes = [100, 500]\n\n# Total number of particles in the distribution\nnumber_total = 1e3\n\n# Generate a log-normal probability density function (pdf) with an area equal to 1\npdf_dist = distribution_discretization.discretize(\n    interval=diameters,  # Array of diameters over which to compute the distribution\n    disttype=\"lognormal\",  # Type of distribution\n    gsigma=sigma,  # Geometric standard deviation\n    mode=modes,  # Modes of the distribution\n    nparticles=number_total,  # Total number of particles\n).m\n\n# Convert the distribution from pdf to probability mass function (pmf)\nnumber_conc_pms = (\n    convert.distribution_convert_pdf_pms(\n        x_array=diameters,  # Array of diameters\n        distribution=pdf_dist,  # The generated pdf\n        to_pdf=False,  # Specifies the conversion to pmf\n    )\n    * number_total\n)  # Scale to the total number of particles\n\n# Check the total number of particles are equal to the specified number\nprint(f\"Total number of particles: {np.sum(number_conc_pms)}\")\n\n# Visualization\nfig, ax = plt.subplots()\nax.plot(diameters, number_conc_pms, label=\"Base Distribution\")\nax.set_xscale(\"log\")  # Logarithmic scale for diameters\nax.set_xlabel(\"Diameter (nm)\")  # X-axis label\nax.set_ylabel(\"Number concentration (#/cm^3)\")  # Y-axis label\nax.legend()  # Show legend\nplt.show()  # Display the plot\n</pre> # Define the range and resolution of particle diameters (in nanometers) diameters = np.linspace(20, 1000, 500)  # From 20 nm to 1000 nm with 500 points  # Standard deviation of the log-normal distribution (dimensionless) sigma = 1.25  # Define the modes (peak diameters) for a bimodal distribution modes = [100, 500]  # Total number of particles in the distribution number_total = 1e3  # Generate a log-normal probability density function (pdf) with an area equal to 1 pdf_dist = distribution_discretization.discretize(     interval=diameters,  # Array of diameters over which to compute the distribution     disttype=\"lognormal\",  # Type of distribution     gsigma=sigma,  # Geometric standard deviation     mode=modes,  # Modes of the distribution     nparticles=number_total,  # Total number of particles ).m  # Convert the distribution from pdf to probability mass function (pmf) number_conc_pms = (     convert.distribution_convert_pdf_pms(         x_array=diameters,  # Array of diameters         distribution=pdf_dist,  # The generated pdf         to_pdf=False,  # Specifies the conversion to pmf     )     * number_total )  # Scale to the total number of particles  # Check the total number of particles are equal to the specified number print(f\"Total number of particles: {np.sum(number_conc_pms)}\")  # Visualization fig, ax = plt.subplots() ax.plot(diameters, number_conc_pms, label=\"Base Distribution\") ax.set_xscale(\"log\")  # Logarithmic scale for diameters ax.set_xlabel(\"Diameter (nm)\")  # X-axis label ax.set_ylabel(\"Number concentration (#/cm^3)\")  # Y-axis label ax.legend()  # Show legend plt.show()  # Display the plot <pre>Total number of particles: 999.533377633679\n</pre> In\u00a0[3]: Copied! <pre># Hygroscopic growth parameters\nkappa = 0.61  # Kappa value for ammonium sulfate\nwater_activity_low = 0.3  # Low water activity (30% RH)\nwater_activity_mid = 0.6  # Mid water activity (60% RH)\nwater_activity_high = 0.9  # High water activity (90% RH)\n\n# Convert particle diameters from the base distribution to volumes\nvolume_sizer = convert.length_to_volume(diameters, length_type=\"diameter\")\n\n# Calculate water volume absorbed by particles at different RH\nvolume_water_low = convert.kappa_volume_water(\n    volume_solute=volume_sizer, kappa=kappa, water_activity=water_activity_low\n)  # showing the input parameters\nvolume_water_mid = convert.kappa_volume_water(\n    volume_sizer, kappa, water_activity_mid\n)\nvolume_water_high = convert.kappa_volume_water(\n    volume_sizer, kappa, water_activity_high\n)\n\n# Calculate new particle diameters after hygroscopic growth\ndiameters_low = convert.volume_to_length(\n    volume=volume_sizer + volume_water_low, length_type=\"diameter\"\n)  # showing the input parameters\ndiameters_mid = convert.volume_to_length(\n    volume_sizer + volume_water_mid, \"diameter\"\n)\ndiameters_high = convert.volume_to_length(\n    volume_sizer + volume_water_high, \"diameter\"\n)\n\n# Plotting the base and grown size distributions\nfig, ax = plt.subplots()\nax.plot(diameters, number_conc_pms, label=\"Base Distribution\")\nax.plot(\n    diameters_low,\n    number_conc_pms,\n    label=f\"Low Water Activity: {water_activity_low*100}% RH\",\n)\nax.plot(\n    diameters_mid,\n    number_conc_pms,\n    label=f\"Mid Water Activity: {water_activity_mid*100}% RH\",\n)\nax.plot(\n    diameters_high,\n    number_conc_pms,\n    label=f\"High Water Activity: {water_activity_high*100}% RH\",\n)\nax.set_xscale(\"log\")\nax.set_xlabel(\"Diameter (nm)\")\nax.set_ylabel(\"Number concentration (#/cm^3)\")\nax.legend(loc=\"upper right\")\nplt.show()\n</pre> # Hygroscopic growth parameters kappa = 0.61  # Kappa value for ammonium sulfate water_activity_low = 0.3  # Low water activity (30% RH) water_activity_mid = 0.6  # Mid water activity (60% RH) water_activity_high = 0.9  # High water activity (90% RH)  # Convert particle diameters from the base distribution to volumes volume_sizer = convert.length_to_volume(diameters, length_type=\"diameter\")  # Calculate water volume absorbed by particles at different RH volume_water_low = convert.kappa_volume_water(     volume_solute=volume_sizer, kappa=kappa, water_activity=water_activity_low )  # showing the input parameters volume_water_mid = convert.kappa_volume_water(     volume_sizer, kappa, water_activity_mid ) volume_water_high = convert.kappa_volume_water(     volume_sizer, kappa, water_activity_high )  # Calculate new particle diameters after hygroscopic growth diameters_low = convert.volume_to_length(     volume=volume_sizer + volume_water_low, length_type=\"diameter\" )  # showing the input parameters diameters_mid = convert.volume_to_length(     volume_sizer + volume_water_mid, \"diameter\" ) diameters_high = convert.volume_to_length(     volume_sizer + volume_water_high, \"diameter\" )  # Plotting the base and grown size distributions fig, ax = plt.subplots() ax.plot(diameters, number_conc_pms, label=\"Base Distribution\") ax.plot(     diameters_low,     number_conc_pms,     label=f\"Low Water Activity: {water_activity_low*100}% RH\", ) ax.plot(     diameters_mid,     number_conc_pms,     label=f\"Mid Water Activity: {water_activity_mid*100}% RH\", ) ax.plot(     diameters_high,     number_conc_pms,     label=f\"High Water Activity: {water_activity_high*100}% RH\", ) ax.set_xscale(\"log\") ax.set_xlabel(\"Diameter (nm)\") ax.set_ylabel(\"Number concentration (#/cm^3)\") ax.legend(loc=\"upper right\") plt.show() In\u00a0[4]: Copied! <pre># Define the refractive index of dry particle material and water\nrefractive_index_dry = 1.5 + 0.5j  # Dimensionless\nrefractive_index_water = 1.33  # Dimensionless\n\n# Calculate effective refractive indices for each water activity level,\n# utilizing the volumes of each component\nn_effective_low = convert.effective_refractive_index(\n    m_zero=refractive_index_dry,\n    m_one=refractive_index_water,\n    volume_zero=volume_sizer[-1],\n    volume_one=volume_water_low[-1],\n)\nn_effective_mid = convert.effective_refractive_index(\n    m_zero=refractive_index_dry,\n    m_one=refractive_index_water,\n    volume_zero=volume_sizer[-1],\n    volume_one=volume_water_mid[-1],\n)\nn_effective_high = convert.effective_refractive_index(\n    m_zero=refractive_index_dry,\n    m_one=refractive_index_water,\n    volume_zero=volume_sizer[-1],\n    volume_one=volume_water_high[-1],\n)\n\n# Output the effective refractive indices\nprint(f\"Effective Refractive Index at Low Water Activity: {n_effective_low}\")\nprint(f\"Effective Refractive Index at Mid Water Activity: {n_effective_mid}\")\nprint(f\"Effective Refractive Index at High Water Activity: {n_effective_high}\")\n</pre> # Define the refractive index of dry particle material and water refractive_index_dry = 1.5 + 0.5j  # Dimensionless refractive_index_water = 1.33  # Dimensionless  # Calculate effective refractive indices for each water activity level, # utilizing the volumes of each component n_effective_low = convert.effective_refractive_index(     m_zero=refractive_index_dry,     m_one=refractive_index_water,     volume_zero=volume_sizer[-1],     volume_one=volume_water_low[-1], ) n_effective_mid = convert.effective_refractive_index(     m_zero=refractive_index_dry,     m_one=refractive_index_water,     volume_zero=volume_sizer[-1],     volume_one=volume_water_mid[-1], ) n_effective_high = convert.effective_refractive_index(     m_zero=refractive_index_dry,     m_one=refractive_index_water,     volume_zero=volume_sizer[-1],     volume_one=volume_water_high[-1], )  # Output the effective refractive indices print(f\"Effective Refractive Index at Low Water Activity: {n_effective_low}\") print(f\"Effective Refractive Index at Mid Water Activity: {n_effective_mid}\") print(f\"Effective Refractive Index at High Water Activity: {n_effective_high}\") <pre>Effective Refractive Index at Low Water Activity: (1.475296758061568+0.3877520937699365j)\nEffective Refractive Index at Mid Water Activity: (1.4339992549471252+0.24759591899911076j)\nEffective Refractive Index at High Water Activity: (1.363603614921365+0.06976083730498348j)\n</pre> In\u00a0[5]: Copied! <pre># Define the wavelength of incident light in nanometers\nwavelength = 550\n\n# Calculate optical properties for dry and humidified particles\noptics_dry = mie_bulk.mie_size_distribution(\n    m_sphere=refractive_index_dry,\n    wavelength=wavelength,\n    diameter=diameters,\n    number_per_cm3=number_conc_pms,\n    pms=True,\n    discretize=False,\n)\n\noptics_low = mie_bulk.mie_size_distribution(\n    m_sphere=n_effective_low,\n    wavelength=wavelength,\n    diameter=diameters_low,\n    number_per_cm3=number_conc_pms,\n    pms=True,\n    discretize=False,\n)\n\noptics_mid = mie_bulk.mie_size_distribution(\n    m_sphere=n_effective_mid,\n    wavelength=wavelength,\n    diameter=diameters_mid,\n    number_per_cm3=number_conc_pms,\n    pms=True,\n    discretize=False,\n)\n\noptics_high = mie_bulk.mie_size_distribution(\n    m_sphere=n_effective_high,\n    wavelength=wavelength,\n    diameter=diameters_high,\n    number_per_cm3=number_conc_pms,\n    pms=True,\n    discretize=False,\n)\n\n# Combine the results for visualization\ncombined_optics = np.vstack((optics_dry, optics_low, optics_mid, optics_high))\nwater_activity_series = [\n    0,\n    water_activity_low,\n    water_activity_mid,\n    water_activity_high,\n]\n\n# Visualization of optical properties across different humidity levels\nfig, ax = plt.subplots()\nax.plot(water_activity_series, combined_optics[:, 0], label=\"Extinction\")\nax.plot(water_activity_series, combined_optics[:, 1], label=\"Scattering\")\nax.plot(water_activity_series, combined_optics[:, 2], label=\"Absorption\")\nax.set_xlabel(\"Water Activity\")\nax.set_ylabel(\"Coefficient (1/Mm)\")\nax.legend()\nplt.show()\n</pre> # Define the wavelength of incident light in nanometers wavelength = 550  # Calculate optical properties for dry and humidified particles optics_dry = mie_bulk.mie_size_distribution(     m_sphere=refractive_index_dry,     wavelength=wavelength,     diameter=diameters,     number_per_cm3=number_conc_pms,     pms=True,     discretize=False, )  optics_low = mie_bulk.mie_size_distribution(     m_sphere=n_effective_low,     wavelength=wavelength,     diameter=diameters_low,     number_per_cm3=number_conc_pms,     pms=True,     discretize=False, )  optics_mid = mie_bulk.mie_size_distribution(     m_sphere=n_effective_mid,     wavelength=wavelength,     diameter=diameters_mid,     number_per_cm3=number_conc_pms,     pms=True,     discretize=False, )  optics_high = mie_bulk.mie_size_distribution(     m_sphere=n_effective_high,     wavelength=wavelength,     diameter=diameters_high,     number_per_cm3=number_conc_pms,     pms=True,     discretize=False, )  # Combine the results for visualization combined_optics = np.vstack((optics_dry, optics_low, optics_mid, optics_high)) water_activity_series = [     0,     water_activity_low,     water_activity_mid,     water_activity_high, ]  # Visualization of optical properties across different humidity levels fig, ax = plt.subplots() ax.plot(water_activity_series, combined_optics[:, 0], label=\"Extinction\") ax.plot(water_activity_series, combined_optics[:, 1], label=\"Scattering\") ax.plot(water_activity_series, combined_optics[:, 2], label=\"Absorption\") ax.set_xlabel(\"Water Activity\") ax.set_ylabel(\"Coefficient (1/Mm)\") ax.legend() plt.show() In\u00a0[6]: Copied! <pre>help(convert.kappa_volume_water)\n</pre> help(convert.kappa_volume_water) <pre>Help on function kappa_volume_water in module particula.util.convert:\n\nkappa_volume_water(volume_solute: Union[float, numpy.ndarray[Any, numpy.dtype[numpy.float64]]], kappa: Union[float, numpy.ndarray[Any, numpy.dtype[numpy.float64]]], water_activity: Union[float, numpy.ndarray[Any, numpy.dtype[numpy.float64]]]) -&gt; Union[float, numpy.ndarray[Any, numpy.dtype[numpy.float64]]]\n    Calculate the volume of water given volume of solute, kappa parameter,\n    and water activity.\n    \n    Args:\n    -----------\n        volume_solute: The volume of solute.\n        kappa: The kappa parameter.\n        water_activity: The water activity.\n    \n    Returns:\n    --------\n        The volume of water as a float.\n\n</pre> In\u00a0[7]: Copied! <pre>help(convert.effective_refractive_index)\n</pre> help(convert.effective_refractive_index) <pre>Help on function effective_refractive_index in module particula.util.convert:\n\neffective_refractive_index(m_zero: Union[float, complex], m_one: Union[float, complex], volume_zero: float, volume_one: float) -&gt; Union[float, complex]\n    Calculate the effective refractive index of a mixture of two solutes, given\n    the refractive index of each solute and the volume of each solute. The\n    mixing is based on volume-weighted molar refraction.\n    \n    Args:\n    -----------\n        m_zero (float or complex): The refractive index of solute 0.\n        m_one (float or complex): The refractive index of solute 1.\n        volume_zero (float): The volume of solute 0.\n        volume_one (float): The volume of solute 1.\n    \n    Returns:\n    -----------\n        The effective refractive index of the mixture.\n    \n    Reference:\n    -----------\n        Liu, Y., &amp;#38; Daum, P. H. (2008).\n        Relationship of refractive index to mass density and self-consistency\n        mixing rules for multicomponent mixtures like ambient aerosols.\n        Journal of Aerosol Science, 39(11), 974-986.\n        https://doi.org/10.1016/j.jaerosci.2008.06.006\n\n</pre>"},{"location":"How-To-Guides/Light_Scattering/notebooks/humid_scattering/#scattering-for-humidified-particles","title":"Scattering for Humidified Particles\u00b6","text":"<p>In atmospheric conditions, aerosol particles frequently undergo hygroscopic growth, absorbing moisture from the air which leads to an increase in their size. This process of water uptake markedly influences the optical properties of aerosols, affecting how they scatter and absorb light. These changes are crucial, as they directly impact climate and visibility, playing a significant role in atmospheric processes such as radiation balance and cloud formation.</p> <p>A thorough understanding of Mie scattering parameters for both dry and humidified (or growing) particles provides essential insights into aerosol behavior under varied humidity conditions. By accurately modeling these changes in light scattering and absorption as particles absorb water, we can enhance our predictions regarding aerosols' effects on the atmosphere.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/humid_scattering/#imports","title":"Imports\u00b6","text":"<p>To proceed with our analysis, we'll start by importing necessary libraries and modules.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/humid_scattering/#base-distribution","title":"Base Distribution\u00b6","text":"<p>This section demonstrates how to create a bimodal log-normal distribution and visualize the number concentration of particles across different diameters.</p> <p>This code snippet generates a bimodal log-normal distribution characterized by specific modes and a standard deviation. The distribution is then scaled to represent the total number of particles, and the resulting number concentration across diameters is visualized using a logarithmic scale. This approach is particularly useful for simulating aerosol size distributions in atmospheric science, enabling researchers to study particle dynamics and interactions based on their size distribution.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/humid_scattering/#hygroscopic-growth-in-aerosol-particles","title":"Hygroscopic Growth in Aerosol Particles\u00b6","text":"<p>Aerosol particles in the atmosphere can absorb water, leading to an increase in their size\u2014a process known as hygroscopic growth. This phenomenon is crucial for studying the optical properties and scattering behavior of aerosols because the particle size directly influences how light interacts with these particles.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/humid_scattering/#key-parameters-for-hygroscopic-growth","title":"Key Parameters for Hygroscopic Growth:\u00b6","text":"<ul> <li>Dry Diameter: The original size of the particles before absorbing water.</li> <li>Water Activity: Defined as the relative humidity divided by 100. It is a measure of the moisture content in the air surrounding the particles.</li> <li>Kappa Parameter: A value that represents the hygroscopicity of the particles, influencing how much water they can absorb under different humidity conditions. The kappa parameter (\u03ba) is used in the kappa-Hygroscopic Growth Factor (HGF) parameterization to model particle growth.</li> </ul> <p>For our analysis, we'll assume a kappa value of 0.61, indicative of ammonium sulfate, a common compound found in atmospheric aerosols. We will explore particle growth at three different water activities (relative humidity levels): 30%, 60%, and 90%.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/humid_scattering/#calculating-and-visualizing-hygroscopic-growth","title":"Calculating and Visualizing Hygroscopic Growth\u00b6","text":"<p>The following Python code calculates the change in particle diameters due to hygroscopic growth at low, mid, and high water activity levels and visualizes the base size distribution alongside the grown distributions.</p> <p>This code section demonstrates how aerosol particles grow in size with increasing humidity, visualized through the changes in their size distribution. Understanding these dynamics is vital for accurately assessing aerosols' environmental and climatic effects.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/humid_scattering/#refractive-index-mixing-rule","title":"Refractive Index Mixing Rule\u00b6","text":"<p>Upon determining the sizes of both wet and dry particles, we proceed to calculate the refractive index for these humidified particles. This calculation employs the Lorentz-Lorenz mixing rule, a straightforward approximation for the refractive index of a mixture comprising two substances. The mixing rule, as detailed by Liu &amp; Daum (2008), underpins the implementation in the <code>convert.effective_refractive_index</code> module, facilitating the computation of the humidified particle's refractive index.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/humid_scattering/#effective-refractive-index-calculation","title":"Effective Refractive Index Calculation\u00b6","text":"<p>The effective refractive index $n_{\\text{effective}}$ for a binary mixture can be derived using molar refraction mixing, as described in the referenced study. The formula is based on the volumes and refractive indices of the individual components:</p> <p>$$ r_{\\text{effective}} = \\frac{V_0}{V_{\\text{total}}} \\cdot \\frac{m_0 - 1}{m_0 + 2} + \\frac{V_1}{V_{\\text{total}}} \\cdot \\frac{m_1 - 1}{m_1 + 2} $$</p> <p>Where:</p> <ul> <li>$V_0$ and $V_1$ are the volumes of components 0 and 1, respectively.</li> <li>$V_{\\text{total}} = V_0 + V_1$ is the total volume.</li> <li>$m_0$ and $m_1$ are the refractive indices of components 0 and 1, respectively.</li> <li>$r_{\\text{effective}}$ is the effective specific refraction.</li> </ul> <p>The effective refractive index is then calculated from $r_{\\text{effective}}$ as follows:</p> <p>$$ n_{\\text{effective}} = \\frac{2R_{\\text{effective}} + 1}{1 - R_{\\text{effective}}} $$</p> <p>This equation allows for the calculation of the effective refractive index of the mixture, accounting for the volumetric contributions and specific refraction of each component.</p> <p>Reference Liu, Y., &amp; Daum, P. H. (2008). Relationship of refractive index to mass density and self-consistency mixing rules for multicomponent mixtures like ambient aerosols. Journal of Aerosol Science, 39(11), 974-986. https://doi.org/10.1016/j.jaerosci.2008.06.006</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/humid_scattering/#calculating-optical-properties-of-humidified-particles","title":"Calculating Optical Properties of Humidified Particles\u00b6","text":"<p>With the sizes and refractive indices of both dry and humidified particles determined, we now turn our attention to calculating their optical properties, including scattering and absorption. These properties are crucial for understanding the particles' behavior in the atmosphere and their potential climate impacts. We leverage the <code>mie_bulk.mie_size_distribution</code> function to compute these properties across different humidity conditions.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/humid_scattering/#implementation-details","title":"Implementation Details\u00b6","text":"<p>The Python code below demonstrates how to calculate the optical properties for particles at varying levels of humidity using Mie scattering theory. These calculations provide insights into how hygroscopic growth affects the particles' ability to scatter and absorb light.</p> <p>This segment calculates and then plots the extinction, scattering, and absorption coefficients for particles at different levels of hygroscopic growth. The <code>water_activity_series</code> corresponds to the range of humidity conditions under consideration, providing a clear visual representation of how water uptake influences the optical properties of aerosol particles.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/humid_scattering/#summary","title":"Summary\u00b6","text":"<p>In this notebook, we delved into the phenomenon of hygroscopic growth of aerosol particles and investigated its consequential effects on their optical characteristics. We started with the construction of a bimodal log-normal distribution, aimed at depicting the number concentration of aerosol particles across a spectrum of diameters. This foundational step allowed us to simulate the real-world complexity of aerosol size distributions.</p> <p>Moving forward, we calculated the hygroscopic growth of these particles under different humidity conditions, employing varying water activities to simulate environmental changes. Through visualization, we effectively illustrated the shifts in particle size distribution resulting from this growth, providing a clear depiction of how particles expand as they absorb moisture.</p> <p>In our subsequent analysis, we determined the refractive index of the humidified particles. This was achieved by applying the Lorentz-Lorenz mixing rule, a theoretical framework that facilitated the computation of the effective refractive index of the particles post-hydration.</p> <p>Our exploration ended in the calculation of the optical properties of aerosol particles across different levels of humidity. By doing so, we unveiled the ways in which hygroscopic growth influences the scattering and absorption behaviors of particles. The insights garnered from this analysis are instrumental in comprehending the broader implications of particle growth on atmospheric optics and climate dynamics.</p> <p>As we proceed, our focus will shift towards finding the kappa parameter based on empirical measurements of particle sizes and extinction coefficients. This reverse engineering process is a common challenge in atmospheric science, offering a deeper understanding of aerosol properties through observational data. Our exploration into this inverse problem marks the next step in bridging theoretical models with practical atmospheric measurements, enhancing our ability to interpret and predict aerosol behavior in the Earth's atmosphere.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/","title":"Fitting Kappa-HGF from Light Extinction","text":"In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# particula imports\nfrom particula.util import convert, distribution_discretization\nfrom particula_beta.data.process import kappa_via_extinction\n</pre> import numpy as np import matplotlib.pyplot as plt  # particula imports from particula.util import convert, distribution_discretization from particula_beta.data.process import kappa_via_extinction In\u00a0[3]: Copied! <pre># Hygroscopic growth parameters\nkappa = 0.61  # Kappa value for ammonium sulfate\nwater_activity_low = 0.3  # Low water activity (30% RH)\nwater_activity_mid = 0.6  # Mid water activity (60% RH)\nwater_activity_high = 0.9  # High water activity (90% RH)\n\n# Define the range and resolution of particle diameters (in nanometers)\ndiameters = np.linspace(20, 1000, 500)  # From 20 nm to 1000 nm with 500 points\n\n# Standard deviation of the log-normal distribution (dimensionless)\nsigma = 1.25\n\n# Define the modes (peak diameters) for a bimodal distribution\nmodes = [100, 500]\n\n# Total number of particles in the distribution\nnumber_total = 1e3\n\n# Generate a log-normal probability density function (pdf) with an area\n# equal to 1\npdf_dist = distribution_discretization.discretize(\n    interval=diameters,  # Array of diameters over which to compute the distribution\n    disttype=\"lognormal\",  # Type of distribution\n    gsigma=sigma,  # Geometric standard deviation\n    mode=modes,  # Modes of the distribution\n    nparticles=number_total,  # Total number of particles\n).m\n\n# Convert the distribution from pdf to probability mass function (pmf)\nnumber_conc_pms = (\n    convert.distribution_convert_pdf_pms(\n        x_array=diameters,  # Array of diameters\n        distribution=pdf_dist,  # The generated pdf\n        to_pdf=False,  # Specifies the conversion to pmf\n    )\n    * number_total\n)  # Scale to the total number of particles\n\n\n# Convert particle diameters from the base distribution to volumes\nvolume_sizer = convert.length_to_volume(diameters, length_type=\"diameter\")\n\n# Calculate water volume absorbed by particles at different RH\nvolume_water_low = convert.kappa_volume_water(\n    volume_solute=volume_sizer, kappa=kappa, water_activity=water_activity_low\n)  # showing the input parameters\nvolume_water_mid = convert.kappa_volume_water(\n    volume_sizer, kappa, water_activity_mid\n)\nvolume_water_high = convert.kappa_volume_water(\n    volume_sizer, kappa, water_activity_high\n)\n\n# Calculate new particle diameters after hygroscopic growth\ndiameters_low = convert.volume_to_length(\n    volume=volume_sizer + volume_water_low, length_type=\"diameter\"\n)  # showing the input parameters\ndiameters_mid = convert.volume_to_length(\n    volume_sizer + volume_water_mid, \"diameter\"\n)\ndiameters_high = convert.volume_to_length(\n    volume_sizer + volume_water_high, \"diameter\"\n)\n\n# Plotting the base and grown size distributions\nfig, ax = plt.subplots()\nax.plot(diameters, number_conc_pms, label=\"Base Distribution\")\nax.plot(\n    diameters_low,\n    number_conc_pms,\n    label=f\"Low Water Activity: {water_activity_low*100}% RH\",\n)\nax.plot(\n    diameters_mid,\n    number_conc_pms,\n    label=f\"Mid Water Activity: {water_activity_mid*100}% RH\",\n)\nax.plot(\n    diameters_high,\n    number_conc_pms,\n    label=f\"High Water Activity: {water_activity_high*100}% RH\",\n)\nax.set_xscale(\"log\")\nax.set_xlabel(\"Diameter (nm)\")\nax.set_ylabel(\"Number concentration (#/cm^3)\")\nax.legend()\nplt.show()\n</pre> # Hygroscopic growth parameters kappa = 0.61  # Kappa value for ammonium sulfate water_activity_low = 0.3  # Low water activity (30% RH) water_activity_mid = 0.6  # Mid water activity (60% RH) water_activity_high = 0.9  # High water activity (90% RH)  # Define the range and resolution of particle diameters (in nanometers) diameters = np.linspace(20, 1000, 500)  # From 20 nm to 1000 nm with 500 points  # Standard deviation of the log-normal distribution (dimensionless) sigma = 1.25  # Define the modes (peak diameters) for a bimodal distribution modes = [100, 500]  # Total number of particles in the distribution number_total = 1e3  # Generate a log-normal probability density function (pdf) with an area # equal to 1 pdf_dist = distribution_discretization.discretize(     interval=diameters,  # Array of diameters over which to compute the distribution     disttype=\"lognormal\",  # Type of distribution     gsigma=sigma,  # Geometric standard deviation     mode=modes,  # Modes of the distribution     nparticles=number_total,  # Total number of particles ).m  # Convert the distribution from pdf to probability mass function (pmf) number_conc_pms = (     convert.distribution_convert_pdf_pms(         x_array=diameters,  # Array of diameters         distribution=pdf_dist,  # The generated pdf         to_pdf=False,  # Specifies the conversion to pmf     )     * number_total )  # Scale to the total number of particles   # Convert particle diameters from the base distribution to volumes volume_sizer = convert.length_to_volume(diameters, length_type=\"diameter\")  # Calculate water volume absorbed by particles at different RH volume_water_low = convert.kappa_volume_water(     volume_solute=volume_sizer, kappa=kappa, water_activity=water_activity_low )  # showing the input parameters volume_water_mid = convert.kappa_volume_water(     volume_sizer, kappa, water_activity_mid ) volume_water_high = convert.kappa_volume_water(     volume_sizer, kappa, water_activity_high )  # Calculate new particle diameters after hygroscopic growth diameters_low = convert.volume_to_length(     volume=volume_sizer + volume_water_low, length_type=\"diameter\" )  # showing the input parameters diameters_mid = convert.volume_to_length(     volume_sizer + volume_water_mid, \"diameter\" ) diameters_high = convert.volume_to_length(     volume_sizer + volume_water_high, \"diameter\" )  # Plotting the base and grown size distributions fig, ax = plt.subplots() ax.plot(diameters, number_conc_pms, label=\"Base Distribution\") ax.plot(     diameters_low,     number_conc_pms,     label=f\"Low Water Activity: {water_activity_low*100}% RH\", ) ax.plot(     diameters_mid,     number_conc_pms,     label=f\"Mid Water Activity: {water_activity_mid*100}% RH\", ) ax.plot(     diameters_high,     number_conc_pms,     label=f\"High Water Activity: {water_activity_high*100}% RH\", ) ax.set_xscale(\"log\") ax.set_xlabel(\"Diameter (nm)\") ax.set_ylabel(\"Number concentration (#/cm^3)\") ax.legend() plt.show() In\u00a0[11]: Copied! <pre># Define the kappa value and refractive index for dry particles\nkappa = 0.61  # Kappa value for ammonium sulfate\nrefractive_index_dry = 1.45  # Refractive index of dry particles\n\n# Define the wavelength of light for extinction calculations\nwavelength = 450  # Wavelength in nanometers\n\n# Calculate extinction coefficients at different humidities\next_low, ext_base = kappa_via_extinction.extinction_ratio_wet_dry(\n    kappa=kappa,\n    number_per_cm3=number_conc_pms,\n    diameters=diameters,\n    water_activity_sizer=0.0,  # Base condition for sizer water activity\n    water_activity_dry=0.0,  # Dry condition water activity\n    water_activity_wet=water_activity_low,  # Low RH condition\n    refractive_index_dry=refractive_index_dry,\n    wavelength=wavelength,\n    discretize=True,\n    return_coefficients=True,\n    return_all_optics=False,\n)\next_mid, _ = kappa_via_extinction.extinction_ratio_wet_dry(\n    kappa=kappa,\n    number_per_cm3=number_conc_pms,\n    diameters=diameters,\n    water_activity_sizer=0.0,\n    water_activity_dry=0.0,\n    water_activity_wet=water_activity_mid,\n    refractive_index_dry=refractive_index_dry,\n    wavelength=wavelength,\n    discretize=True,\n    return_coefficients=True,\n    return_all_optics=False,\n)\next_high, _ = kappa_via_extinction.extinction_ratio_wet_dry(\n    kappa=kappa,\n    number_per_cm3=number_conc_pms,\n    diameters=diameters,\n    water_activity_sizer=0.0,\n    water_activity_dry=0.0,\n    water_activity_wet=water_activity_high,\n    refractive_index_dry=refractive_index_dry,\n    wavelength=wavelength,\n    discretize=True,\n    return_coefficients=True,\n    return_all_optics=False,\n)\n\n# print all extinctions\nprint(f\"Extinction at 0.0% RH: {ext_base}\")\nprint(f\"Extinction at {water_activity_low*100}% RH: {ext_low}\")\nprint(f\"Extinction at {water_activity_mid*100}% RH: {ext_mid}\")\nprint(f\"Extinction at {water_activity_high*100}% RH: {ext_high}\")\n</pre> # Define the kappa value and refractive index for dry particles kappa = 0.61  # Kappa value for ammonium sulfate refractive_index_dry = 1.45  # Refractive index of dry particles  # Define the wavelength of light for extinction calculations wavelength = 450  # Wavelength in nanometers  # Calculate extinction coefficients at different humidities ext_low, ext_base = kappa_via_extinction.extinction_ratio_wet_dry(     kappa=kappa,     number_per_cm3=number_conc_pms,     diameters=diameters,     water_activity_sizer=0.0,  # Base condition for sizer water activity     water_activity_dry=0.0,  # Dry condition water activity     water_activity_wet=water_activity_low,  # Low RH condition     refractive_index_dry=refractive_index_dry,     wavelength=wavelength,     discretize=True,     return_coefficients=True,     return_all_optics=False, ) ext_mid, _ = kappa_via_extinction.extinction_ratio_wet_dry(     kappa=kappa,     number_per_cm3=number_conc_pms,     diameters=diameters,     water_activity_sizer=0.0,     water_activity_dry=0.0,     water_activity_wet=water_activity_mid,     refractive_index_dry=refractive_index_dry,     wavelength=wavelength,     discretize=True,     return_coefficients=True,     return_all_optics=False, ) ext_high, _ = kappa_via_extinction.extinction_ratio_wet_dry(     kappa=kappa,     number_per_cm3=number_conc_pms,     diameters=diameters,     water_activity_sizer=0.0,     water_activity_dry=0.0,     water_activity_wet=water_activity_high,     refractive_index_dry=refractive_index_dry,     wavelength=wavelength,     discretize=True,     return_coefficients=True,     return_all_optics=False, )  # print all extinctions print(f\"Extinction at 0.0% RH: {ext_base}\") print(f\"Extinction at {water_activity_low*100}% RH: {ext_low}\") print(f\"Extinction at {water_activity_mid*100}% RH: {ext_mid}\") print(f\"Extinction at {water_activity_high*100}% RH: {ext_high}\") <pre>Extinction at 0.0% RH: 389.78121694944446\nExtinction at 30.0% RH: 455.3096828541398\nExtinction at 60.0% RH: 606.9502182185444\nExtinction at 90.0% RH: 1242.6228046612712\n</pre> In\u00a0[17]: Copied! <pre>fit_low = kappa_via_extinction.fit_extinction_ratio_with_kappa(\n    b_ext_dry=ext_base,\n    b_ext_wet=ext_low,\n    number_per_cm3=number_conc_pms,\n    diameters=diameters,\n    water_activity_sizer=0.0,\n    water_activity_dry=0.0,\n    water_activity_wet=water_activity_low,\n    refractive_index_dry=refractive_index_dry,\n    wavelength=wavelength,\n    discretize=True,\n    kappa_bounds=(0, 1),\n    kappa_tolerance=1e-12,\n    kappa_maxiter=200,\n)\nfit_mid = kappa_via_extinction.fit_extinction_ratio_with_kappa(\n    b_ext_dry=ext_base,\n    b_ext_wet=ext_mid,\n    number_per_cm3=number_conc_pms,\n    diameters=diameters,\n    water_activity_sizer=0.0,\n    water_activity_dry=0.0,\n    water_activity_wet=water_activity_mid,\n    refractive_index_dry=refractive_index_dry,\n    wavelength=wavelength,\n    discretize=True,\n    kappa_bounds=(0, 1),\n    kappa_tolerance=1e-12,\n    kappa_maxiter=200,\n)\nfit_high = kappa_via_extinction.fit_extinction_ratio_with_kappa(\n    b_ext_dry=ext_base,\n    b_ext_wet=ext_high,\n    number_per_cm3=number_conc_pms,\n    diameters=diameters,\n    water_activity_sizer=0.0,\n    water_activity_dry=0.0,\n    water_activity_wet=water_activity_high,\n    refractive_index_dry=refractive_index_dry,\n    wavelength=wavelength,\n    discretize=True,\n    kappa_bounds=(0, 1),\n    kappa_tolerance=1e-12,\n    kappa_maxiter=200,\n)\n\n# print all fits\n\nprint(f\"Kappa at {water_activity_low*100}% RH: {fit_low}\")\nprint(f\"Kappa at {water_activity_mid*100}% RH: {fit_mid}\")\nprint(f\"Kappa at {water_activity_high*100}% RH: {fit_high}\")\n</pre> fit_low = kappa_via_extinction.fit_extinction_ratio_with_kappa(     b_ext_dry=ext_base,     b_ext_wet=ext_low,     number_per_cm3=number_conc_pms,     diameters=diameters,     water_activity_sizer=0.0,     water_activity_dry=0.0,     water_activity_wet=water_activity_low,     refractive_index_dry=refractive_index_dry,     wavelength=wavelength,     discretize=True,     kappa_bounds=(0, 1),     kappa_tolerance=1e-12,     kappa_maxiter=200, ) fit_mid = kappa_via_extinction.fit_extinction_ratio_with_kappa(     b_ext_dry=ext_base,     b_ext_wet=ext_mid,     number_per_cm3=number_conc_pms,     diameters=diameters,     water_activity_sizer=0.0,     water_activity_dry=0.0,     water_activity_wet=water_activity_mid,     refractive_index_dry=refractive_index_dry,     wavelength=wavelength,     discretize=True,     kappa_bounds=(0, 1),     kappa_tolerance=1e-12,     kappa_maxiter=200, ) fit_high = kappa_via_extinction.fit_extinction_ratio_with_kappa(     b_ext_dry=ext_base,     b_ext_wet=ext_high,     number_per_cm3=number_conc_pms,     diameters=diameters,     water_activity_sizer=0.0,     water_activity_dry=0.0,     water_activity_wet=water_activity_high,     refractive_index_dry=refractive_index_dry,     wavelength=wavelength,     discretize=True,     kappa_bounds=(0, 1),     kappa_tolerance=1e-12,     kappa_maxiter=200, )  # print all fits  print(f\"Kappa at {water_activity_low*100}% RH: {fit_low}\") print(f\"Kappa at {water_activity_mid*100}% RH: {fit_mid}\") print(f\"Kappa at {water_activity_high*100}% RH: {fit_high}\") <pre>Kappa at 30.0% RH: 0.6052222023205297\nKappa at 60.0% RH: 0.610000003943248\nKappa at 90.0% RH: 0.610062704938107\n</pre> In\u00a0[19]: Copied! <pre># Perform kappa-HGF fitting with a humid sizer at mid RH\nfit_humid_sizer_mid = kappa_via_extinction.fit_extinction_ratio_with_kappa(\n    b_ext_dry=ext_base,\n    b_ext_wet=ext_mid,\n    number_per_cm3=number_conc_pms,\n    diameters=diameters_mid,\n    water_activity_sizer=water_activity_mid,  # Sizer humidity set to mid RH\n    water_activity_dry=0.0,  # Dry condition\n    water_activity_wet=water_activity_mid,  # Wet condition matches sizer RH\n    refractive_index_dry=refractive_index_dry,\n    wavelength=wavelength,\n    discretize=True,\n    kappa_bounds=(0, 1),\n    kappa_tolerance=1e-12,\n    kappa_maxiter=200,\n)\n\n# Repeat fitting with humid sizer at mid RH and extinction measurements at\n# low and high RH\nfit_humid_sizer_mid_and_humid_ext_low = (\n    kappa_via_extinction.fit_extinction_ratio_with_kappa(\n        b_ext_dry=ext_low,  # Dry extinction at low RH\n        b_ext_wet=ext_high,  # Wet extinction at high RH\n        number_per_cm3=number_conc_pms,\n        diameters=diameters_mid,  # Diameters adjusted for mid RH\n        water_activity_sizer=water_activity_mid,  # Mid RH for sizer\n        water_activity_dry=water_activity_low,  # Low RH for dry condition\n        water_activity_wet=water_activity_high,  # High RH for wet condition\n        refractive_index_dry=refractive_index_dry,\n        wavelength=wavelength,\n        discretize=True,\n        kappa_bounds=(0, 1),\n        kappa_tolerance=1e-12,\n        kappa_maxiter=200,\n    )\n)\n\n# Print the fitted kappa values\nprint(\n    f\"Kappa from humid sizer at {water_activity_mid*100}% RH and dry Ext: {fit_humid_sizer_mid}\"\n)\nprint(\n    f\"Kappa from humid sizer at {water_activity_mid*100}% RH and humid Ext from {water_activity_low*100}% RH to {water_activity_high*100}% RH: {fit_humid_sizer_mid_and_humid_ext_low}\"\n)\n</pre> # Perform kappa-HGF fitting with a humid sizer at mid RH fit_humid_sizer_mid = kappa_via_extinction.fit_extinction_ratio_with_kappa(     b_ext_dry=ext_base,     b_ext_wet=ext_mid,     number_per_cm3=number_conc_pms,     diameters=diameters_mid,     water_activity_sizer=water_activity_mid,  # Sizer humidity set to mid RH     water_activity_dry=0.0,  # Dry condition     water_activity_wet=water_activity_mid,  # Wet condition matches sizer RH     refractive_index_dry=refractive_index_dry,     wavelength=wavelength,     discretize=True,     kappa_bounds=(0, 1),     kappa_tolerance=1e-12,     kappa_maxiter=200, )  # Repeat fitting with humid sizer at mid RH and extinction measurements at # low and high RH fit_humid_sizer_mid_and_humid_ext_low = (     kappa_via_extinction.fit_extinction_ratio_with_kappa(         b_ext_dry=ext_low,  # Dry extinction at low RH         b_ext_wet=ext_high,  # Wet extinction at high RH         number_per_cm3=number_conc_pms,         diameters=diameters_mid,  # Diameters adjusted for mid RH         water_activity_sizer=water_activity_mid,  # Mid RH for sizer         water_activity_dry=water_activity_low,  # Low RH for dry condition         water_activity_wet=water_activity_high,  # High RH for wet condition         refractive_index_dry=refractive_index_dry,         wavelength=wavelength,         discretize=True,         kappa_bounds=(0, 1),         kappa_tolerance=1e-12,         kappa_maxiter=200,     ) )  # Print the fitted kappa values print(     f\"Kappa from humid sizer at {water_activity_mid*100}% RH and dry Ext: {fit_humid_sizer_mid}\" ) print(     f\"Kappa from humid sizer at {water_activity_mid*100}% RH and humid Ext from {water_activity_low*100}% RH to {water_activity_high*100}% RH: {fit_humid_sizer_mid_and_humid_ext_low}\" ) <pre>Kappa 0.6100000005341284 from humid sizer 60.0% RH and dry Ext: \nKappa 0.6099999998804534 from humid sizer 60.0% RH and humid Ext 30.0% RH: \n</pre> In\u00a0[30]: Copied! <pre># Fitting kappa-HGF with varied assumptions on the dry refractive index\nfit_refractive_index_lower = (\n    kappa_via_extinction.fit_extinction_ratio_with_kappa(\n        b_ext_dry=ext_base,\n        b_ext_wet=ext_mid,\n        number_per_cm3=number_conc_pms,\n        diameters=diameters,\n        water_activity_sizer=0.0,\n        water_activity_dry=0.0,\n        water_activity_wet=water_activity_mid,\n        refractive_index_dry=1.4,  # Assumption of a lower refractive index\n        wavelength=wavelength,\n        kappa_bounds=(0, 1),\n        kappa_tolerance=1e-12,\n        kappa_maxiter=200,\n    )\n)\n\nfit_refractive_index_higher = (\n    kappa_via_extinction.fit_extinction_ratio_with_kappa(\n        b_ext_dry=ext_base,\n        b_ext_wet=ext_mid,\n        number_per_cm3=number_conc_pms,\n        diameters=diameters,\n        water_activity_sizer=0.0,\n        water_activity_dry=0.0,\n        water_activity_wet=water_activity_mid,\n        refractive_index_dry=1.55,  # Assumption of a higher refractive index\n        wavelength=wavelength,\n        kappa_bounds=(0, 1),\n        kappa_tolerance=1e-12,\n        kappa_maxiter=200,\n    )\n)\n\nfit_refractive_index_close = (\n    kappa_via_extinction.fit_extinction_ratio_with_kappa(\n        b_ext_dry=ext_base,\n        b_ext_wet=ext_mid,\n        number_per_cm3=number_conc_pms,\n        diameters=diameters,\n        water_activity_sizer=0.0,\n        water_activity_dry=0.0,\n        water_activity_wet=water_activity_mid,\n        refractive_index_dry=1.48,  # A close but distinct refractive index\n        wavelength=wavelength,\n        kappa_bounds=(0, 1),\n        kappa_tolerance=1e-12,\n        kappa_maxiter=200,\n    )\n)\n\n# Output the fitted kappa values for each refractive index assumption\nprint(f\"Kappa from lower refractive index 1.4: {fit_refractive_index_lower}\")\nprint(\n    f\"Kappa from higher refractive index 1.55: {fit_refractive_index_higher}\"\n)\nprint(f\"Kappa from close refractive index 1.48: {fit_refractive_index_close}\")\n</pre> # Fitting kappa-HGF with varied assumptions on the dry refractive index fit_refractive_index_lower = (     kappa_via_extinction.fit_extinction_ratio_with_kappa(         b_ext_dry=ext_base,         b_ext_wet=ext_mid,         number_per_cm3=number_conc_pms,         diameters=diameters,         water_activity_sizer=0.0,         water_activity_dry=0.0,         water_activity_wet=water_activity_mid,         refractive_index_dry=1.4,  # Assumption of a lower refractive index         wavelength=wavelength,         kappa_bounds=(0, 1),         kappa_tolerance=1e-12,         kappa_maxiter=200,     ) )  fit_refractive_index_higher = (     kappa_via_extinction.fit_extinction_ratio_with_kappa(         b_ext_dry=ext_base,         b_ext_wet=ext_mid,         number_per_cm3=number_conc_pms,         diameters=diameters,         water_activity_sizer=0.0,         water_activity_dry=0.0,         water_activity_wet=water_activity_mid,         refractive_index_dry=1.55,  # Assumption of a higher refractive index         wavelength=wavelength,         kappa_bounds=(0, 1),         kappa_tolerance=1e-12,         kappa_maxiter=200,     ) )  fit_refractive_index_close = (     kappa_via_extinction.fit_extinction_ratio_with_kappa(         b_ext_dry=ext_base,         b_ext_wet=ext_mid,         number_per_cm3=number_conc_pms,         diameters=diameters,         water_activity_sizer=0.0,         water_activity_dry=0.0,         water_activity_wet=water_activity_mid,         refractive_index_dry=1.48,  # A close but distinct refractive index         wavelength=wavelength,         kappa_bounds=(0, 1),         kappa_tolerance=1e-12,         kappa_maxiter=200,     ) )  # Output the fitted kappa values for each refractive index assumption print(f\"Kappa from lower refractive index 1.4: {fit_refractive_index_lower}\") print(     f\"Kappa from higher refractive index 1.55: {fit_refractive_index_higher}\" ) print(f\"Kappa from close refractive index 1.48: {fit_refractive_index_close}\") <pre>Kappa from lower refractive index 1.4: 0.5112947684591423\nKappa from higher refractive index 1.55: 0.7043352860513862\nKappa from close refractive index 1.48: 0.6563062276765905\n</pre> In\u00a0[32]: Copied! <pre>fit_low_number = kappa_via_extinction.fit_extinction_ratio_with_kappa(\n    b_ext_dry=ext_base,\n    b_ext_wet=ext_high,\n    number_per_cm3=number_conc_pms\n    * 0.2,  # 20% of the base number concentration\n    diameters=diameters,\n    water_activity_sizer=0.0,\n    water_activity_dry=0.0,\n    water_activity_wet=water_activity_high,\n    refractive_index_dry=refractive_index_dry,\n    wavelength=wavelength,\n    discretize=True,\n    kappa_bounds=(0, 1),\n    kappa_tolerance=1e-12,\n    kappa_maxiter=200,\n)\n# print result\nprint(\n    f\"Kappa at {water_activity_high*100}% RH with 20% of the particles: {fit_low_number}\"\n)\n</pre> fit_low_number = kappa_via_extinction.fit_extinction_ratio_with_kappa(     b_ext_dry=ext_base,     b_ext_wet=ext_high,     number_per_cm3=number_conc_pms     * 0.2,  # 20% of the base number concentration     diameters=diameters,     water_activity_sizer=0.0,     water_activity_dry=0.0,     water_activity_wet=water_activity_high,     refractive_index_dry=refractive_index_dry,     wavelength=wavelength,     discretize=True,     kappa_bounds=(0, 1),     kappa_tolerance=1e-12,     kappa_maxiter=200, ) # print result print(     f\"Kappa at {water_activity_high*100}% RH with 20% of the particles: {fit_low_number}\" ) <pre>Kappa at 90.0% RH with 20% of the particles: 0.6100627049381072\n</pre> In\u00a0[33]: Copied! <pre>help(kappa_via_extinction.extinction_ratio_wet_dry)\n</pre> help(kappa_via_extinction.extinction_ratio_wet_dry) <pre>Help on function extinction_ratio_wet_dry in module particula_beta.data.process.kappa_via_extinction:\n\nextinction_ratio_wet_dry(kappa: Union[float, numpy.ndarray[Any, numpy.dtype[numpy.float64]]], number_per_cm3: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], diameters: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], water_activity_sizer: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], water_activity_dry: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], water_activity_wet: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], refractive_index_dry: Union[complex, float] = 1.45, water_refractive_index: Union[complex, float] = 1.33, wavelength: float = 450, discretize: bool = True, return_coefficients: bool = False, return_all_optics: bool = False) -&gt; Union[float, Tuple[numpy.ndarray[Any, numpy.dtype[+_ScalarType_co]], numpy.ndarray[Any, numpy.dtype[+_ScalarType_co]]]]\n    Calculates the extinction ratio between wet and dry aerosols, considering\n    water uptake through kappa. This function uses Mie theory to determine the\n    optical properties of aerosols with varying water content, allowing for\n    analysis of hygroscopic growth and its impact on aerosol optical\n    characteristics.\n    \n    Parameters\n    ----------\n    kappa : Union[float, NDArray[np.float64]]\n        Hygroscopicity parameter, defining water uptake ability of particles.\n    number_per_cm3 : NDArray[np.float64]\n        Number concentration of particles per cubic centimeter for each size\n        bin.\n    diameters : NDArray[np.float64]\n        Diameters of particles in nanometers for each size bin.\n    water_activity_sizer : NDArray[np.float64]\n        Water activity of the aerosol size distribution.\n    water_activity_dry : NDArray[np.float64]\n        Water activity for the calculation of 'dry' aerosol properties.\n    water_activity_wet : NDArray[np.float64]\n        Water activity for the calculation of 'wet' aerosol properties.\n    refractive_index_dry : Union[complex, float, np.float16], optional\n        Refractive index of the dry aerosol particles.\n    water_refractive_index : Union[complex, float], optional\n        Refractive index of water.\n    wavelength : float, optional\n        Wavelength of the incident light in nanometers.\n    discretize : bool, optional\n        If True, discretizes input parameters for Mie calculations to enable\n        caching.\n    return_coefficients : bool, optional\n        If True, returns the individual extinction coefficients for wet and\n        dry aerosols instead of their ratio.\n    return_all_optics : bool, optional\n        If True, returns all optical properties calculated by Mie theory,\n        not just extinction.\n    \n    Returns\n    -------\n    Union[float, Tuple[NDArray, NDArray]]\n        By default, returns the ratio of wet to dry aerosol extinction.\n        If `return_coefficients` is True, returns a tuple of NDArrays\n        containing the extinction coefficients for wet and dry aerosols,\n        respectively.\n\n</pre> In\u00a0[34]: Copied! <pre>help(kappa_via_extinction.fit_extinction_ratio_with_kappa)\n</pre> help(kappa_via_extinction.fit_extinction_ratio_with_kappa) <pre>Help on function fit_extinction_ratio_with_kappa in module particula_beta.data.process.kappa_via_extinction:\n\nfit_extinction_ratio_with_kappa(b_ext_dry: Union[float, numpy.float64], b_ext_wet: Union[float, numpy.float64], number_per_cm3: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], diameters: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], water_activity_sizer: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], water_activity_dry: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], water_activity_wet: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], refractive_index_dry: Union[complex, float] = 1.45, water_refractive_index: Union[complex, float] = 1.33, wavelength: float = 450, discretize: bool = True, kappa_bounds: Tuple[float, float] = (0, 1), kappa_tolerance: float = 1e-06, kappa_maxiter: int = 100) -&gt; Union[float, numpy.float64]\n    Fits the kappa parameter based on the measured extinction ratios of dry\n    and wet aerosols, utilizing Mie theory to account for water uptake\n    effects. This method optimizes kappa to minimize the difference between\n    the calculated and observed extinction ratio of wet to dry aerosols.\n    \n    Parameters\n    ----------\n    b_ext_dry : Union[float, np.float64]\n        The measured extinction of the dry aerosol.\n    b_ext_wet : Union[float, np.float64]\n        The measured extinction of the wet aerosol.\n    number_per_cm3 : NDArray[np.float64]\n        The number concentration of particles per cubic centimeter for each\n        size bin.\n    diameters : NDArray[np.float64]\n        The diameters of particles in nanometers for each size bin.\n    water_activity_sizer : NDArray[np.float64]\n        The water activity corresponding to the aerosol size distribution.\n    water_activity_dry : NDArray[np.float64]\n        The water activity for the 'dry' aerosol condition.\n    water_activity_wet : NDArray[np.float64]\n        The water activity for the 'wet' aerosol condition.\n    refractive_index_dry : Union[complex, float, np.float16], optional\n        The refractive index of the dry aerosol particles.\n    water_refractive_index : Union[complex, float], optional\n        The refractive index of water.\n    wavelength : float, optional\n        The wavelength of incident light in nanometers.\n    discretize : bool, optional\n        If True, discretizes input parameters for Mie calculations to enable\n        caching.\n    kappa_bounds : Tuple[float, float], optional\n        The bounds within which to fit the kappa parameter.\n    kappa_tolerance : float, optional\n        The tolerance level for the optimization of kappa.\n    kappa_maxiter : int, optional\n        The maximum number of iterations allowed in the optimization process.\n    \n    Returns\n    -------\n    Union[float, np.float64]\n        The optimized kappa parameter that best fits the observed extinction\n        ratios.\n\n</pre> In\u00a0[37]: Copied! <pre>help(kappa_via_extinction.kappa_from_extinction_looped)\n</pre> help(kappa_via_extinction.kappa_from_extinction_looped) <pre>Help on function kappa_from_extinction_looped in module particula_beta.data.process.kappa_via_extinction:\n\nkappa_from_extinction_looped(extinction_dry: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], extinction_wet: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], number_per_cm3: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], diameter: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], water_activity_sizer: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], water_activity_sample_dry: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], water_activity_sample_wet: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], refractive_index_dry: Union[complex, float] = 1.45, water_refractive_index: Union[complex, float] = 1.33, wavelength: float = 450, discretize: bool = True) -&gt; numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]]\n    Fits the extinction ratio to the kappa value for a given set of\n    measurements, looping over time indexes in number_per_cm3. This function\n    is tailored for analyzing data from a CAPS (Cavity Attenuated Phase Shift)\n    instrument under varying humidities.\n    \n    Parameters\n    ----------\n    extinction_dry : NDArray[np.float64]\n        Array of dry aerosol extinction measurements.\n    extinction_wet : NDArray[np.float64]\n        Array of wet aerosol extinction measurements.\n    number_per_cm3 : NDArray[np.float64]\n        Array of particle number concentrations in #/cm\u00b3.\n    diameter : NDArray[np.float64]\n        Array of particle diameters.\n    water_activity_sizer : NDArray[np.float64]\n        Water activity (relative humidity/100) of the sizing instrument's air.\n    water_activity_sample_dry : NDArray[np.float64]\n        Water activity (relative humidity/100) of the air for dry measurements.\n    water_activity_sample_wet : NDArray[np.float64]\n        Water activity (relative humidity/100) of the air for wet measurements.\n    refractive_index_dry : Union[complex, float], optional\n        Refractive index of dry particles. Default is 1.45.\n    water_refractive_index : Union[complex, float], optional\n        Refractive index of water. Default is 1.33.\n    wavelength : float, optional\n        Wavelength of the light source in nanometers. Default is 450.\n    discretize : bool, optional\n        If True, calculations are performed with discretized parameter values\n        to potentially improve performance. Default is True.\n    \n    Returns\n    -------\n    NDArray[np.float64]\n        A 2D array where each row corresponds to the time-indexed kappa value,\n        lower and upper bounds of the kappa estimation, structured as\n        [kappa, lower, upper].\n\n</pre>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#fitting-kappa-hgf-from-light-extinction","title":"Fitting Kappa-HGF from Light Extinction\u00b6","text":"<p>In the field of atmospheric science, understanding the hygroscopic growth of aerosol particles is crucial for predicting their impact on climate, weather patterns, and human health. The kappa-Hygroscopic Growth Factor (kappa-HGF) is a parameter that quantifies the ability of aerosol particles to absorb water under varying levels of relative humidity. However, kappa-HGF is not measured directly in atmospheric observations. Instead, researchers infer this parameter by analyzing changes in aerosol properties, such as light extinction, diameter growth, or light scattering, as a function of humidity.</p> <p>This notebook focuses on deriving kappa-HGF from light extinction measurements at different humidity levels. Light extinction, which encompasses both scattering and absorption by aerosol particles, is a fundamental optical property that can significantly vary with particle size and composition, especially as particles undergo hygroscopic growth in humid conditions. By comparing light extinction under low and high humidity conditions, we can back-calculate kappa-HGF, offering insights into the water uptake and swelling behavior of aerosols in the atmosphere.</p> <p>The methodology and analyses presented here for light extinction can be analogously applied to other aerosol properties affected by humidity changes. Understanding these dynamics is key to improving our models of aerosol-cloud interactions, radiative forcing, and visibility impairment.</p> <p>Imports</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#generating-synthetic-data-for-kappa-hgf-analysis","title":"Generating Synthetic Data for Kappa-HGF Analysis\u00b6","text":"<p>To conduct a meaningful analysis of kappa-HGF (Hygroscopic Growth Factor) based on light extinction, we first need to simulate synthetic data that closely mimics atmospheric aerosol properties. This involves creating a size distribution of aerosol particles and calculating their light extinction coefficients under different humidity conditions.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#hygroscopic-growth-parameters","title":"Hygroscopic Growth Parameters\u00b6","text":"<p>For our synthetic dataset, we consider ammonium sulfate as the aerosol composition with a kappa value of 0.61. We examine the particle behavior at three distinct relative humidity (RH) levels: 30%, 60%, and 90%, representing low, mid, and high water activity scenarios, respectively.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#size-distribution","title":"Size Distribution\u00b6","text":"<p>The synthetic size distribution is modeled as a bimodal log-normal distribution, capturing the diversity in particle sizes typically observed in atmospheric aerosols. The distribution spans from 20 nm to 1000 nm with 500 discrete points, encompassing a wide range of aerosol sizes. The distribution is characterized by two modes at 100 nm and 500 nm and a geometric standard deviation of 1.25, simulating a realistic atmospheric aerosol size distribution.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#calculation-steps","title":"Calculation Steps\u00b6","text":"<ol> <li><p>Volume Conversion: Convert the diameters from the base distribution to volumes, facilitating the subsequent calculation of water volume absorbed by the particles at different RH levels.</p> </li> <li><p>Water Volume Calculation: For each RH level, calculate the volume of water absorbed by the particles, leveraging the kappa value to simulate hygroscopic growth.</p> </li> <li><p>Diameter Adjustment: Adjust the particle diameters based on the absorbed water volume to obtain new diameters that reflect hygroscopic growth at each RH level.</p> </li> </ol>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#simulating-light-extinction-for-hygroscopic-growth","title":"Simulating Light Extinction for Hygroscopic Growth\u00b6","text":"<p>After generating synthetic aerosol distributions and analyzing their hygroscopic growth, the next crucial step is to simulate how these changes affect the aerosols' light extinction properties at different humidity levels. Light extinction, a key optical property, combines the effects of both scattering and absorption by particles, and varies significantly with particle size, composition, and ambient humidity. In this section, we calculate the extinction coefficients for our simulated aerosol distributions at low, mid, and high relative humidity (RH) conditions.</p> <p>To simulate the extinction at each specified humidity, we employ the <code>extinction_ratio_wet_dry</code> function from the <code>kappa_via_extinction</code> module. This function calculates the extinction coefficients by considering the hygroscopic growth of particles and their resulting optical properties.</p> <p>Note: The size distribution hygroscopic growth and refractive index mixing are handled internally by the <code>extinction_ratio_wet_dry</code> function, so we only need to provide the base size distribution and the RH levels of interest.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#fitting-kappa-hgf-to-dry-aerosol-data","title":"Fitting Kappa-HGF to Dry Aerosol Data\u00b6","text":"<p>After generating synthetic aerosol distributions and simulating their light extinction at various humidity levels, we proceed to fit the kappa-Hygroscopic Growth Factor (kappa-HGF) to our synthetic data. This process involves using the extinction data previously calculated to infer the kappa-HGF values that best describe the observed changes in light extinction due to hygroscopic growth.</p> <p>To achieve this, we employ the <code>fit_extinction_ratio_with_kappa</code> function from the <code>kappa_via_extinction</code> module. This function optimizes the kappa value to match the simulated wet-to-dry extinction ratio, providing a quantitative measure of the particles' hygroscopicity. The fitting is performed for low, mid, and high humidity conditions, allowing us to observe how kappa-HGF varies with relative humidity (RH) of the measurement environment. It should be relatively close to the kappa value used in the synthetic data generation (0.61).</p> <p>This code snippet demonstrates the process of fitting kappa-HGF to the synthetic data, highlighting the kappa values obtained for different RH conditions. The fitting process is crucial for translating the wet-to-dry extinction ratios into a meaningful measure of particle hygroscopicity, which can then be used to improve our understanding of aerosol behavior in the atmosphere.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#insensitivity-to-humidity-in-sizer-and-extinction-measurements","title":"Insensitivity to Humidity in Sizer and Extinction Measurements\u00b6","text":"<p>In the complex world of atmospheric measurements, not all data are perfect. One of the challenges is dealing with the humidity effects on both the size distribution measurements and the extinction coefficients for dry and wet aerosol particles. This section demonstrates that by carefully accounting for humidity in these measurements, the fitted kappa-Hygroscopic Growth Factor (kappa-HGF) remains robust and insensitive to variations in the size distribution caused by humidity. This resilience is particularly crucial when dealing with ambient measurements, where the assumption of perfectly dry particles may not hold.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#fitting-kappa-hgf-under-varied-humidity-conditions","title":"Fitting Kappa-HGF under Varied Humidity Conditions\u00b6","text":"<p>To illustrate this point, we perform kappa-HGF fitting under conditions where the sizer humidity is not zero, simulating more realistic scenarios where dry particles may still retain some moisture. We explore how kappa-HGF responds to changes in the humidity of the sizer used to measure the size distribution and the humidity conditions under which dry and wet extinction coefficients are obtained.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#key-insights","title":"Key Insights\u00b6","text":"<p>The kappa-HGF fitting outcomes demonstrate that the approach remains effective even when the sizer's humidity and the conditions for measuring dry and wet extinctions incorporate realistic humidity levels. This robustness ensures that kappa-HGF can be accurately determined from ambient aerosol measurements, accommodating the inherent moisture that may be present in \"dry\" aerosol samples.</p> <p>Understanding and adjusting for these nuances in humidity is essential for accurately characterizing aerosol optical properties and their environmental impacts. By acknowledging and accounting for these factors in our analysis, we enhance the reliability of kappa-HGF estimations, paving the way for more accurate atmospheric models and predictions.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#examining-the-impact-of-dry-refractive-index-on-kappa-hgf-estimation","title":"Examining the Impact of Dry Refractive Index on Kappa-HGF Estimation\u00b6","text":"<p>The refractive index of aerosol particles plays a pivotal role in determining their optical properties, such as light extinction. When deducing the kappa-Hygroscopic Growth Factor (kappa-HGF) from light extinction data, the choice of the dry aerosol particle's refractive index can significantly influence the precision of the resulting kappa estimation. This exploration delves into the sensitivity of the kappa-HGF fitting process to variations in the assumed dry refractive index, underscoring the necessity of accurately selecting this parameter for reliable hygroscopicity assessments.</p> <p>Initially, we assumed a dry refractive index of (1.45 + 0i) for generating extinction data. Here, we venture into how alterations in this assumed refractive index affect the kappa-HGF fitting outcomes. Specifically, we examine the response of the kappa-HGF fitting to both modest deviations and more pronounced shifts from the initially assumed value, including both lower and higher indices, to ascertain the robustness of kappa estimation against such variations.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#robustness-of-kappa-hgf-estimation-against-particle-number-variations","title":"Robustness of Kappa-HGF Estimation Against Particle Number Variations\u00b6","text":"<p>A noteworthy aspect of estimating the kappa-Hygroscopic Growth Factor (kappa-HGF) from light extinction measurements is its robustness to variations in the total number of aerosol particles. This resilience is attributed to the optimization process focusing on the extinction ratio rather than absolute extinction values. Consequently, discrepancies in the total particle number do not significantly affect the kappa-HGF fitting outcomes. This section demonstrates this principle by applying kappa-HGF fitting to synthetic datasets with deliberately altered particle number concentrations.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#demonstrating-insensitivity-through-particle-number-variation","title":"Demonstrating Insensitivity through Particle Number Variation\u00b6","text":"<p>The experiment involves adjusting the particle number concentration to 20% of its original value in the synthetic dataset. Despite this substantial reduction, we observe that the kappa-HGF estimation process yields consistent results, underscoring the fitting technique's insensitivity to the absolute particle count within the aerosol distribution. This characteristic is particularly advantageous when dealing with ambient measurements, where precise particle counting may be challenging.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#implications-for-ambient-aerosol-measurements","title":"Implications for Ambient Aerosol Measurements\u00b6","text":"<p>The findings from this sensitivity analysis have implications for conducting ambient aerosol measurements and analyses. They highlight the fitting process's adaptability and accuracy, even when the exact particle count is uncertain or varies across measurements. As long as the relative distribution of particle sizes remains representative of the aerosol population under study, kappa-HGF estimates can be reliably derived, facilitating accurate assessments of aerosol hygroscopicity and its atmospheric impacts.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#summary-of-kappa-hgf-fitting-notebook","title":"Summary of Kappa-HGF Fitting Notebook\u00b6","text":"<p>This notebook delves into the intricacies of fitting the kappa-Hygroscopic Growth Factor (kappa-HGF) using synthetic aerosol data. The kappa-HGF is an essential parameter in atmospheric sciences, quantifying the water-absorbing capabilities of aerosol particles and their influence on cloud formation and climate. The notebook methodically guides through generating synthetic aerosol size distributions, simulating their light extinction under various humidity conditions, and employing these simulations to estimate kappa-HGF. Key highlights include:</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#synthetic-data-generation","title":"Synthetic Data Generation\u00b6","text":"<ul> <li>Aerosol Size Distribution: We created synthetic bimodal log-normal size distributions, representing atmospheric aerosol populations. This step is crucial for simulating realistic aerosol behaviors under varying environmental conditions.</li> <li>Hygroscopic Growth Simulation: By adjusting the size distribution for different relative humidity levels, we simulated the hygroscopic growth of aerosols, providing a foundation for subsequent extinction calculations.</li> <li>Extinction Coefficients: We calculated light extinction coefficients for the synthetic aerosol distributions at low, mid, and high humidity levels.</li> </ul>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#kappa-hgf-fitting","title":"Kappa-HGF Fitting\u00b6","text":"<ul> <li>Insensitivity to Humidity in Measurements: Demonstrating the fitting process's robustness, we showed that kappa-HGF estimation is insensitive to variations in humidity during size distribution measurements, ensuring reliable kappa estimations from ambient aerosol data.</li> <li>Sensitivity to Dry Refractive Index: Exploring the impact of the assumed dry refractive index on kappa-HGF fitting revealed the importance of accurately selecting this parameter for precise hygroscopicity assessments.</li> <li>Insensitivity Against Particle Number: The kappa-HGF fitting process proved to be resilient to changes in the total number of aerosol particles, emphasizing the method's suitability for ambient aerosol studies where particle counts may fluctuate.</li> </ul>"},{"location":"How-To-Guides/Light_Scattering/notebooks/kappa_vs_extinction/#conclusions-and-implications","title":"Conclusions and Implications\u00b6","text":"<p>The notebook's findings underscore the efficacy and reliability of the kappa-HGF fitting process, even under the inherent variability and uncertainties of atmospheric aerosol measurements. By accurately estimating kappa-HGF from light extinction data, scientists can better understand aerosol hygroscopic growth, improving models of aerosol-cloud interactions and assessing aerosols' environmental and climatic impacts. This work highlights the critical role of precise parameter selection and the robustness of the kappa-HGF estimation process, contributing valuable insights to the field of atmospheric sciences.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/mie_basics/","title":"Mie Scattering","text":"In\u00a0[1]: Copied! <pre>import numpy as np\n\n# particula imports\nfrom particula_beta.data.process import mie_bulk\n</pre> import numpy as np  # particula imports from particula_beta.data.process import mie_bulk In\u00a0[6]: Copied! <pre># Calculate Mie coefficients for a non absorbing sphere\nmie_coefficients = mie_bulk.discretize_auto_mieq(\n    m_sphere=1.5, wavelength=550.0, diameter=200.0\n)\n# Calculate Mie coefficients for a absorbing sphere\nmie_coefficients_abs = mie_bulk.discretize_auto_mieq(\n    m_sphere=1.5 + 0.5j, wavelength=550.0, diameter=200.0\n)\n\n# Print the table header, with padding spaces\nprint(\n    f\"{'Property':&lt;10} {'Non-absorbing Sphere':&lt;25} {'Absorbing Sphere':&lt;25}\"\n)\n# Print each row of Q efficiencies\nproperties = [\"Qext\", \"Qsca\", \"Qabs\", \"g\", \"Qpr\", \"Qback\", \"Qratio\"]\nfor i, prop in enumerate(properties):\n    print(\n        f\"{prop:&lt;10} {mie_coefficients[i]:&lt;25} {mie_coefficients_abs[i]:&lt;25}\"\n    )\n</pre> # Calculate Mie coefficients for a non absorbing sphere mie_coefficients = mie_bulk.discretize_auto_mieq(     m_sphere=1.5, wavelength=550.0, diameter=200.0 ) # Calculate Mie coefficients for a absorbing sphere mie_coefficients_abs = mie_bulk.discretize_auto_mieq(     m_sphere=1.5 + 0.5j, wavelength=550.0, diameter=200.0 )  # Print the table header, with padding spaces print(     f\"{'Property':&lt;10} {'Non-absorbing Sphere':&lt;25} {'Absorbing Sphere':&lt;25}\" ) # Print each row of Q efficiencies properties = [\"Qext\", \"Qsca\", \"Qabs\", \"g\", \"Qpr\", \"Qback\", \"Qratio\"] for i, prop in enumerate(properties):     print(         f\"{prop:&lt;10} {mie_coefficients[i]:&lt;25} {mie_coefficients_abs[i]:&lt;25}\"     ) <pre>Property   Non-absorbing Sphere      Absorbing Sphere         \nQext       0.3381182292121606        1.6386392163349293       \nQsca       0.33811822921216056       0.44129028057104036      \nQabs       5.551115123125783e-17     1.197348935763889        \ng          0.26718570666763686       0.2920132136174058       \nQpr        0.24777787120289949       1.5097766233672532       \nQback      0.23099105902557251       0.26574242780352025      \nQratio     0.6831665348650324        0.6021941554199722       \n</pre> In\u00a0[8]: Copied! <pre># Define the refractive index of the particles (complex for absorbing,\n# real for non-absorbing)\nm_sphere = 1.5 + 0.01j  # Example for slightly absorbing particles\n\n# Define the wavelength of the incident light in nanometers\nwavelength = 550.0\n\n# Create a linearly spaced array of particle diameters in nanometers\n# From 100 nm to 300 nm, 50 sizes\ndiameter_sizes = np.linspace(100.0, 500.0, 50)\n\n# Simulate a number concentration for each particle size in #/cm\u00b3\n# Decreasing concentration from 1000 to 500 #/cm\u00b3\nnumber_per_cm3 = np.linspace(1000.0, 500.0, 50)\n\n# Refractive index of the surrounding medium (e.g., air)\nn_medium = 1.0\n\n\n# Calculate the Mie scattering parameters for the given size distribution\nmie_results = mie_bulk.mie_size_distribution(\n    m_sphere=m_sphere,\n    wavelength=wavelength,\n    diameter=diameter_sizes,\n    number_per_cm3=number_per_cm3,\n    n_medium=n_medium,\n    as_dict=True,\n)\n\n# Print or process the Mie scattering results\n\nfor key, value in mie_results.items():\n    print(f\"{key:&lt;8}: {value}\")\n</pre> # Define the refractive index of the particles (complex for absorbing, # real for non-absorbing) m_sphere = 1.5 + 0.01j  # Example for slightly absorbing particles  # Define the wavelength of the incident light in nanometers wavelength = 550.0  # Create a linearly spaced array of particle diameters in nanometers # From 100 nm to 300 nm, 50 sizes diameter_sizes = np.linspace(100.0, 500.0, 50)  # Simulate a number concentration for each particle size in #/cm\u00b3 # Decreasing concentration from 1000 to 500 #/cm\u00b3 number_per_cm3 = np.linspace(1000.0, 500.0, 50)  # Refractive index of the surrounding medium (e.g., air) n_medium = 1.0   # Calculate the Mie scattering parameters for the given size distribution mie_results = mie_bulk.mie_size_distribution(     m_sphere=m_sphere,     wavelength=wavelength,     diameter=diameter_sizes,     number_per_cm3=number_per_cm3,     n_medium=n_medium,     as_dict=True, )  # Print or process the Mie scattering results  for key, value in mie_results.items():     print(f\"{key:&lt;8}: {value}\") <pre>b_ext   : 5024.182718003773\nb_sca   : 4784.810191293377\nb_abs   : 239.3725267103955\nG       : 0.6715751320242711\nb_pr    : 1810.8231820748451\nb_back  : 657.8815183489643\nb_ratio : 584.2817024545769\n</pre> In\u00a0[10]: Copied! <pre>help(mie_bulk.discretize_auto_mieq)\n</pre> help(mie_bulk.discretize_auto_mieq) <pre>Help on _lru_cache_wrapper in module particula_beta.data.process.mie_bulk:\n\ndiscretize_auto_mieq(m_sphere: Union[complex, float], wavelength: float, diameter: float, m_medium: float = 1.0) -&gt; Tuple[float, ...]\n    Computes Mie coefficients for a spherical particle based on its material\n    properties, size, and the properties of the surrounding medium.\n    \n    This function leverages the PyMieScatt library to calculate the extinction\n    (q_ext), scattering (q_sca), absorption (q_abs) efficiencies, the\n    asymmetry factor (g), radiation pressure efficiency (q_pr), backscatter\n    efficiency (q_back), and the ratio of backscatter to extinction efficiency\n    (q_ratio) for a single sphere under specified conditions.\n    \n    This function is optimized with an LRU (Least Recently Used) cache to\n    enhance performance by storing up to 100,000 recent calls. The cache\n    memorizes the results of expensive function calls and returns the cached\n    result when the same inputs occur again, reducing the need to recompute\n    these values.\n    \n    Args\n    ----------\n    m_sphere : The complex refractive index of the sphere. For non-absorbing\n        material a real number can be provided.\n    wavelength : The wavelength of the incident light in nanometers (nm).\n    diameter : The diameter of the sphere in nanometers (nm).\n    mMedium : The refractive index of the surrounding medium.\n        Defaults to 1.0, corresponding to vacuum.\n    \n    Returns\n    -------\n    Tuple[float, float, float, float, float, float, float]\n        A tuple containing the calculated Mie efficiencies and parameters:\n        q_ext (extinction efficiency), q_sca (scattering efficiency),\n        q_abs (absorption efficiency), g (asymmetry factor),\n        q_pr (radiation pressure efficiency), q_back (backscatter efficiency),\n        and q_ratio (the ratio of backscatter to extinction efficiency).\n\n</pre> In\u00a0[11]: Copied! <pre>help(mie_bulk.mie_size_distribution)\n</pre> help(mie_bulk.mie_size_distribution) <pre>Help on function mie_size_distribution in module particula_beta.data.process.mie_bulk:\n\nmie_size_distribution(m_sphere: Union[complex, float], wavelength: float, diameter: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], number_per_cm3: numpy.ndarray[typing.Any, numpy.dtype[numpy.float64]], n_medium: float = 1.0, pms: bool = True, as_dict: bool = False, extinction_only: bool = False, discretize: bool = False, truncation_calculation: bool = False, truncation_b_sca_multiple: Optional[float] = None) -&gt; Union[numpy.ndarray[Any, numpy.dtype[numpy.float64]], dict[str, numpy.ndarray[Any, numpy.dtype[numpy.float64]]], Tuple[numpy.ndarray[Any, numpy.dtype[numpy.float64]], ...]]\n    Calculates Mie scattering parameters for a size distribution of spherical\n    particles.\n    \n    This function computes optical properties such as extinction, scattering,\n    absorption coefficients, asymmetry factor, backscatter efficiency, and\n    their ratios for a given size distribution of spherical particles. It\n    supports various modes of calculation, including discretization of input\n    parameters and optional truncation of the scattering efficiency.\n    \n    Parameters\n    ----------\n    m_sphere : Union[complex, float]\n        The complex refractive index of the particles. Real values can be used\n        for non-absorbing materials.\n    wavelength : float\n        The wavelength of the incident light in nanometers (nm).\n    diameter : NDArray[np.float64]\n        An array of particle diameters in nanometers (nm).\n    number_per_cm3 : NDArray[np.float64]\n        The number distribution of particles per cubic centimeter (#/cm^3).\n    n_medium : float, optional\n        The refractive index of the medium. Defaults to 1.0 (air or vacuum).\n    pms : bool, optional\n        Specifies if the size distribution is in probability mass form.\n    as_dict : bool, optional\n        If True, results are returned as a dictionary. Otherwise, as a tuple.\n    extinction_only : bool, optional\n        If True, only the extinction coefficient is calculated and returned.\n    discretize : bool, optional\n        If True, input parameters (m, wavelength, dp) are discretized for\n        computation. Defaults to False.\n    truncation_calculation : bool, optional\n        Enables truncation of the scattering efficiency based on a multiple\n        of the backscattering coefficient. Defaults to False.\n    truncation_b_sca_multiple : Optional[float], optional\n        The multiple of the backscattering coefficient used for truncating the\n        scattering efficiency. Required if `truncation_calculation` is True.\n    \n    Returns\n    -------\n    Union[float, Dict[str, float], Tuple[float, ...]]\n        Depending on the parameters `asDict` and `extinction_only`, the\n        function can return:\n        - A single float (extinction coefficient) if `extinction_only` is True.\n        - A dictionary of computed optical properties if `asDict` is True.\n        - A tuple of computed optical properties.\n    \n    Raises\n    ------\n    ValueError\n        If `truncation_calculation` is True but `truncation_b_sca_multiple`\n        is not specified.\n\n</pre>"},{"location":"How-To-Guides/Light_Scattering/notebooks/mie_basics/#mie-scattering","title":"Mie Scattering\u00b6","text":"<p>Mie scattering is the scattering of light by spherical particles. It is named after the German physicist Gustav Mie. The Mie solution to Maxwell's equations is an exact solution that describes the scattering of electromagnetic radiation by a sphere. Here we will discuss our integration (wrapper) of the pyMieScatt package, which is a Python implementation of the Mie solution.</p> <p>We have built this wrapper for two main reasons:</p> <ol> <li>Reduce the complexity of calling the original package so integrations with data analysis is easier.</li> <li>The codification of data types in the interface. This also allows for Least Recently Used cache (<code>lru_cache</code>) to be used, which is a Python decorator that caches the results of a function, so if the function is called with the same arguments, the result is returned from the cache instead of calling the function again. This only works if the arguments types are hashable, so immutable.</li> </ol> <p>The following imports are needed</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/mie_basics/#mie-coefficients-discretize_auto_mieq","title":"Mie Coefficients: <code>discretize_auto_mieq</code>\u00b6","text":"<p>The <code>discretize_auto_mieq</code> function plays a critical role our interface with pyMieScatt, by computing Mie coefficients for a spherical particle. These coefficients are pivotal in understanding how light interacts with particles in the atmosphere, influencing phenomena such as visibility, climate change, and radiative forcing.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/mie_basics/#function-overview","title":"Function Overview\u00b6","text":"<p>This function utilizes the PyMieScatt library to derive several key efficiencies and factors for a single sphere, given its material properties, size, and the surrounding medium's properties. By leveraging an LRU (Least Recently Used) cache, <code>discretize_auto_mieq</code> significantly enhances performance for repetitive calculations, storing up to 100,000 recent calls to avoid recalculating identical inputs.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/mie_basics/#parameters","title":"Parameters\u00b6","text":"<ul> <li><p><code>m_sphere</code>: Complex or real refractive index of the sphere. Real numbers can be used for non-absorbing materials, while complex numbers account for both the real and imaginary parts of the refractive index, representing absorption characteristics.</p> </li> <li><p><code>wavelength</code>: Wavelength of the incident light, specified in nanometers (nm). This parameter is crucial as Mie scattering varies with wavelength, affecting how light is scattered or absorbed by particles.</p> </li> <li><p><code>diameter</code>: Diameter of the spherical particle in nanometers (nm). Particle size relative to the wavelength influences scattering behavior, making this a key parameter in Mie theory.</p> </li> <li><p><code>m_medium</code> (optional): Refractive index of the medium surrounding the particle, defaulting to 1.0 to represent a vacuum. This context is important for accurately modeling light-particle interactions in various environments.</p> </li> </ul>"},{"location":"How-To-Guides/Light_Scattering/notebooks/mie_basics/#returns","title":"Returns\u00b6","text":"<p>A tuple containing the calculated Mie efficiencies and parameters:</p> <ul> <li><p><code>q_ext</code>: Extinction efficiency, representing the fraction of light extinguished (either absorbed or scattered) by the particle.</p> </li> <li><p><code>q_sca</code>: Scattering efficiency, indicating the fraction of light scattered by the particle.</p> </li> <li><p><code>q_abs</code>: Absorption efficiency, showing the fraction of light absorbed by the particle.</p> </li> <li><p><code>g</code>: Asymmetry factor, describing the average cosine of the scattering angle, which influences the directionality of scattering.</p> </li> <li><p><code>q_pr</code>: Radiation pressure efficiency, quantifying the momentum transfer from the light to the particle, affecting particle movement.</p> </li> <li><p><code>q_back</code>: Backscatter efficiency, indicating the fraction of light scattered in directions reverse to the incident light.</p> </li> <li><p><code>q_ratio</code>: The ratio of backscatter to extinction efficiency, useful for understanding reflective properties.</p> </li> </ul>"},{"location":"How-To-Guides/Light_Scattering/notebooks/mie_basics/#the-mie_size_distribution-function","title":"The <code>mie_size_distribution</code> Function\u00b6","text":"<p>The <code>mie_size_distribution</code> function extends the capabilities of single-particle Mie scattering calculations to entire particle size distributions, providing a comprehensive view of aerosol optical properties. This section focuses on the new variables introduced in this function, which enable it to handle distributions and offer various modes of computation.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/mie_basics/#new-variables-explained","title":"New Variables Explained\u00b6","text":"<ul> <li><p><code>number_per_cm3</code>: This array represents the number distribution of particles per cubic centimeter for each diameter in the size distribution. It's crucial for calculating the collective optical properties of the particle ensemble, allowing for a more accurate representation of aerosol behavior in the atmosphere.</p> </li> <li><p><code>n_medium</code>: The refractive index of the medium surrounding the particles, with a default value of 1.0, representing air or vacuum. This parameter is essential for adjusting the Mie scattering calculations based on the medium's optical properties.</p> </li> <li><p><code>pms</code> (Probability Mass Function): A boolean flag indicating whether the size distribution is provided in the form of a probability mass function. When <code>True</code>, it signifies that the <code>number_per_cm3</code> array represents a binned distribution (sum of bins is total number), affecting how the bulk optical properties are computed. When <code>False</code>, the function assumes the array represents a continuous probability density function, where the area under the curve is equal to total particle number.</p> </li> <li><p><code>as_dict</code>: This boolean flag determines the format of the function's output. If set to <code>True</code>, the function returns a dictionary containing the calculated optical properties, providing a convenient structure for accessing specific values.</p> </li> <li><p><code>extinction_only</code>: When set to <code>True</code>, this flag limits the calculations to only the extinction coefficient, simplifying the output for applications focused solely on particle extinction properties.</p> </li> <li><p><code>discretize</code>: This flag enables the discretization of input parameters (<code>m_sphere</code>, <code>wavelength</code>, <code>diameter</code>) for potentially improved calculation stability and performance. Discretization can introduce computational errors by truncating the significant digits.</p> </li> <li><p><code>truncation_calculation</code>: A boolean flag that, when <code>True</code>, activates the truncation of the scattering efficiency calculation. This adjustment is based on a multiple of the backscattering coefficient, addressing truncation errors inherent in certain measurement instruments.</p> </li> <li><p><code>truncation_b_sca_multiple</code>: An optional float specifying the multiple of the scattering coefficient used for truncating the scattering efficiency. This parameter is required if <code>truncation_calculation</code> is <code>True</code> and is key to accurately correcting for instrument truncation effects.</p> </li> </ul>"},{"location":"How-To-Guides/Light_Scattering/notebooks/mie_basics/#practical-implications","title":"Practical Implications\u00b6","text":"<p>These variables introduce flexibility and precision into aerosol optics modeling, allowing for detailed analysis of particle distributions under varying environmental conditions and measurement setups. By adjusting these parameters, we can simulate a wide range of atmospheric particles, enhancing our understanding of aerosol impacts on climate and air quality.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/mie_basics/#note","title":"Note\u00b6","text":"<p>This is also where we deviate from <code>pyMieScatt</code>, as this is now not a wrapper of the same functions in the original package, but a new function that calls the underlining <code>discretize_auto_mieq</code>. We then calculate the results with more flexibility.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/mie_basics/#summary","title":"Summary\u00b6","text":"<p>The provided code segments form a foundational framework for analyzing aerosol optics, specifically through the calculation of Mie scattering parameters. This framework includes:</p> <ol> <li><p>Discretization of Mie Coefficients (<code>discretize_auto_mieq</code>): This function computes essential Mie scattering parameters such as extinction, scattering, absorption efficiencies, the asymmetry factor, radiation pressure efficiency, backscatter efficiency, and the ratio of backscatter to extinction efficiency for spherical particles. It leverages the PyMieScatt library for calculations and optimizes performance with an LRU cache, effectively reducing computational overhead for repeated calculations.</p> </li> <li><p>Mie Scattering for Size Distributions (<code>mie_size_distribution</code>): Extending the application to aerosol size distributions, this function calculates optical properties across a range of particle sizes. It supports various modes of operation, including discretization of input parameters for improved computational performance and optional truncation of scattering efficiencies. The function can output results as either a dictionary or a tuple, accommodating different analytical needs.</p> </li> <li><p>Example Usage with Linearly Spaced Size Distribution: Demonstrated how to apply the <code>mie_size_distribution</code> function to a linearly spaced array of particle diameters, simulating a realistic aerosol size distribution. This example showcases how to generate a size distribution, calculate Mie scattering parameters for it, and then access these parameters for further analysis.</p> </li> </ol>"},{"location":"How-To-Guides/Light_Scattering/notebooks/mie_basics/#building-blocks-for-advanced-analysis","title":"Building Blocks for Advanced Analysis\u00b6","text":"<p>These components serve as critical building blocks for more advanced analyses, particularly in studying the next section: Humidified Particle Scattering. In real atmospheric conditions, aerosol particles often undergo hygroscopic growth, absorbing water from the air and increasing in size. This water uptake significantly affects the optical properties of aerosols, influencing their scattering and absorption behaviors and, consequently, their impact on climate and visibility.</p> <p>The code and methodologies discussed provide a starting point for such advanced analyses, enabling the exploration of how aerosol optical properties change with humidity. This understanding is crucial for accurately assessing aerosols' environmental and climatic impacts, highlighting the importance of these computational tools in atmospheric sciences.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/scattering_truncation/","title":"Scattering Truncation Corrections","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# particula imports\nfrom particula.particles.properties.lognormal_size_distribution import (\n    lognormal_pmf_distribution,\n)\nfrom particula_beta.data.process import scattering_truncation\n</pre> import numpy as np import matplotlib.pyplot as plt  # particula imports from particula.particles.properties.lognormal_size_distribution import (     lognormal_pmf_distribution, ) from particula_beta.data.process import scattering_truncation In\u00a0[2]: Copied! <pre># Define the refractive index of the aerosol particle and the light wavelength\nm_sphere = 1.5  # Refractive index of the particle\nwavelength = 450.0  # Wavelength of incident light in nanometers (nm)\n\n# Perform the truncation correction for a single particle of a given diameter\ntrunc_corr, z_axis, qsca_trunc, qsca_ideal, theta1, theta2 = (\n    scattering_truncation.trunc_mono(\n        m_sphere=m_sphere,  # Refractive index of the particle\n        wavelength=wavelength,  # Wavelength of incident light\n        diameter=100,  # Diameter of the particle in nanometers\n        full_output=True,  # Request full output for detailed analysis\n        calibrated_trunc=True,\n    )\n)\n\n# Output the calculated scattering efficiencies and truncation correction\n# factor\nprint(f\"Ideal Q_sca: {qsca_ideal} for a perfect instrument\")\nprint(f\"Truncated Q_sca: {qsca_trunc} for a truncated CAPS instrument\")\nprint(f\"Truncation correction factor: {trunc_corr}\")\n</pre> # Define the refractive index of the aerosol particle and the light wavelength m_sphere = 1.5  # Refractive index of the particle wavelength = 450.0  # Wavelength of incident light in nanometers (nm)  # Perform the truncation correction for a single particle of a given diameter trunc_corr, z_axis, qsca_trunc, qsca_ideal, theta1, theta2 = (     scattering_truncation.trunc_mono(         m_sphere=m_sphere,  # Refractive index of the particle         wavelength=wavelength,  # Wavelength of incident light         diameter=100,  # Diameter of the particle in nanometers         full_output=True,  # Request full output for detailed analysis         calibrated_trunc=True,     ) )  # Output the calculated scattering efficiencies and truncation correction # factor print(f\"Ideal Q_sca: {qsca_ideal} for a perfect instrument\") print(f\"Truncated Q_sca: {qsca_trunc} for a truncated CAPS instrument\") print(f\"Truncation correction factor: {trunc_corr}\") <pre>Ideal Q_sca: 0.5478174528225519 for a perfect instrument\nTruncated Q_sca: 0.5363833503036404 for a truncated CAPS instrument\nTruncation correction factor: 0.99888593093735\n</pre> In\u00a0[3]: Copied! <pre># Generating diameters in lin space from 50 nm to 1000 nm with 200 points\ndiameters = np.linspace(50, 2000, 100)\n\ntruncation_array = scattering_truncation.truncation_for_diameters(\n    m_sphere=m_sphere,\n    wavelength=wavelength,\n    diameter_sizes=diameters,\n    discretize=False,\n    calibrated_trunc=True,\n)\n\n# Plot the truncation correction factor as a function of particle diameter\nplt.figure()\nplt.plot(diameters, truncation_array, \".\")\nplt.xlabel(\"Particle diameter (nm)\")\nplt.ylabel(\"Truncation correction factor\")\nplt.title(\"Truncation correction factor as a function of particle diameter\")\nplt.show()\n</pre> # Generating diameters in lin space from 50 nm to 1000 nm with 200 points diameters = np.linspace(50, 2000, 100)  truncation_array = scattering_truncation.truncation_for_diameters(     m_sphere=m_sphere,     wavelength=wavelength,     diameter_sizes=diameters,     discretize=False,     calibrated_trunc=True, )  # Plot the truncation correction factor as a function of particle diameter plt.figure() plt.plot(diameters, truncation_array, \".\") plt.xlabel(\"Particle diameter (nm)\") plt.ylabel(\"Truncation correction factor\") plt.title(\"Truncation correction factor as a function of particle diameter\") plt.show() In\u00a0[5]: Copied! <pre># Define the refractive index and wavelength for the aerosols\nm_sphere = 1.5\nwavelength = 450.0  # in nanometers\n\n# Generate diameters using a log-spaced array for better representation\ndiameters = np.logspace(\n    np.log10(50), np.log10(800), 100\n)  # From 50 nm to 800 nm\n\n# Parameters for the log-normal size distribution\nsigma = 1.4  # Geometric standard deviation\nmodes = 250  # Peak diameter for monomodal distribution\nnumber_total = 1e3  # Total number of particles\n\nnumber_conc = lognormal_pmf_distribution(\n    x_values=diameters,\n    mode=np.array([modes]),\n    geometric_standard_deviation=np.array([sigma]),\n    number_of_particles=np.array([number_total]),\n)\n\n# Calculate the truncation correction for the entire size distribution\ntrunc_correction = scattering_truncation.correction_for_distribution(\n    m_sphere=m_sphere,\n    wavelength=wavelength,\n    diameter_sizes=diameters,\n    number_per_cm3=number_conc,\n    discretize=True,\n)\n\n# Visualize the size distribution and truncation correction\nplt.figure()\nplt.plot(diameters, number_conc, label=\"Size Distribution\")\nplt.xlabel(\"Particle Diameter (nm)\")\nplt.ylabel(\"Number Concentration (#/cm\u00b3)\")\nplt.title(\n    f\"Truncation Correction: {trunc_correction:.5f} for the entire distribution\"\n)\nplt.legend()\nplt.show()\n\nprint(\n    f\"Overall truncation correction for the distribution: {trunc_correction}\"\n)\n</pre> # Define the refractive index and wavelength for the aerosols m_sphere = 1.5 wavelength = 450.0  # in nanometers  # Generate diameters using a log-spaced array for better representation diameters = np.logspace(     np.log10(50), np.log10(800), 100 )  # From 50 nm to 800 nm  # Parameters for the log-normal size distribution sigma = 1.4  # Geometric standard deviation modes = 250  # Peak diameter for monomodal distribution number_total = 1e3  # Total number of particles  number_conc = lognormal_pmf_distribution(     x_values=diameters,     mode=np.array([modes]),     geometric_standard_deviation=np.array([sigma]),     number_of_particles=np.array([number_total]), )  # Calculate the truncation correction for the entire size distribution trunc_correction = scattering_truncation.correction_for_distribution(     m_sphere=m_sphere,     wavelength=wavelength,     diameter_sizes=diameters,     number_per_cm3=number_conc,     discretize=True, )  # Visualize the size distribution and truncation correction plt.figure() plt.plot(diameters, number_conc, label=\"Size Distribution\") plt.xlabel(\"Particle Diameter (nm)\") plt.ylabel(\"Number Concentration (#/cm\u00b3)\") plt.title(     f\"Truncation Correction: {trunc_correction:.5f} for the entire distribution\" ) plt.legend() plt.show()  print(     f\"Overall truncation correction for the distribution: {trunc_correction}\" ) <pre>Overall truncation correction for the distribution: 1.0416062342287757\n</pre> In\u00a0[6]: Copied! <pre># Define the refractive index and wavelength for the aerosols\nm_sphere = 1.5\nwavelength = 450.0  # in nanometers\n\n# Generate diameters using a log-spaced array for better representation\ndiameters = np.logspace(\n    np.log10(200), np.log10(2500), 100\n)  # From 50 nm to 800 nm\n\n# Parameters for the log-normal size distribution\nsigma = 1.4  # Geometric standard deviation\nmodes = 1000  # Peak diameter for monomodal distribution\nnumber_total = 1e3  # Total number of particles\n\n# Create a log-normal size distribution\nnumber_conc_large = lognormal_pmf_distribution(\n    x_values=diameters,\n    mode=np.array([modes]),\n    geometric_standard_deviation=np.array([sigma]),\n    number_of_particles=np.array([number_total]),\n)\n\n# Calculate the truncation correction for the entire size distribution\ntrunc_correction = scattering_truncation.correction_for_distribution(\n    m_sphere=m_sphere,\n    wavelength=wavelength,\n    diameter_sizes=diameters,\n    number_per_cm3=number_conc_large,\n    discretize=True,\n)\n\n# Visualize the size distribution and truncation correction\nplt.figure()\nplt.plot(diameters, number_conc_large, label=\"Size Distribution\")\nplt.xlabel(\"Particle Diameter (nm)\")\nplt.ylabel(\"Number Concentration (#/cm\u00b3)\")\nplt.title(\n    f\"Truncation Correction: {trunc_correction:.5f} for the entire distribution\"\n)\nplt.legend()\nplt.show()\n\nprint(\n    f\"Overall truncation correction for the distribution: {trunc_correction}\"\n)\n</pre> # Define the refractive index and wavelength for the aerosols m_sphere = 1.5 wavelength = 450.0  # in nanometers  # Generate diameters using a log-spaced array for better representation diameters = np.logspace(     np.log10(200), np.log10(2500), 100 )  # From 50 nm to 800 nm  # Parameters for the log-normal size distribution sigma = 1.4  # Geometric standard deviation modes = 1000  # Peak diameter for monomodal distribution number_total = 1e3  # Total number of particles  # Create a log-normal size distribution number_conc_large = lognormal_pmf_distribution(     x_values=diameters,     mode=np.array([modes]),     geometric_standard_deviation=np.array([sigma]),     number_of_particles=np.array([number_total]), )  # Calculate the truncation correction for the entire size distribution trunc_correction = scattering_truncation.correction_for_distribution(     m_sphere=m_sphere,     wavelength=wavelength,     diameter_sizes=diameters,     number_per_cm3=number_conc_large,     discretize=True, )  # Visualize the size distribution and truncation correction plt.figure() plt.plot(diameters, number_conc_large, label=\"Size Distribution\") plt.xlabel(\"Particle Diameter (nm)\") plt.ylabel(\"Number Concentration (#/cm\u00b3)\") plt.title(     f\"Truncation Correction: {trunc_correction:.5f} for the entire distribution\" ) plt.legend() plt.show()  print(     f\"Overall truncation correction for the distribution: {trunc_correction}\" ) <pre>Overall truncation correction for the distribution: 1.1821845913839402\n</pre>"},{"location":"How-To-Guides/Light_Scattering/notebooks/scattering_truncation/#scattering-truncation-corrections","title":"Scattering Truncation Corrections\u00b6","text":"<p>The study of aerosol optical properties, such as Single Scattering Albedo (SSA), plays a crucial role in understanding the effects of aerosols on climate and air quality. SSA, a measure of the fraction of light scattered by particles relative to the total light extinction (scattering plus absorption), is essential for assessing aerosol radiative forcing. Instruments like the Cavity Attenuated Phase Shift (CAPS) SSA instrument are pivotal in making these measurements with high accuracy and reliability. However, one of the critical challenges in accurately determining SSA, especially with the CAPS instrument, is the phenomenon of scattering truncation.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/scattering_truncation/#the-importance-of-correcting-for-scattering-truncation","title":"The Importance of Correcting for Scattering Truncation\u00b6","text":"<p>Scattering truncation occurs due to the limited angular range over which scattering measurements can be made, leading to an underestimation of the total scattering by aerosol particles. This limitation is particularly pronounced in instruments like the CAPS, where the design inherently restricts the detection of scattered light to a finite angular range (missing some back scatter and forward scattering light). The consequence of this truncation is a potential bias (low) in the measured SSA values, which can significantly affect the interpretation of aerosol optical properties and, by extension, their climatic and environmental impacts.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/scattering_truncation/#size-dependent-nature-of-scattering-truncation","title":"Size-Dependent Nature of Scattering Truncation\u00b6","text":"<p>The extent of scattering truncation is not uniform across all particle sizes; rather, it exhibits a pronounced size dependency. This variation arises because the scattering efficiency and the angular distribution of scattered light are functions of particle size relative to the wavelength of the incident light. Smaller particles tend to scatter light more isotropically, while larger particles preferentially scatter light in the forward direction. As a result, instruments with limited angular detection ranges may miss a substantial portion of the forward-scattered light from larger particles, leading to more significant truncation effects for these particle sizes.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/scattering_truncation/#addressing-the-challenge","title":"Addressing the Challenge\u00b6","text":"<p>Correcting for scattering truncation is thus vital for ensuring the accuracy of SSA measurements and, by extension, our understanding of aerosol optical properties. This notebook focuses on methodologies for correcting scattering truncation effects in SSA measurements made using the CAPS instrument. By implementing these corrections, we aim to achieve more accurate and representative SSA values, enhancing our ability to model and predict aerosol impacts on atmospheric processes.</p> <p>The following sections will delve into scattering truncation, explore its size-dependent characteristics, and introduce correction techniques tailored to the CAPS SSA instrument.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/scattering_truncation/#addressing-scattering-truncation-for-single-particle-measurements","title":"Addressing Scattering Truncation for Single Particle Measurements\u00b6","text":"<p>In the quest to accurately assess aerosol optical properties using the Cavity Attenuated Phase Shift (CAPS) SSA instrument, accounting for scattering truncation is paramount. Scattering truncation arises due to the instrument's limited angular range in capturing scattered light, necessitating corrections to obtain true scattering coefficients. This section introduces the methodology to correct for scattering truncation for a single aerosol particle using the <code>scattering_truncation.trunc_mono</code> function.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/scattering_truncation/#implementing-truncation-correction-with-python","title":"Implementing Truncation Correction with Python\u00b6","text":"<p>The process begins by defining the optical properties of the particle (refractive index and wavelength) and calculating both the ideal (untruncated) and the actual (truncated) scattering efficiencies. We then determine the correction factor needed to adjust for the truncation effect observed with the CAPS instrument.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/scattering_truncation/#insights-from-truncation-correction","title":"Insights from Truncation Correction\u00b6","text":"<ul> <li><p>Ideal vs. Truncated Scattering Efficiency: The <code>qsca_ideal</code> output represents the scattering efficiency as if measured by a perfect instrument with no angular limitations. In contrast, <code>qsca_trunc</code> reflects the efficiency as captured by the CAPS instrument, which is inherently limited by scattering truncation.</p> </li> <li><p>Correction Factor: The <code>trunc_corr</code> value indicates the factor by which the measured (truncated) scattering efficiency needs to be adjusted to align with the ideal, untruncated scenario. This correction is crucial for accurately interpreting SSA measurements and understanding aerosol scattering behaviors.</p> </li> </ul> <p>This correction approach for a single particle lays the groundwork for more comprehensive analyses.</p> <p>Note: The <code>calibrated_trunc</code> boolean applies an additional correction factor of 1.0224, so at a 150 nm diameter the truncation correction is 1. This factor is numerically determined and it is not clear where this error is introduced in the angular truncation calculation.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/scattering_truncation/#full-size-dependence-of-truncation","title":"Full Size Dependence of Truncation\u00b6","text":"<p>The size-dependent nature of scattering truncation necessitates a thorough understanding of how the phenomenon varies across different particle sizes. This section explores the full size dependence of scattering truncation and introduces the <code>scattering_truncation.truncation_for_diameters</code> function to correct for truncation effects across a range of particle sizes.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/scattering_truncation/#correcting-scattering-truncation-250-nm-distribution","title":"Correcting Scattering Truncation 250 nm Distribution\u00b6","text":"<p>Aerosol populations in the atmosphere exhibit a broad range of particle sizes, each contributing distinctively to the overall optical scattering properties. This section demonstrates the application of the <code>scattering_truncation.correction_for_distribution</code> function, which facilitates the adjustment for scattering truncation across an aerosol size distributions measurement.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/scattering_truncation/#generating-a-representative-aerosol-distribution","title":"Generating a Representative Aerosol Distribution\u00b6","text":"<p>For this analysis, we focus on a distribution centered around 250 nm, a size range significant for many atmospheric aerosol studies. We simulate this aerosol population using a log-normal distribution, a common representation for atmospheric aerosol size distributions.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/scattering_truncation/#correcting-scattering-truncation-1000-nm-distribution","title":"Correcting Scattering Truncation 1000 nm Distribution\u00b6","text":"<p>In addition to the 250 nm distribution, we also explore the correction of scattering truncation for a larger aerosol population centered around 1000 nm. This size range is particularly relevant for understanding the scattering properties of coarse-mode aerosols, which play a crucial role in atmospheric processes.</p> <p>The truncation factor is much larger for the 1000 nm distribution, indicating the more significant impact of truncation for larger particles. This underscores the importance of accounting for scattering truncation across the full range of aerosol sizes to obtain accurate SSA measurements.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/scattering_truncation/#summary","title":"Summary\u00b6","text":"<p>Understanding the climatic and environmental impacts of aerosols necessitates accurate determinations of their optical properties, especially Single Scattering Albedo (SSA). The challenge of scattering truncation, however, complicates the precise measurement of SSA, a dilemma particularly pronounced with the use of the Cavity Attenuated Phase Shift (CAPS) SSA instrument. Throughout this notebook, we have explored and applied methodologies to correct for the effects of scattering truncation, delving into its size-dependent characteristics and the broader implications for accurately assessing SSA.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/scattering_truncation/#addressing-humidified-aerosol-measurements","title":"Addressing Humidified Aerosol Measurements\u00b6","text":"<p>Our exploration does extend to humidified aerosol measurements, where changes in particle refractive index and diameter under varying humidity levels introduce additional complexity. The <code>scattering_truncation</code> module, through its <code>correction_for_humidified</code> and <code>correction_for_humidified_looped</code> functions, offers robust solutions for incorporating these dynamic factors into SSA calculations, ensuring that measurements remain reflective of actual atmospheric conditions.</p>"},{"location":"How-To-Guides/Light_Scattering/notebooks/scattering_truncation/#leveraging-truncation-corrections-for-broader-applications","title":"Leveraging Truncation Corrections for Broader Applications\u00b6","text":"<p>The adaptability of these correction techniques to a wide range of real-world aerosol distributions highlights their significance. Aerosols in the atmosphere exhibit a vast diversity in size, composition, and behavior, necessitating flexible and accurate measurement and analysis methods. By enhancing the precision of SSA and other optical property measurements, we lay a stronger foundation for atmospheric modeling, contributing to more reliable predictions of aerosol impacts on climate and environmental health.</p>"},{"location":"How-To-Guides/Setup_Particula/","title":"Index: Setup Particula","text":""},{"location":"How-To-Guides/Setup_Particula/#getting-started-with-python","title":"Getting Started with Python","text":"<p>If you are new to Python, it's highly recommended to go through an introductory course to build a solid foundation. \"Python for Everybody\" is an excellent free resource that covers the basics and beyond:</p> <ul> <li>Access the course and materials at Python for Everybody.</li> </ul>"},{"location":"How-To-Guides/Setup_Particula/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<p>Step 1: Install Visual Studio Code (VSCode)</p> <p>To edit and manage your code efficiently, download and install Visual Studio Code, a popular code editor that supports Python and many other languages.</p> <ul> <li>Visit the Visual Studio Code website to download the installer for Windows.</li> <li>Follow the installation prompts to install VSCode on your machine.</li> <li>Once installed, launch VSCode to configure it for Python development.</li> </ul> <p>Step 2: Install Miniconda</p> <p>Install Miniconda, which includes Conda, a powerful package and environment manager. This tool will help you manage different project dependencies separately and efficiently.</p> <ul> <li>Download Miniconda for Windows from Miniconda's website.</li> <li>Follow the installation instructions to install Miniconda on your system.</li> </ul> <p>Step 3: Install Git</p> <p>Install Git to manage your code repositories effectively. Git is a version control system that lets you track changes, revert to previous stages, and collaborate on projects with others.</p> <ul> <li>Download Git from Git's official website.</li> <li>Run the downloaded file to start the installation.</li> <li>During the installation, choose your preferred editor for Git, and make sure to select \"Git from the command line and also from 3<sup>rd</sup>-party software\" to ensure it integrates well with your system's command prompt.</li> <li>Complete the installation by following the on-screen instructions.</li> </ul> <p>Once installed, you can verify the installation by opening a command prompt or terminal and typing <code>git --version</code>, which should display the installed version of Git.</p> <p>Step 4: Setup Proxy</p> <p>If you are behind a proxy, you may need to configure your proxy settings to allow Conda, Pip, git, and VScode to access the internet.</p> <p>Step 5: Create a New Python Environment </p> <p>Avoid conflicts with other development projects by creating an isolated Python environment. Here\u2019s how:</p> <ul> <li>Open VSCode, then open the integrated terminal (<code>Terminal &gt; New Terminal</code>).</li> <li>Be sure to select <code>cmd</code> for command prompt.</li> <li>Use the following Conda command to create an environment named <code>analysisV1</code> with Python 3.11:</li> </ul> <pre><code>conda create --name analysisV1 python=3.11\n</code></pre> <p>Step 6: Activate the Environment </p> <p>Ensure you\u2019re working within the context of your new environment: - In the VSCode terminal, activate your environment by running:</p> <pre><code>conda activate analysisV1\n</code></pre>"},{"location":"How-To-Guides/Setup_Particula/#installing-the-project","title":"Installing the Project","text":"<p>Step 7: Install the Project</p> <p>Now, install the <code>Particula</code> using pip in your activated environment, use one of the following methods:</p>"},{"location":"How-To-Guides/Setup_Particula/#install-the-pip-package","title":"Install the pip package","text":"<pre><code>pip install particula\n</code></pre>"},{"location":"How-To-Guides/Setup_Particula/#or-install-the-main-repository","title":"or Install the main repository","text":"<pre><code>pip install git+https://github.com/uncscode/particula.git\n</code></pre>"},{"location":"How-To-Guides/Setup_Particula/#or-install-the-gorkowski-fork","title":"or Install the Gorkowski fork","text":"<pre><code>pip install git+https://github.com/Gorkowski/particula.git\n</code></pre>"},{"location":"Tutorials/","title":"Tutorials","text":"<ul> <li> <p>Aerosol</p> <p>Learn what goes into the Aerosol object and why it is used.</p> <p> Tutorial</p> </li> <li> <p>Dynamics</p> <p>Dynamics is a collection of classes that processes for the <code>Aerosol</code> objects, to perform coagulation, condensation, and other processes.</p> <p> Tutorial</p> </li> <li> <p>Gas Phase</p> <p>Learn how to represent the Gas Phase, including vapor pressures and atmospheric properties.</p> <p> Tutorial</p> </li> <li> <p>Particle Phase</p> <p>Learn about how to represent the Particle Phase, including different particle representations; radius bins, speciated mass bins, and particle resolved.</p> <p> Tutorial</p> </li> <li> <p>Data Analysis </p> <p>in BETA  Some preliminary tools available for Data Analysis.</p> <p> Tutorial</p> </li> </ul>"},{"location":"Tutorials/Data_Analysis/","title":"Index: Data Analysis","text":"<ul> <li>Fitting Lognormal PDF: 2 Modes</li> <li>Converting Size Distributions</li> </ul>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/","title":"Converting Size Distributions","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# particula imports\nfrom particula.util import convert, distribution_discretization\nfrom particula.util.size_distribution_convert import (\n    SizerConverter,\n    get_conversion_strategy,\n)\n</pre> import numpy as np import matplotlib.pyplot as plt  # particula imports from particula.util import convert, distribution_discretization from particula.util.size_distribution_convert import (     SizerConverter,     get_conversion_strategy, ) <pre>\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[1], line 5\n      2 import matplotlib.pyplot as plt\n      4 # particula imports\n----&gt; 5 from particula.util import convert, distribution_discretization\n      6 from particula.util.size_distribution_convert import SizerConverter, get_conversion_strategy\n\nImportError: cannot import name 'distribution_discretization' from 'particula.util' (c:\\Users\\253393\\AppData\\Local\\miniconda3\\envs\\ParticulaBeta_p311\\Lib\\site-packages\\particula\\util\\__init__.py)</pre> In\u00a0[\u00a0]: Copied! <pre># Define the range and resolution of particle diameters (in nanometers)\ndiameters = np.linspace(20, 1000, 500)  # From 20 nm to 1000 nm with 500 points\n\n# Standard deviation of the log-normal distribution (dimensionless)\nsigma = 1.25\n\n# Define the modes (peak diameters) for a bimodal distribution\nmodes = [100, 500]\n\n# Total number of particles in the distribution\nnumber_total = 1e3\n\n# Generate a log-normal probability density function (pdf) with an area\n# equal to 1, then scale by total number of particles\npdf_dist = (\n    distribution_discretization.discretize(\n        interval=diameters,  # Array of diameters over which to compute the distribution\n        disttype=\"lognormal\",  # Type of distribution\n        gsigma=sigma,  # Geometric standard deviation\n        mode=modes,  # Modes of the distribution\n        nparticles=number_total,  # Total number of particles\n    ).m\n    * number_total\n)\n\n# Visualization\nfig, ax = plt.subplots()\nax.plot(diameters, pdf_dist, label=\"PDF\")\nax.set_xscale(\"log\")  # Logarithmic scale for diameters\nax.set_xlabel(\"Diameter (nm)\")  # X-axis label\nax.set_ylabel(\"Number concentration\")  # Y-axis label\nax.legend()  # Show legend\nplt.show()  # Display the plot\n</pre> # Define the range and resolution of particle diameters (in nanometers) diameters = np.linspace(20, 1000, 500)  # From 20 nm to 1000 nm with 500 points  # Standard deviation of the log-normal distribution (dimensionless) sigma = 1.25  # Define the modes (peak diameters) for a bimodal distribution modes = [100, 500]  # Total number of particles in the distribution number_total = 1e3  # Generate a log-normal probability density function (pdf) with an area # equal to 1, then scale by total number of particles pdf_dist = (     distribution_discretization.discretize(         interval=diameters,  # Array of diameters over which to compute the distribution         disttype=\"lognormal\",  # Type of distribution         gsigma=sigma,  # Geometric standard deviation         mode=modes,  # Modes of the distribution         nparticles=number_total,  # Total number of particles     ).m     * number_total )  # Visualization fig, ax = plt.subplots() ax.plot(diameters, pdf_dist, label=\"PDF\") ax.set_xscale(\"log\")  # Logarithmic scale for diameters ax.set_xlabel(\"Diameter (nm)\")  # X-axis label ax.set_ylabel(\"Number concentration\")  # Y-axis label ax.legend()  # Show legend plt.show()  # Display the plot In\u00a0[\u00a0]: Copied! <pre># Get the conversion strategy\npms_to_pdf = get_conversion_strategy(input_scale=\"pms\", output_scale=\"pdf\")\n\n# Create the converter\npms_to_pdf_converter = SizerConverter(pms_to_pdf)\n\n# Convert distribution\npms_dist = pms_to_pdf_converter.convert(\n    diameters=diameters,\n    concentration=pdf_dist,\n    inverse=True,\n)\n\n# Visualization\nfig, ax = plt.subplots()\nax.plot(diameters, pdf_dist, label=\"PDF\")\nax.plot(diameters, pms_dist, label=\"PMS\")\nax.set_xscale(\"log\")  # Logarithmic scale for diameters\nax.set_xlabel(\"Diameter (nm)\")  # X-axis label\nax.set_ylabel(\"Number concentration\")  # Y-axis label\nax.legend()  # Show legend\nplt.show()  # Display the plot\n</pre> # Get the conversion strategy pms_to_pdf = get_conversion_strategy(input_scale=\"pms\", output_scale=\"pdf\")  # Create the converter pms_to_pdf_converter = SizerConverter(pms_to_pdf)  # Convert distribution pms_dist = pms_to_pdf_converter.convert(     diameters=diameters,     concentration=pdf_dist,     inverse=True, )  # Visualization fig, ax = plt.subplots() ax.plot(diameters, pdf_dist, label=\"PDF\") ax.plot(diameters, pms_dist, label=\"PMS\") ax.set_xscale(\"log\")  # Logarithmic scale for diameters ax.set_xlabel(\"Diameter (nm)\")  # X-axis label ax.set_ylabel(\"Number concentration\")  # Y-axis label ax.legend()  # Show legend plt.show()  # Display the plot In\u00a0[\u00a0]: Copied! <pre># Get the conversion strategy\ndNdlogDp_to_pdf = get_conversion_strategy(\n    input_scale=\"dn/dlogdp\", output_scale=\"pdf\"\n)\n\n# Create the converter\ndNdlogDp_to_pdf_converter = SizerConverter(dNdlogDp_to_pdf)\n\n# Convert distribution\ndNdlogDp_dist = dNdlogDp_to_pdf_converter.convert(\n    diameters=diameters,\n    concentration=pdf_dist,\n    inverse=True,\n)\n\n# Visualization\nfig, ax = plt.subplots()\nax.plot(diameters, pdf_dist, label=\"PDF\")\nax.plot(diameters, pms_dist, label=\"PMS\")\nax.plot(diameters, dNdlogDp_dist, label=\"dN/dlogDp\")\nax.set_xscale(\"log\")  # Logarithmic scale for diameters\nax.set_yscale(\"log\")  # Logarithmic scale for concentrations\nax.set_xlabel(\"Diameter (nm)\")  # X-axis label\nax.set_ylabel(\"Number concentration\")  # Y-axis label\nax.legend()  # Show legend\nplt.show()  # Display the plot\n</pre> # Get the conversion strategy dNdlogDp_to_pdf = get_conversion_strategy(     input_scale=\"dn/dlogdp\", output_scale=\"pdf\" )  # Create the converter dNdlogDp_to_pdf_converter = SizerConverter(dNdlogDp_to_pdf)  # Convert distribution dNdlogDp_dist = dNdlogDp_to_pdf_converter.convert(     diameters=diameters,     concentration=pdf_dist,     inverse=True, )  # Visualization fig, ax = plt.subplots() ax.plot(diameters, pdf_dist, label=\"PDF\") ax.plot(diameters, pms_dist, label=\"PMS\") ax.plot(diameters, dNdlogDp_dist, label=\"dN/dlogDp\") ax.set_xscale(\"log\")  # Logarithmic scale for diameters ax.set_yscale(\"log\")  # Logarithmic scale for concentrations ax.set_xlabel(\"Diameter (nm)\")  # X-axis label ax.set_ylabel(\"Number concentration\")  # Y-axis label ax.legend()  # Show legend plt.show()  # Display the plot In\u00a0[\u00a0]: Copied! <pre>help(get_conversion_strategy)\n</pre> help(get_conversion_strategy) <pre>Help on function get_conversion_strategy in module particula.util.size_distribution_convert:\n\nget_conversion_strategy(input_scale: str, output_scale: str) -&gt; particula.util.size_distribution_convert.ConversionStrategy\n    Factory function to create and return an appropriate conversion\n    strategy based on input and output scales. Use the inverse flag in the\n    converter to invert the directions of the input and output scales.\n    \n    Args:\n        input_scale: The scale of the input concentration values.\n            Either 'dn/dlogdp' or 'pms'.\n        output_scale: The desired scale of the output concentration\n            values. Either 'pms' or 'pdf'. Use inverse flag to invert the input\n            and output scales.\n    \n    Returns:\n        ConversionStrategy: A strategy object capable of converting between\n            the specified scales.\n    \n    Raises:\n        ValueError: If the input_scale or output_scale is not supported, or\n            if the specified conversion is unsupported.\n    \n    Example:\n        ``` py title=\"Convert dn/dlogdp to PMS\"\n        strategy = get_conversion_strategy('dn/dlogdp', 'pms')\n        converter = Converter(strategy)\n        converted_concentration = converter.convert(\n            diameters, concentration, inverse=False)\n        ```\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>help(SizerConverter)\n</pre> help(SizerConverter) <pre>Help on class SizerConverter in module particula.util.size_distribution_convert:\n\nclass SizerConverter(builtins.object)\n |  SizerConverter(strategy: particula.util.size_distribution_convert.ConversionStrategy)\n |  \n |  A converter that uses a specified ConversionStrategy to convert\n |  particle size distribution data between different formats.\n |  \n |  Methods defined here:\n |  \n |  __init__(self, strategy: particula.util.size_distribution_convert.ConversionStrategy)\n |      Initializes the converter with a conversion strategy.\n |      \n |      Args:\n |          strategy (ConversionStrategy): The strategy to use for conversion.\n |  \n |  convert(self, diameters: numpy.ndarray, concentration: numpy.ndarray, inverse: bool = False) -&gt; numpy.ndarray\n |      Converts particle size distribution data using the specified\n |      strategy.\n |      \n |      Args:\n |          diameters (np.ndarray): The particle diameters.\n |          concentration (np.ndarray): The concentration values.\n |          inverse (bool): Flag to perform the inverse conversion.\n |      \n |      Returns:\n |          np.ndarray: The converted concentration values.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables\n |  \n |  __weakref__\n |      list of weak references to the object\n\n</pre>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/#converting-size-distributions","title":"Converting Size Distributions\u00b6","text":"<p>Particle size distributions play a critical role in aerosol science, impacting areas such as air quality monitoring, environmental health, and various industrial processes. These distributions provide insights into the number, volume, or mass concentration of aerosol particles across different size ranges, aiding in the analysis of aerosol behavior and effects. For specific analyses or models, converting between size distribution scales, like from number to volume concentration, is often necessary. This document introduces a Python-based size distribution converter designed to facilitate conversions across different aerosol size distribution scales without the need to manually handle conversion formulas or functions, employing a factory pattern to determine the appropriate conversion strategy based on the scales involved.</p>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/#aerosol-size-distribution-scales","title":"Aerosol Size Distribution Scales:\u00b6","text":"<p>Aerosol size distributions are characterized through various metrics and scales, with each serving specific purposes in analysis and practical application. These descriptions offer a more nuanced understanding of each type:</p> <ol> <li><p>Number Concentration (dN/dlogDp): This metric quantifies the number of aerosol particles within a specified size range per unit volume of air. It is essential for assessing particle prevalence in environmental and health-related studies, providing a direct measure of how many particles are present in a given volume.</p> </li> <li><p>NOT IMPLEMENTED Mass Concentration (dM/dlogDp): Indicates the total mass of particles within a specific size range per unit volume of air. This measure is crucial for understanding the mass-related impacts of aerosols on health, visibility, and climate, as it relates directly to the amount of material in the atmosphere.</p> </li> <li><p>NOT IMPLEMENTED Volume Concentration (dV/dlogDp): This scale measures the collective volume of particles within a specified size range per unit volume of air. It's particularly useful for understanding the packing density and potential space occupancy of aerosols in a given volume, affecting light scattering and particle deposition processes.</p> </li> <li><p>NOT IMPLEMENTED Surface Area Concentration (dA/dlogDp): Provides the total surface area of particles within each size bin per unit volume of air. Though not implemented yet, this metric is vital for evaluating the interaction potential of aerosols with gases and other particles, as well as their reactivity and potential health impacts related to surface chemistry.</p> </li> <li><p>Probability Density Function (PDF): Wikipedia link A statistical representation that describes the likelihood of particles existing within specific size intervals. The PDF is instrumental in stochastic modeling, offering insights into the overall shape and characteristics of the size distribution, which helps in predicting aerosol behavior under various conditions.</p> </li> <li><p>Probability Mass Spectrum (PMS): Wikipedia link Acts as a discrete counterpart to the PDF, providing a quantized view of the size distribution that is especially useful for numerical and computational modeling. The PMS is essentially a snapshot of the number concentration (dN) distributed across discrete size bins, enabling detailed analysis and simulation of aerosol dynamics.</p> </li> </ol> <p>These metrics facilitate a comprehensive analysis of aerosol properties and behaviors, enabling researchers and practitioners to tailor their approaches based on the specific requirements of their work, whether it be in environmental monitoring, health impact assessment, or atmospheric science.</p>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/#the-converter-class-and-factory-pattern","title":"The Converter Class and Factory Pattern:\u00b6","text":"<p>The converter class is structured to allow for the conversion between different aerosol size distribution formats efficiently.</p> <p>The core <code>Converter</code> class is designed not to implement conversion behaviors directly. Instead, it utilizes a <code>ConversionStrategy</code> object that follows a specified interface to carry out conversions. This strategy-based design permits the flexible exchange of conversion methodologies without requiring changes to the <code>Converter</code> class, promoting maintainability.</p> <p>A key component of this system is the <code>get_conversion_strategy</code> factory function, which streamlines the acquisition of an appropriate conversion strategy based on the provided input and output scales. This function assesses the scales and furnishes an instance of the suitable strategy class, thus simplifying the conversion operation.</p>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/#examples","title":"Examples:\u00b6","text":"<p>Subsequent sections will showcase the practical application of the size distribution converter through examples. These examples will cover conversions such as transforming number concentration to the probability mass spectrum and the reverse process, illustrating the converter's utility and flexibility.</p>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/#generate-pdf","title":"Generate PDF\u00b6","text":"<p>To understand the behavior these different representations, we start by making a PDF. Probability Density Function (PDF), offers a continuous function representing the relative likelihood of particle sizes within a specific range.</p> <p>Lognormal distributions are characterized by their ability to model particle sizes that cannot be negative and are skewed towards larger sizes. This distribution is defined by a geometric mean diameter and a geometric standard deviation, indicating the spread of particle sizes.</p> <p>Expanding on the markdown for generating a Probability Density Function (PDF) for a lognormal distribution, including details for the code example provided:</p>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/#variables","title":"Variables\u00b6","text":"<ul> <li><p>Diameters: A range of particle diameters is defined from 20 nm to 1000 nm, discretized into 500 points, to cover the typical size range of interest in aerosol studies.</p> </li> <li><p>Sigma &amp; Modes: The lognormal distribution's geometric standard deviation (sigma) and modes (peak diameters) are specified, reflecting the distribution's breadth and the sizes most represented in the sample.</p> </li> <li><p>Number Total: The total number of particles within the distribution is set, allowing for the scaling of the PDF to represent actual particle concentrations.</p> </li> <li><p>Distribution Discretization: The <code>distribution_discretization.discretize</code> function computes the PDF based on the specified parameters. This PDF is then scaled by the total number of particles to reflect the actual size distribution.</p> </li> </ul>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/#pdf-to-pms-conversion","title":"PDF to PMS Conversion\u00b6","text":"<p>The PDF provides a continuous function indicating the likelihood of particle sizes within a specific range, while the PMS offers a discrete representation, particularly useful for computational modeling and real measurements.</p>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/#conversion-from-pms-to-pdf","title":"Conversion from PMS to PDF\u00b6","text":"<p>In practice, converting between these distributions allows researchers to leverage the strengths of both representations for comprehensive analysis. The provided code snippet illustrates how to convert a PMS distribution to a PDF using a predefined conversion strategy within a Python implementation. This process involves obtaining the appropriate conversion strategy from <code>pms</code> to <code>pdf</code>, creating a converter with this strategy, and then applying the conversion to a given distribution.</p>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/#code-description","title":"Code Description\u00b6","text":"<ul> <li>Conversion Strategy: A strategy for converting from PMS to PDF is obtained using <code>get_conversion_strategy</code>, indicating that the conversion will take the form of a probability mass spectrum to a probability density function.<ul> <li>Input_scale can either be 'dn/dlogDp' or 'pms'.</li> <li>Output_scale can either be 'pms' or 'pdf'.</li> <li>Use the inverse flag to invert the direction of the conversion.</li> </ul> </li> <li>Converter Creation: A <code>SizerConverter</code> object is instantiated with the conversion strategy, setting the stage for the conversion process.</li> <li>Distribution Conversion: The actual conversion is performed by calling the <code>convert</code> method on the <code>pms_to_pdf_converter</code>, specifying the diameters and concentration from the original PDF distribution and setting <code>inverse=True</code> to indicate the direction of conversion.</li> </ul>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/#comparison-insights","title":"Comparison Insights\u00b6","text":"<ul> <li>Scale Adaptability: The logarithmic scale for diameters effectively demonstrates the wide range of particle sizes and emphasizes the adaptability of both distributions across this range.</li> <li>Distribution Form: The PDF, being continuous, smoothly varies across sizes, while the PMS, as a discrete spectrum, provides specific values at each size interval, useful for detailed numerical analysis.</li> <li>Application Suitability: The visualization and comparison underline the suitability of each distribution form for different applications: the PDF for theoretical studies and understanding general trends, and the PMS for detailed numerical simulations and modeling.</li> </ul> <p>Through this conversion and comparison, researchers can better interpret aerosol size distributions, applying the most appropriate representation to their specific research needs and analytical frameworks.</p>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/#pdf-to-dndlogdp-conversion","title":"PDF to dN/dlogDp Conversion\u00b6","text":"<p>In this code block, we address the conversion of aerosol size distribution data from the $dN/dlogD_p$ format to a probability density function (PDF). The $dN/dlogD_p$ format represents the number concentration of particles distributed per unit log diameter, a common representation in aerosol science for describing size distributions, particularly useful for emphasizing the distribution of particles across logarithmically spaced diameter bins.</p>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/#code-description","title":"Code Description\u00b6","text":"<p>Obtaining the Conversion Strategy: The first step involves acquiring the appropriate conversion strategy to transition from $dN/dlogD_p$ to PDF. This is achieved through the <code>get_conversion_strategy</code> function, specifying <code>input_scale='dn/dlogdp'</code> and <code>output_scale='pdf'</code>, which determines the correct algorithm or mathematical approach for the conversion.</p> <p>Creating the Converter: A <code>SizerConverter</code> object is then instantiated with the obtained conversion strategy. This converter encapsulates the conversion logic, providing a clean interface for applying the conversion to aerosol size distribution data.</p> <p>Performing the Conversion: The actual conversion is executed by invoking the <code>convert</code> method of the converter, passing in the diameters and concentration values from the previously obtained or generated distribution. The <code>inverse=True</code> parameter indicates that the conversion direction is from the output scale back to the input scale, effectively generating a $dN/dlogD_p$ distribution from a PDF.</p>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/#visualization","title":"Visualization:\u00b6","text":"<p>The code block concludes with the visualization of the original PDF, the previously converted PMS distribution, and the newly converted $dN/dlogD_p$ distribution. Plotting these distributions together, especially on logarithmic scales for both diameters and concentrations, facilitates a comprehensive comparison of how particle sizes and their concentrations are represented across these different formats.</p> <p>This conversion and visualization process underscores the versatility and analytical depth achieved through transitioning between different aerosol size distribution formats. By comparing the continuous PDF, discrete PMS, and $dN/dlogD_p$ formats, researchers can gain nuanced insights into particle size distributions, enhancing the understanding of aerosol dynamics, source contributions, and environmental impacts.</p>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/#conclusions","title":"Conclusions\u00b6","text":"<p>This discussion centers around the implementation of a flexible and modular system for converting between various aerosol size distribution formats using Python. The system leverages a factory pattern to dynamically select conversion strategies based on input and output scale requirements. Through examples, we have demonstrated conversions between Probability Density Function (PDF), Probability Mass Spectrum (PMS), and the number concentration (dN/dlogDp) formats.</p>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/#summary-of-implementation","title":"Summary of Implementation\u00b6","text":"<p>The process begins with the generation of a PDF for a lognormal distribution, representing the likelihood of finding particles within specific size ranges. This foundational step is crucial for understanding aerosol particle distributions and their environmental or health impacts.</p> <p>Subsequently, we showcased how to convert this PDF into a PMS distribution and then from PMS to PDF, utilizing a <code>SizerConverter</code> class. This class applies a strategy pattern, enabling the flexible application of different conversion strategies without altering the core logic of the converter itself.</p> <p>The final code block illustrates converting from the number concentration format (dN/dlogDp) back to a PDF, underscoring the system's versatility in handling diverse aerosol size distribution data. This versatility is critical for aerosol scientists who work across various measurement techniques and modeling approaches.</p>"},{"location":"Tutorials/Data_Analysis/Converting_Size_Distributions/#expand-the-factory","title":"Expand the Factory\u00b6","text":"<p>The factory pattern has proven effective in providing a scalable and maintainable framework for extending the conversion capabilities of the system. To incorporate additional conversions not yet implemented, such as volume or mass concentration (dV/dlogDp or dM/dlogDp) and surface area concentration (dA/dlogDp), the following steps can be followed:</p> <ol> <li><p>Define New Strategy Classes: For each new conversion requirement, a corresponding strategy class implementing the <code>ConversionStrategy</code> interface should be defined. These classes would encapsulate the specific logic for converting between the new formats and the existing ones.</p> </li> <li><p>Extend the Factory Function: The <code>get_conversion_strategy</code> function should be expanded to recognize and return instances of the new strategy classes based on the provided input and output scales. This extension involves updating the function's logic to handle the new scale identifiers and ensuring that all valid conversion paths are accounted for.</p> </li> <li><p>Validation and Testing: With each new conversion strategy added, comprehensive validation and testing are crucial to ensure accuracy and reliability. This may involve comparing the results with known benchmarks or empirical data.</p> </li> </ol> <p>By following this approach, the system can be readily expanded to cover a broader range of aerosol size distribution formats as needed, while maintaining the core advantages of flexibility and modularity provided by the factory pattern. This design not only facilitates the easy integration of new conversion capabilities but also ensures that the system remains adaptable to the evolving needs of aerosol science research and application.</p>"},{"location":"Tutorials/Data_Analysis/Fitting_lognormal_PDFs_2modes/","title":"Fitting Lognormal PDFs: 2 Modes","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom particula.particles.properties import lognormal_pdf_distribution\nfrom particula_beta.data.process.ml_analysis import (\n    generate_and_train_2mode_sizer,\n)\n</pre> import numpy as np import matplotlib.pyplot as plt from particula.particles.properties import lognormal_pdf_distribution from particula_beta.data.process.ml_analysis import (     generate_and_train_2mode_sizer, ) In\u00a0[2]: Copied! <pre># Generate x-values (particle sizes)\nx_values = np.logspace(1.5, 3, 150)\n\n# Generate lognormal PDF with noise\nconcentration_pdf = lognormal_pdf_distribution(\n    x_values=x_values,\n    mode=np.array([80, 150]),\n    geometric_standard_deviation=np.array([1.2, 1.3]),\n    number_of_particles=np.array([200, 500]),\n)\n\n# Introduce noise to the data\nconcentration_pdf = concentration_pdf * np.random.uniform(\n    low=0.8, high=1.2, size=concentration_pdf.shape\n)\n\n# Plot the noisy data\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, concentration_pdf, label=\"Noisy Data\")\nplt.xscale(\"log\")\nplt.xlabel(\"Particle Size\")\nplt.ylabel(\"Concentration PDF\")\nplt.title(\"Simulated Lognormal Data with Noise\")\nplt.legend()\nplt.show()\n</pre> # Generate x-values (particle sizes) x_values = np.logspace(1.5, 3, 150)  # Generate lognormal PDF with noise concentration_pdf = lognormal_pdf_distribution(     x_values=x_values,     mode=np.array([80, 150]),     geometric_standard_deviation=np.array([1.2, 1.3]),     number_of_particles=np.array([200, 500]), )  # Introduce noise to the data concentration_pdf = concentration_pdf * np.random.uniform(     low=0.8, high=1.2, size=concentration_pdf.shape )  # Plot the noisy data plt.figure(figsize=(8, 6)) plt.plot(x_values, concentration_pdf, label=\"Noisy Data\") plt.xscale(\"log\") plt.xlabel(\"Particle Size\") plt.ylabel(\"Concentration PDF\") plt.title(\"Simulated Lognormal Data with Noise\") plt.legend() plt.show() In\u00a0[3]: Copied! <pre># Get initial guess from the ML model\n(\n    mode_values_guess,\n    geometric_standard_deviation_guess,\n    number_of_particles_guess,\n) = generate_and_train_2mode_sizer.lognormal_2mode_ml_guess(\n    logspace_x=x_values,\n    concentration_pdf=concentration_pdf,\n)\n\n# Display the initial guess results\nprint(\"Initial Guess:\")\nprint(f\"Mode: {mode_values_guess}\")\nprint(f\"GSD: {geometric_standard_deviation_guess}\")\nprint(f\"Number of particles: {number_of_particles_guess}\")\n</pre> # Get initial guess from the ML model (     mode_values_guess,     geometric_standard_deviation_guess,     number_of_particles_guess, ) = generate_and_train_2mode_sizer.lognormal_2mode_ml_guess(     logspace_x=x_values,     concentration_pdf=concentration_pdf, )  # Display the initial guess results print(\"Initial Guess:\") print(f\"Mode: {mode_values_guess}\") print(f\"GSD: {geometric_standard_deviation_guess}\") print(f\"Number of particles: {number_of_particles_guess}\") <pre>\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[3], line 6\n      1 # Get initial guess from the ML model\n      2 (\n      3     mode_values_guess,\n      4     geometric_standard_deviation_guess,\n      5     number_of_particles_guess,\n----&gt; 6 ) = generate_and_train_2mode_sizer.lognormal_2mode_ml_guess(\n      7     logspace_x=x_values,\n      8     concentration_pdf=concentration_pdf,\n      9 )\n     11 # Display the initial guess results\n     12 print(\"Initial Guess:\")\n\nFile C:\\Code\\particula-beta\\particula_beta\\data\\process\\ml_analysis\\generate_and_train_2mode_sizer.py:478, in lognormal_2mode_ml_guess(logspace_x, concentration_pdf)\n    476 folder_path = get_ml_analysis_folder()\n    477 load_path = os.path.join(folder_path, SAVE_NAME)\n--&gt; 478 ml_pipeline = load_and_cache_pipeline(filename=load_path)\n    480 # Generate ml_x_values based on the range of x_values\n    481 ml_x_values = np.logspace(\n    482     start=np.log10(logspace_x.min()),\n    483     stop=np.log10(logspace_x.max()),\n    484     num=X_ARRAY_MAX_INDEX,\n    485     dtype=np.float64,\n    486 )\n\nFile C:\\Code\\particula-beta\\particula_beta\\data\\process\\ml_analysis\\generate_and_train_2mode_sizer.py:381, in load_and_cache_pipeline(filename)\n    378 global _cached_pipeline  # pylint: disable=global-statement\n    380 if _cached_pipeline is None:\n--&gt; 381     _cached_pipeline = load_pipeline(filename=filename)\n    383 return _cached_pipeline\n\nFile C:\\Code\\particula-beta\\particula_beta\\data\\process\\ml_analysis\\generate_and_train_2mode_sizer.py:365, in load_pipeline(filename)\n    355 def load_pipeline(filename: str) -&gt; Pipeline:\n    356     \"\"\"\n    357     Load a pipeline from a file.\n    358 \n   (...)\n    363         The loaded pipeline.\n    364     \"\"\"\n--&gt; 365     return joblib.load(filename)\n\nFile c:\\Users\\253393\\AppData\\Local\\miniconda3\\envs\\ParticulaBeta_p311\\Lib\\site-packages\\joblib\\numpy_pickle.py:658, in load(filename, mmap_mode)\n    652             if isinstance(fobj, str):\n    653                 # if the returned file object is a string, this means we\n    654                 # try to load a pickle file generated with an version of\n    655                 # Joblib so we load it with joblib compatibility function.\n    656                 return load_compatibility(fobj)\n--&gt; 658             obj = _unpickle(fobj, filename, mmap_mode)\n    659 return obj\n\nFile c:\\Users\\253393\\AppData\\Local\\miniconda3\\envs\\ParticulaBeta_p311\\Lib\\site-packages\\joblib\\numpy_pickle.py:577, in _unpickle(fobj, filename, mmap_mode)\n    575 obj = None\n    576 try:\n--&gt; 577     obj = unpickler.load()\n    578     if unpickler.compat_mode:\n    579         warnings.warn(\"The file '%s' has been generated with a \"\n    580                       \"joblib version less than 0.10. \"\n    581                       \"Please regenerate this pickle file.\"\n    582                       % filename,\n    583                       DeprecationWarning, stacklevel=3)\n\nFile c:\\Users\\253393\\AppData\\Local\\miniconda3\\envs\\ParticulaBeta_p311\\Lib\\pickle.py:1213, in _Unpickler.load(self)\n   1211             raise EOFError\n   1212         assert isinstance(key, bytes_types)\n-&gt; 1213         dispatch[key[0]](self)\n   1214 except _Stop as stopinst:\n   1215     return stopinst.value\n\nFile c:\\Users\\253393\\AppData\\Local\\miniconda3\\envs\\ParticulaBeta_p311\\Lib\\pickle.py:1538, in _Unpickler.load_stack_global(self)\n   1536 if type(name) is not str or type(module) is not str:\n   1537     raise UnpicklingError(\"STACK_GLOBAL requires str\")\n-&gt; 1538 self.append(self.find_class(module, name))\n\nFile c:\\Users\\253393\\AppData\\Local\\miniconda3\\envs\\ParticulaBeta_p311\\Lib\\pickle.py:1580, in _Unpickler.find_class(self, module, name)\n   1578     elif module in _compat_pickle.IMPORT_MAPPING:\n   1579         module = _compat_pickle.IMPORT_MAPPING[module]\n-&gt; 1580 __import__(module, level=0)\n   1581 if self.proto &gt;= 4:\n   1582     return _getattribute(sys.modules[module], name)[0]\n\nModuleNotFoundError: No module named 'particula.data'</pre> In\u00a0[\u00a0]: Copied! <pre>(\n    mode_values_optimized,\n    gsd_optimized,\n    number_of_particles_optimized,\n    r2_optimized,\n    optimization_results,\n) = generate_and_train_2mode_sizer.optimize_lognormal_2mode(\n    mode_guess=mode_values_guess,\n    geometric_standard_deviation_guess=geometric_standard_deviation_guess,\n    number_of_particles_in_mode_guess=number_of_particles_guess,\n    x_values=x_values,\n    concentration_pdf=concentration_pdf,\n)\n</pre> (     mode_values_optimized,     gsd_optimized,     number_of_particles_optimized,     r2_optimized,     optimization_results, ) = generate_and_train_2mode_sizer.optimize_lognormal_2mode(     mode_guess=mode_values_guess,     geometric_standard_deviation_guess=geometric_standard_deviation_guess,     number_of_particles_in_mode_guess=number_of_particles_guess,     x_values=x_values,     concentration_pdf=concentration_pdf, ) In\u00a0[\u00a0]: Copied! <pre># Display the optimized results\nprint(\"Optimized Fit:\")\nprint(f\"Optimized mode values: {mode_values_optimized}\")\nprint(f\"Optimized GSD: {gsd_optimized}\")\nprint(f\"Optimized number of particles: {number_of_particles_optimized}\")\nprint(f\"Optimized R\u00b2: {r2_optimized}\")\nprint(f\"Best optimization method: {optimization_results['method']}\")\n</pre> # Display the optimized results print(\"Optimized Fit:\") print(f\"Optimized mode values: {mode_values_optimized}\") print(f\"Optimized GSD: {gsd_optimized}\") print(f\"Optimized number of particles: {number_of_particles_optimized}\") print(f\"Optimized R\u00b2: {r2_optimized}\") print(f\"Best optimization method: {optimization_results['method']}\") <pre>Optimized Fit:\nOptimized mode values: [ 77.92048882 146.4391948 ]\nOptimized GSD: [1.19264682 1.30742154]\nOptimized number of particles: [175.92002921 544.65291938]\nOptimized R\u00b2: 0.9736896055718199\nBest optimization method: trust-constr\n</pre> In\u00a0[\u00a0]: Copied! <pre># Generate concentration PDFs for guess and optimized values\nconcentration_pdf_guess = lognormal_pdf_distribution(\n    x_values=x_values,\n    mode=mode_values_guess,\n    geometric_standard_deviation=geometric_standard_deviation_guess,\n    number_of_particles=number_of_particles_guess,\n)\n\nconcentration_pdf_optimized = lognormal_pdf_distribution(\n    x_values=x_values,\n    mode=mode_values_optimized,\n    geometric_standard_deviation=gsd_optimized,\n    number_of_particles=number_of_particles_optimized,\n)\n\n# Plot the original, guess, and optimized PDFs\nplt.figure(figsize=(10, 7))\nplt.plot(x_values, concentration_pdf, label=\"Original Data\", linewidth=2)\nplt.plot(\n    x_values,\n    concentration_pdf_guess,\n    label=\"ML Guess\",\n    linestyle=\"--\",\n    linewidth=3,\n)\nplt.plot(\n    x_values,\n    concentration_pdf_optimized,\n    label=\"Optimized Fit\",\n    linestyle=\"-.\",\n    linewidth=4,\n)\nplt.xscale(\"log\")\nplt.xlabel(\"Particle Size\")\nplt.ylabel(\"Concentration PDF\")\nplt.title(\"Comparison of Original, ML Guess, and Optimized Fit\")\nplt.legend()\nplt.show()\n</pre> # Generate concentration PDFs for guess and optimized values concentration_pdf_guess = lognormal_pdf_distribution(     x_values=x_values,     mode=mode_values_guess,     geometric_standard_deviation=geometric_standard_deviation_guess,     number_of_particles=number_of_particles_guess, )  concentration_pdf_optimized = lognormal_pdf_distribution(     x_values=x_values,     mode=mode_values_optimized,     geometric_standard_deviation=gsd_optimized,     number_of_particles=number_of_particles_optimized, )  # Plot the original, guess, and optimized PDFs plt.figure(figsize=(10, 7)) plt.plot(x_values, concentration_pdf, label=\"Original Data\", linewidth=2) plt.plot(     x_values,     concentration_pdf_guess,     label=\"ML Guess\",     linestyle=\"--\",     linewidth=3, ) plt.plot(     x_values,     concentration_pdf_optimized,     label=\"Optimized Fit\",     linestyle=\"-.\",     linewidth=4, ) plt.xscale(\"log\") plt.xlabel(\"Particle Size\") plt.ylabel(\"Concentration PDF\") plt.title(\"Comparison of Original, ML Guess, and Optimized Fit\") plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre># Example\nx_values_example1 = np.logspace(1.2, 4, 150)\nconcentration_pdf_example1 = lognormal_pdf_distribution(\n    x_values=x_values_example1,\n    mode=np.array([100, 800]),\n    geometric_standard_deviation=np.array([1.1, 1.1]),\n    number_of_particles=np.array([300, 800]),\n)\n\n# Add noise\nconcentration_pdf_example1 = concentration_pdf_example1 * np.random.uniform(\n    low=0.8, high=1.25, size=concentration_pdf_example1.shape\n)\n\n# Obtain initial guess\nmode_guess1, gsd_guess1, particles_guess1 = (\n    generate_and_train_2mode_sizer.lognormal_2mode_ml_guess(\n        logspace_x=x_values_example1,\n        concentration_pdf=concentration_pdf_example1,\n    )\n)\n\n# Optimize\nmode_opt1, gsd_opt1, particles_opt1, r2_opt1, results1 = (\n    generate_and_train_2mode_sizer.optimize_lognormal_2mode(\n        mode_guess=mode_guess1,\n        geometric_standard_deviation_guess=gsd_guess1,\n        number_of_particles_in_mode_guess=particles_guess1,\n        x_values=x_values_example1,\n        concentration_pdf=concentration_pdf_example1,\n    )\n)\n</pre> # Example x_values_example1 = np.logspace(1.2, 4, 150) concentration_pdf_example1 = lognormal_pdf_distribution(     x_values=x_values_example1,     mode=np.array([100, 800]),     geometric_standard_deviation=np.array([1.1, 1.1]),     number_of_particles=np.array([300, 800]), )  # Add noise concentration_pdf_example1 = concentration_pdf_example1 * np.random.uniform(     low=0.8, high=1.25, size=concentration_pdf_example1.shape )  # Obtain initial guess mode_guess1, gsd_guess1, particles_guess1 = (     generate_and_train_2mode_sizer.lognormal_2mode_ml_guess(         logspace_x=x_values_example1,         concentration_pdf=concentration_pdf_example1,     ) )  # Optimize mode_opt1, gsd_opt1, particles_opt1, r2_opt1, results1 = (     generate_and_train_2mode_sizer.optimize_lognormal_2mode(         mode_guess=mode_guess1,         geometric_standard_deviation_guess=gsd_guess1,         number_of_particles_in_mode_guess=particles_guess1,         x_values=x_values_example1,         concentration_pdf=concentration_pdf_example1,     ) ) In\u00a0[\u00a0]: Copied! <pre># Display and plot\nprint(\"Example 1:\")\nprint(f\"Initial Guess:\")\nprint(f\"Mode: {mode_guess1}\")\nprint(f\"GSD: {gsd_guess1}\")\nprint(f\"Number of particles: {particles_guess1}\")\n\nprint(f\"Optimized mode values: {mode_opt1}\")\nprint(f\"Optimized GSD: {gsd_opt1}\")\nprint(f\"Optimized number of particles: {particles_opt1}\")\nprint(f\"Optimized R\u00b2: {r2_opt1}\")\nprint(f\"Best optimization method: {results1['method']}\")\nplt.figure(figsize=(8, 6))\nplt.plot(x_values_example1, concentration_pdf_example1, label=\"Original Data\")\nplt.plot(\n    x_values_example1,\n    lognormal_pdf_distribution(\n        x_values=x_values_example1,\n        mode=mode_opt1,\n        geometric_standard_deviation=gsd_opt1,\n        number_of_particles=particles_opt1,\n    ),\n    label=\"Optimized Fit\",\n)\nplt.xscale(\"log\")\nplt.xlabel(\"Particle Size\")\nplt.ylabel(\"Concentration PDF\")\nplt.legend()\nplt.show()\n</pre> # Display and plot print(\"Example 1:\") print(f\"Initial Guess:\") print(f\"Mode: {mode_guess1}\") print(f\"GSD: {gsd_guess1}\") print(f\"Number of particles: {particles_guess1}\")  print(f\"Optimized mode values: {mode_opt1}\") print(f\"Optimized GSD: {gsd_opt1}\") print(f\"Optimized number of particles: {particles_opt1}\") print(f\"Optimized R\u00b2: {r2_opt1}\") print(f\"Best optimization method: {results1['method']}\") plt.figure(figsize=(8, 6)) plt.plot(x_values_example1, concentration_pdf_example1, label=\"Original Data\") plt.plot(     x_values_example1,     lognormal_pdf_distribution(         x_values=x_values_example1,         mode=mode_opt1,         geometric_standard_deviation=gsd_opt1,         number_of_particles=particles_opt1,     ),     label=\"Optimized Fit\", ) plt.xscale(\"log\") plt.xlabel(\"Particle Size\") plt.ylabel(\"Concentration PDF\") plt.legend() plt.show() <pre>Example 1:\nInitial Guess:\nMode: [149.48451487 739.05747291]\nGSD: [1.18937477 1.07885284]\nNumber of particles: [350.00811131 776.51103601]\nOptimized mode values: [ 99.38098192 795.02739947]\nOptimized GSD: [1.10427722 1.09958701]\nOptimized number of particles: [284.06490163 819.18581964]\nOptimized R\u00b2: 0.9779916485645566\nBest optimization method: Powell\n</pre>"},{"location":"Tutorials/Data_Analysis/Fitting_lognormal_PDFs_2modes/#fitting-lognormal-pdfs-2-modes","title":"Fitting Lognormal PDFs: 2 Modes\u00b6","text":"<p>This notebook demonstrates how to use a neural network to obtain an initial guess for fitting a lognormal probability density function (PDF) with two modes. It also shows how to optimize the initial guess using a cost function and visualize the results.</p>"},{"location":"Tutorials/Data_Analysis/Fitting_lognormal_PDFs_2modes/#generate-simulated-data","title":"Generate Simulated Data\u00b6","text":"<p>We start by generating simulated lognormal data for two modes. The data includes some noise to simulate real-world conditions.</p>"},{"location":"Tutorials/Data_Analysis/Fitting_lognormal_PDFs_2modes/#guess-using-neural-network","title":"Guess Using Neural Network\u00b6","text":"<p>Next, we use a pre-trained neural network model to get an initial guess for the lognormal parameters (mode, geometric standard deviation, and number of particles).</p>"},{"location":"Tutorials/Data_Analysis/Fitting_lognormal_PDFs_2modes/#optimize-the-lognormal-fit","title":"Optimize the Lognormal Fit\u00b6","text":"<p>With the initial guess obtained, we now optimize the parameters using a cost function to minimize the difference between the actual data and the fitted lognormal distribution.</p>"},{"location":"Tutorials/Data_Analysis/Fitting_lognormal_PDFs_2modes/#visualization","title":"Visualization\u00b6","text":"<p>Finally, we compare the original data, the initial guess from the neural network, and the optimized fit. This step helps us understand how close the initial guess was and how much improvement was achieved through optimization.</p>"},{"location":"Tutorials/Data_Analysis/Fitting_lognormal_PDFs_2modes/#additional-fits","title":"Additional Fits\u00b6","text":"<p>Let's try different initial conditions to see how the optimization behaves with varying data sets.</p>"},{"location":"Tutorials/Data_Analysis/Fitting_lognormal_PDFs_2modes/#conclusion","title":"Conclusion\u00b6","text":"<p>This notebook demonstrated the use of a neural network for generating an initial guess for fitting a lognormal PDF with two modes. The initial guess was then optimized to improve the fit, and the results were visualized to compare the original data, the initial guess, and the optimized fit.</p>"},{"location":"contribute/","title":"Contribute","text":"<p>Here we have summary and links to the different ways you can contribute to the project.</p>"},{"location":"contribute/#contents","title":"Contents","text":"<ul> <li>Code of Conduct</li> <li>Contributing Guidelines</li> <li>Style Guide (Google)</li> </ul>"},{"location":"contribute/CODE_OF_CONDUCT/","title":"Code of Conduct","text":""},{"location":"contribute/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"contribute/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"contribute/CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"contribute/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"contribute/CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at . All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"contribute/CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"contribute/CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"contribute/CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"contribute/CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"contribute/CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"contribute/CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"contribute/CONTRIBUTING/","title":"Contributor Guidelines","text":"<p>First of all, thank you for considering contributing to this project! While we have specific guidelines below, we also encourage you to contribute to the best of your ability and not let these guidelines hinder your productivity and creativity. We are open to any contribution, and we are always looking for new ways to improve the project. We are also open to any feedback, however small or large, and in any area (from documentation to calculation to presentation).</p> <p>The cycle of contribution goes something like this:</p> <ol> <li> <p>See if there is an issue open that you can help with. If there is not one, please open one.</p> </li> <li> <p>Create a personal fork of this repository; and in it, create a branch (from <code>uncscode:main</code>) with the <code>issue000</code> in the name of the branch (e.g. <code>username/issue000</code> or <code>issue000</code>), where <code>000</code> is the number of the issue from step 1.</p> </li> <li> <p>Set up an appropriate environment:</p> <ul> <li>an easy option is just to use the <code>.devcontainer</code> in root</li> <li>another option is to either <code>pip install</code> or <code>conda install</code> the packages required for development in <code>requirements.txt</code> in root.</li> </ul> </li> <li> <p>Write your code in the branch. This usually includes the following.</p> <p>a. Code to be implemented.</p> <p>b. Documentation associated with added code in a.</p> <p>c. Tests associated with added code in a.</p> <p>d. Ideally, you'd also add a Jupyter notebook to showcase your work (if applicable).</p> </li> <li> <p>Optionally, you can run standard linting and testing calls on your code locally to make sure it works as expected. This can be done in several ways, for example the <code>pylint</code>, <code>flake8</code>, and <code>pytest</code> calls below. These calls will be run once you submit your pull request.</p> </li> <li> <p>Submit a pull request to the <code>main</code> branch of this repository. Upon submission, standard automated tests will be run on your code.</p> </li> <li> <p>If you don't hear back from maintainers, feel free to mention one of us directly in the comments of the PR. Expect to have speedy feedback and help from us to finalize the PR.</p> </li> </ol> <pre><code>pylint $(find particula/ -name \"*.py\" | xargs)\n</code></pre> <pre><code>flake8 particula/ --count\n</code></pre> <pre><code>pytest particula/\n</code></pre> <p>More information about contributing to this project can be found below. We are excited and looking forward to your contribution!</p>"},{"location":"contribute/CONTRIBUTING/#github","title":"GitHub","text":"<p>We use GitHub to develop <code>particula</code> completely in the open. Our repository is available here: https://uncscode.github.io/particula/. There are several ways to use GitHub: through the command line via <code>git</code> and/or <code>gh</code>, through the web interface and/or the GitHub web editor, or through an IDE like PyCharm or a code editor like Visual Studio Code. In general, we recommend that you fork our repository, that you work with VS Code, and that submit a pull request based on an issue. If any of these sound unfamiliar or if you need help, please see more information below and feel free to contact us directly to discuss options. We look forward to getting you started and up to speed on this project with us!</p> <p>Links: https://docs.github.com/en/get-started</p>"},{"location":"contribute/CONTRIBUTING/#vs-code","title":"VS Code","text":"<p>Visual Studio Code is a free and open-source code editor for writing code and it has a rich ecosystem of extensions that allow you to write code in a variety of languages with a lot of helpful features and tools.</p> <p>Links: https://code.visualstudio.com/</p>"},{"location":"contribute/CONTRIBUTING/#devcontainer","title":"<code>.devcontainer</code>","text":"<p>In the root of our repository, we have a <code>.devcontainer</code> folder that contains the environment variables required for development. This is a convenient way to set up the environment for development. It requires Docker to be installed and running. And the first time it runs it may take a few minutes to install the Docker image (a minimal operating system with Python 3.9).</p> <p>Links: https://code.visualstudio.com/docs/remote/containers</p>"},{"location":"contribute/CONTRIBUTING/#python-code-style","title":"Python code style","text":"<p>We follow the Google's Python style guide. We encourage you to follow it too, but we also encourage you to contribute to the best of your ability and not let these guidelines hinder your productivity and creativity.</p> <p>Links: https://google.github.io/styleguide/pyguide.html</p>"},{"location":"contribute/CONTRIBUTING/#running-particula-locally","title":"Running <code>particula</code> locally","text":"<p>Once you are in the root directory, you will be able to import <code>particula</code> as a package/model and thus all documentation on website applies. You must be in the root directory.</p>"},{"location":"contribute/CONTRIBUTING/#writing-tests","title":"Writing tests","text":"<p>It is essential that every piece of code has an associated test. This is a good way to ensure that the code is working as intended. It also ensures that the code is not broken and that the code is not too complex. However small or big, a test is always required.</p>"},{"location":"contribute/CONTRIBUTING/#running-testinglinting-locally","title":"Running testing/linting locally","text":"<p>We use <code>pytest</code>, <code>pylint</code>, and <code>flake8</code> to run tests and linting. The command below can be run in the root directory like you'd run the package above.</p> <pre><code>pylint $(find particula/ -name \"*.py\" | xargs)\n</code></pre> <pre><code>flake8 particula/ --count\n</code></pre> <pre><code>pytest particula/\n</code></pre>"},{"location":"contribute/CONTRIBUTING/#building-particula-locally","title":"Building <code>particula</code> locally","text":"<p>To build <code>particula</code> locally, you must be in the root directory. You have two options, depending on your usage case.</p> <ol> <li>You can use <code>python -m build</code> to build the package wheels locally (note: you will need to install <code>build</code> too, via <code>pip install build</code>).</li> <li>You can build the conda recipe available at https://github.com/conda-forge/particula-feedstock either via <code>python build-locally.py</code> in the root of <code>particula-feedstock</code> or via <code>conda build recipe</code> (equivalently, but faster, <code>mamba build recipe</code>). For the latter, you will need to have <code>conda-build</code> installed (for <code>conda build</code> to work) or <code>boa</code> (for <code>mamba build</code> to work). In either case, you can install package with conda via, <code>conda install conda-build</code> or <code>mamba install boa</code>.</li> </ol> <p>Links: https://packaging.python.org/en/latest/tutorials/packaging-projects/ and https://docs.conda.io/projects/conda-build/en/latest/user-guide/index.html</p>"},{"location":"contribute/CONTRIBUTING/#documentation-writing","title":"Documentation writing","text":"<p>We prefer that the tutorials are written in the form of Jupyter notebooks after the package is released and published. A convenient option is using Google's Colaboratory to write the notebooks.</p> <p>Links: https://colab.research.google.com/</p>"},{"location":"contribute/CONTRIBUTING/#more-information","title":"More information","text":"<p>We will update this regularly with more information, but in the meanwhile, please feel free to contact us directly on GitHub.</p>"},{"location":"next/","title":"Next Outline","text":"<p>Next is the next iteration of the particula simulation model. It is a complete rewrite of the model, with a focus on improving the modularity and extensibility of the model. The goal is to make it easier to add new features and to make the model more flexible and easier to use.</p>"},{"location":"next/#particle-resolved","title":"Particle Resolved","text":"<ul> <li> Coagulation needs to be implemented</li> <li> check equilibrium for condensation and evaporation</li> </ul>"},{"location":"next/#continuous-particle-pdf-and-pmf","title":"Continuous Particle PDF and PMF","text":"<ul> <li> Bin Remapping after condensation and evaporation is needed, or dr/dt method</li> <li> Coagulation needs to be checked for PMF vs PDF handling</li> <li> ODE solver for both needs to be implemented</li> </ul>"},{"location":"next/#scavenging","title":"Scavenging","text":"<ul> <li> Scavenging needs to be implemented for Particle Resolved</li> </ul>"},{"location":"next/#activity-coefficients","title":"Activity Coefficients","text":"<ul> <li> BAT model needs added to activity coefficient calculation</li> </ul>"},{"location":"next/#guides-for-developers","title":"Guides for Developers","text":"<ul> <li> More focused reorganization of the code and examples.</li> </ul>"}]}